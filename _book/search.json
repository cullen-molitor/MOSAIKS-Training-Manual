[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIKS Training Manual",
    "section": "",
    "text": "Welcome\nThis is the first iteration of the MOSAIKS Training Manual! The manual serves as a comprehensive reference for understanding MOSAIKS, its capabilities, and practical implementation guidance.\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. It is a framework designed to simplify the use of satellite imagery and machine learning for analyzing socioeconomic and environmental outcomes across different geographic contexts and time periods.\nThis comprehensive two-week program is designed for academics, professionals, and policymakers interested in leveraging MOSAIKS to better understand socioeconomic and environmental challenges. The course is particularly valuable for those working in:\nThroughout this course, you’ll learn practical applications through a combination of:\nThe curriculum covers the complete MOSAIKS workflow:\nWhether you’re new to MOSAIKS or looking to deepen your expertise, this course provides the tools and knowledge needed to effectively utilize this powerful framework."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "MOSAIKS Training Manual",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMOSAIKS was developed and is supported by researchers across multiple partner organizations:\nCore Team:\nBenjamin Recht, Esther Rolf, Tamma Carleton, Eugenio Noda, Hikari Murayama, Hannah Druckenmiller, Ian Bolliger, Jeanette Tseng, Jessica Katz, Jonathan Proctor, Luke Sherman, Miyabi Ishihara, Simon Greenhill, Solomon Hsiang, Taryn Fransen, Trinetta Chong, Vaishaal Shankar, Graeme Blair, Darin Christensen, Shopnavo Biswas, Karena Yan, Cullen Molitor, Grace Lewin, Steven Cognac, Juliet Cohen\nPartner Organizations:\n\nCenter for Effective Global Action (CEGA; UCB)\nEnvironmental Markets Lab (emLab; UCSB)\nGlobal Policy Lab (GPL; Stanford University)\nHarvard Data Science Initiative (HDSI; Harvard University)\nProject on Resources and Governance (PRG; UCLA)\nMaster of Environmental Data Science Program (MEDS; UCSB)\n\nFunding Support:\n\nThe Patrick J. McGovern Foundation\nThe United States Agency for International Development (USAID)\nUnited Nations Development Programme (UNDP)"
  },
  {
    "objectID": "intro.html#course-structure",
    "href": "intro.html#course-structure",
    "title": "Introduction",
    "section": "Course structure",
    "text": "Course structure\nThis course is designed as an intensive two-week program that combines lectures, demonstrations, and hands-on sessions. Each day is structured as follows:\n\n\n\nTime\nActivity\n\n\n\n9:00 - 10:30\nMorning Session 1\n\n\n10:30 - 11:00\nBreak\n\n\n11:00 - 12:30\nMorning Session 2\n\n\n12:30 - 1:30\nLunch\n\n\n1:30 - 3:00\nAfternoon Session 1\n\n\n3:00 - 3:30\nBreak\n\n\n3:30 - 4:30\nAfternoon Session 2\n\n\n4:30 - 5:00\nFeedback and Development\n\n\n\nEach day concludes with a Q&A and feedback session from 4:30-5:00, providing opportunities to clarify concepts and share ideas. It is expected that this first course will spur many new ideas and concepts which should be included in the future trainings. Please remember to take notes throughout each day with particular emphasis in areas you think could be explained better or that need additional topics covered. These can be areas that you struggled with or that you would anticipate could be difficult for others."
  },
  {
    "objectID": "intro.html#schedule-overview",
    "href": "intro.html#schedule-overview",
    "title": "Introduction",
    "section": "Schedule overview",
    "text": "Schedule overview\n\nCourse schedule by week\n\n\n\n\n\n\nDay\nWeek 1\nWeek 2\n\n\n\nMonday\nOrientation and introduction to MOSAIKS\nHoliday - No Class\n\n\nTuesday\nGround labels and data processing fundamentals\nTask modeling and machine learning applications\n\n\nWednesday\nAgriculture applications and MOSAIKS API\nUnderstanding uncertainty in MOSAIKS\n\n\nThursday\nSatellite imagery fundamentals and processing\nSurvey data processing and design\n\n\nFriday\nDeep dive into random convolutional features (RCFs)\nFuture directions and advanced applications\n\n\n\nMain topics covered each day:\nWeek 1\n\n\nDay 1: MOSAIKS framework, accessing features, basic workflow\n\nDay 2: Understanding ground truth data, data cleaning, spatial resolution\n\nDay 3: Agricultural yield prediction, downloading features via API\n\nDay 4: Types of satellite imagery, processing considerations, quality control\n\nDay 5: Feature computation, theory behind RCFs, hands-on implementation\nWeek 2\n\n\nDay 1: Martin Luther King Jr. Day - No Class\n\n\nDay 2: Model selection, hyperparameter tuning, cross-validation\n\nDay 3: Error analysis, confidence intervals, reporting uncertainty\n\nDay 4: Survey design principles, geolocation methods, sampling strategies\n\nDay 5: Advanced topics, emerging applications, future development"
  },
  {
    "objectID": "intro.html#training-expectations",
    "href": "intro.html#training-expectations",
    "title": "Introduction",
    "section": "Training expectations",
    "text": "Training expectations\nWhat you will learn\n\nUnderstanding of MOSAIKS framework and capabilities\nPractical skills in satellite imagery processing\n\nExperience with machine learning applications\nHands-on practice with real-world datasets\nKnowledge of survey data integration\nBest practices for model implementation\nPrerequisites\nThere are no explicit prerequisites, though this course does cover some advanced topics in:\n\nThe Python programming language\nMachine learning\nGeospatial data\nParticipant expectations\n\nActive participation in discussions and hands-on sessions\nCompletion of assigned homework (particularly the Week 1 Friday assignment)\n\nEngagement in Q&A sessions\nContribution to feedback sessions for course improvement\nComputing requirements\nThe course includes hands-on computing sessions. You will need:\n\nA computer with access to the internet\nA Google account\nAccess to Google Colaboratory\nAccess to necessary data (details to be provided)\nHomework and presentations\nThere will be a homework assignment at the end of Week 1, which participants will present on Tuesday of Week 2. This assignment is designed to reinforce learning and provide practical experience with MOSAIKS tools."
  },
  {
    "objectID": "intro-compute.html#requirements",
    "href": "intro-compute.html#requirements",
    "title": "1  Compute setup",
    "section": "\n1.1 Requirements",
    "text": "1.1 Requirements\nTo participate in the coding portions of this course, you’ll need:\n\nA laptop or desktop computer\nReliable internet connection\nA Google account (if you don’t have one, create one at accounts.google.com)\nA modern web browser (Chrome recommended)"
  },
  {
    "objectID": "intro-compute.html#getting-started-with-google-colab",
    "href": "intro-compute.html#getting-started-with-google-colab",
    "title": "1  Compute setup",
    "section": "\n1.2 Getting started with Google Colab",
    "text": "1.2 Getting started with Google Colab\n\n1.2.1 Accessing Colab\n\nGo to colab.research.google.com\n\nSign in with your Google account\nClick “New Notebook” to create your first Colab notebook\n\n1.2.2 Understanding the interface\nThe Colab interface is similar to Jupyter notebooks, with a few key components:\n\n\nMenu Bar: Contains File, Edit, View, Insert, Runtime, Tools, and Help options\n\nToolbar: Quick access to common actions like adding code/text cells\n\nCell Area: Where you write and execute code or text\n\nRuntime Status: Shows the state of your notebook’s connection to Google’s servers\n\n1.2.3 Basic operations\n\n\nCreating Cells:\n\nCode cells: Click “+ Code” or use Ctrl+M B\nText cells: Click “+ Text” or use Ctrl+M M\n\n\n\nRunning Cells:\n\nClick the play button next to the cell\nUse Shift+Enter\nSelect Runtime &gt; Run all from the menu\n\n\n\nCell Types:\n\nCode cells: For Python code execution\nText cells: For documentation (supports Markdown)\n\n\n\n1.2.4 Important features\n\n\nRuntime Type:\n\nClick Runtime &gt; Change runtime type\nSelect Python 3 as the runtime\nFor GPU access: Change hardware accelerator to GPU when needed\n\n\n\nFile Management:\n\nFiles uploaded to Colab are temporary\nConnect to Google Drive for persistent storage:\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nPackage Installation:\n\nInstall additional packages using:\n\n\nconda\npip\n\n\n\n# add a -c conda-forge to select the conda-forge channel\n# add a -q flag to install quietly (reduced output)\n# add a -y flag to prememptively accept other changes\n!conda install package_name\n\n\n!pip install package_name\n\n\n\n\n1.2.5 Best practices\n\nSave Your Work:\n\nRegularly save to Google Drive (File &gt; Save a copy in Drive)\nDownload important notebooks locally as backups\n\n\nResource Management:\n\nClose unused notebooks to free up resources\nBe aware of idle timeouts (notebooks disconnect after extended inactivity)\n\n\nMemory Usage:\n\nMonitor memory usage through Runtime &gt; Resource usage\nUse Runtime &gt; Factory reset runtime if you run into memory issues\n\n\n\n1.2.6 Keyboard shortcuts\nHere are the most useful keyboard shortcuts for working in Colab:\n\n\nWindows/Linux\nMac\n\n\n\n\n\nAction\nShortcut\n\n\n\nRun current cell\nCtrl+Enter\n\n\nRun cell and move to next\nShift+Enter\n\n\nRun cell and insert below\nAlt+Enter\n\n\nInsert code cell above\nCtrl+M A\n\n\nInsert code cell below\nCtrl+M B\n\n\nConvert to text cell\nCtrl+M M\n\n\nConvert to code cell\nCtrl+M Y\n\n\nDelete current cell\nCtrl+M D\n\n\nToggle line numbers\nCtrl+M L\n\n\nToggle output\nCtrl+M O\n\n\nCut cell\nCtrl+M X\n\n\nCopy cell\nCtrl+M C\n\n\nPaste cell below\nCtrl+M V\n\n\nSelect multiple cells\nShift+Up/Down\n\n\nFind and replace\nCtrl+F\n\n\nSave notebook\nCtrl+S\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\nRun current cell\n⌘+Enter\n\n\nRun cell and move to next\nShift+Enter\n\n\nRun cell and insert below\nOption+Enter\n\n\nInsert code cell above\n⌘+M A\n\n\nInsert code cell below\n⌘+M B\n\n\nConvert to text cell\n⌘+M M\n\n\nConvert to code cell\n⌘+M Y\n\n\nDelete current cell\n⌘+M D\n\n\nToggle line numbers\n⌘+M L\n\n\nToggle output\n⌘+M O\n\n\nCut cell\n⌘+M X\n\n\nCopy cell\n⌘+M C\n\n\nPaste cell below\n⌘+M V\n\n\nSelect multiple cells\nShift+Up/Down\n\n\nFind and replace\n⌘+F\n\n\nSave notebook\n⌘+S\n\n\n\n\n\n\nYou can view all available shortcuts in Colab by pressing Ctrl+M H (⌘+M H on Mac) or through Help &gt; Keyboard shortcuts in the menu.\n\n1.2.7 Common issues and solutions\n\nRuntime Disconnections:\n\nClick “Reconnect” when prompted\nYour variables will be reset, but saved code remains\n\n\nPackage Installation Issues:\n\nRestart runtime after installing new packages\nUse Runtime &gt; Restart runtime\n\n\nMemory Errors:\n\nClear unnecessary variables\nRestart runtime\nConsider using smaller data samples during development\n\n\n\n\n\n\n\n\n\nMemory errors are common when working with large datasets or complex models on the free tier of Colab. If you encounter these issues, consider using a paid version of Colab or connecting a Google Cloud Platform vitual machine (VM).\n\n\n\n\n1.2.8 Getting help\n\nAccess Colab’s built-in documentation: Help &gt; Colab Overview\n\nView keyboard shortcuts: Help &gt; Keyboard shortcuts\n\nSearch the Help menu for specific topics\nUse the Help &gt; Search Solutions feature"
  },
  {
    "objectID": "intro-compute.html#ai-assistance-in-colab",
    "href": "intro-compute.html#ai-assistance-in-colab",
    "title": "1  Compute setup",
    "section": "\n1.3 AI assistance in Colab",
    "text": "1.3 AI assistance in Colab\nGoogle Gemini is a powerful AI assistant that is seemlesly integrated with Google Colab. You can use it to generate code, comments, or markdown text to improve your notebooks. Gemini can be accessed in several ways in Colab to help you with your work, all start by selecting the Gemini icon in different parts of the notebook editor.\n\n\n\n\n\n\nGemini icon\n\n\n\n\nLook for this icon to indicate where you can click to access Gemini in Colab.\n\n\nHere are a few ways you can use Google Gemini effectively in Colab:\n\n1.3.1 Chat support\nClick the “Gemini” button in the top-right corner to open a chat interface where you can ask questions about your code, debug issues, or get explanations of concepts. This option is especially useful for beginners or for complex problems.\n\n1.3.2 Code generation\nUse the “Generate code” option (the sparkle icon) above any empty code cell to generate new code based on your description. You can ask it to many different things including:\n\nLoading a dataset called my_data.csv\n\nPlot a histogram of the data\nBuild a model to predict y from X\n\n\n1.3.3 Code explanation\nUse the “Explain code” option (the sparkle icon) above any complete code cell to open a chat interface that will automatically explain the code in the cell. This is useful for understanding code written by someone else, learning new concepts you are not familiar with, or getting a second opinion on your work.\n\n1.3.4 Code completion\nColab provides intelligent autocomplete as you type:\n\nPress Tab to accept suggestions\nUse Ctrl+Space (Cmd+Space on Mac) to manually trigger suggestions\nGet real-time documentation and parameter hints\n\n\n\n\n\n\n\nWhile these AI tools are helpful, always review and understand the code they suggest before using it in your work."
  },
  {
    "objectID": "intro-compute.html#accessing-course-notebooks",
    "href": "intro-compute.html#accessing-course-notebooks",
    "title": "1  Compute setup",
    "section": "\n1.4 Accessing course notebooks",
    "text": "1.4 Accessing course notebooks\nAll course notebooks are hosted on GitHub and can be accessed directly in Google Colab. There are two ways to access the notebooks:\n\n1.4.1 Method 1: Direct links\nEach section of this book includes direct “Open in Colab” links for relevant notebooks. Simply click the badge to open the notebook:\nExample \n\n1.4.2 Method 2: Clone the notebook\nTo choose a notebook from the repository (Add link to demo/interactive notebooks here):\n\nOpen Google Colab (colab.research.google.com)\nClick File &gt; Open Notebook\n\nSelect the GitHub tab\nEnter the repository URL: https://github.com/[username]/[repo] (UPDATE WITH REPO)\nSelect the notebook you want to open\n\n1.4.3 Saving your work\nWhen you open a notebook from GitHub in Colab, it creates a temporary copy. To save your work:\n\nClick File &gt; \"Save a copy in Drive\"\n\nThis creates your own editable copy in your Google Drive\nAll future changes will be saved to your copy\n\n1.4.4 Notebook organization\nThe course notebooks are organized into:\n\n\ndemos/: Complete demonstration notebooks\n\nexercises/: Interactive notebooks with exercises to complete\n\nsolutions/: Complete versions of exercise notebooks\n\nEach notebook includes:\n\nClear instructions in markdown cells\nCode cells with examples or exercises\nTO DO sections for exercises\nValidation cells to check your work"
  },
  {
    "objectID": "intro-compute.html#data-access-and-management",
    "href": "intro-compute.html#data-access-and-management",
    "title": "1  Compute setup",
    "section": "\n1.5 Data access and management",
    "text": "1.5 Data access and management\nThere are several ways to access data in Colab notebooks. Here are the main approaches:\n\n1.5.1 Direct downloads\nFor data hosted on repositories like Zenodo, you can download directly using wget:\n# Set the data directory\ndata_dir = \"LSMS-ISA-data\"\n\n# Download the data\n!wget https://zenodo.org/records/14040658/files/Data.zip\n\n# Unzip and organize\n!unzip Data.zip\n!mv Data {data_dir}\n!rm Data.zip\n\n1.5.2 Google Drive integration\nFor data stored in Google Drive:\n\nFirst mount your Google Drive:\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nAccess your data using the mounted path:\n\ndrive_path = \"/content/drive/MyDrive/project_folder\"\n\nFor better performance, copy data to local VM storage:\n\n\n\n\n\n\n\nRemember that the VM’s storage is temporary - files will be deleted when the runtime disconnects. Always keep a backup of your data in Drive or another permanent storage location.\n\n\n\nimport os\nimport shutil\n\n# Create local directory\nlocal_dir = \"/content/data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Copy data from Drive to VM\ndrive_data = os.path.join(drive_path, \"my_data\") \nshutil.copytree(drive_data, local_dir, dirs_exist_ok=True)\n\n1.5.3 Why copy data to the VM?\nWhen working with data in Colab, copying files from Google Drive to the virtual machine (VM) can significantly improve performance:\n\nFaster Access: Reading directly from Google Drive requires data to be transferred over the network for each operation. Local VM storage provides much faster read/write speeds.\nReduced Latency: Network latency between Colab and Google Drive can slow down operations that require multiple data accesses. Local data eliminates this latency.\nMore Reliable: Network connectivity issues or Drive access problems won’t interrupt your analysis once data is copied locally.\nBetter for Iterative Processing: If your code needs to read the same data multiple times (like in machine learning training loops), local access is much more efficient.\n\nFor example, reading a 1GB dataset from Drive might take 30 seconds, while reading from local VM storage could take just a few seconds. The time spent copying data once at the start of your session can save significant time during analysis. This can be especially true in a notebook environment when a user is developing code and may need to access and reaccess the data multiple times.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will take a closer look at MOSAIKS."
  },
  {
    "objectID": "intro-mosaiks.html#the-challenge",
    "href": "intro-mosaiks.html#the-challenge",
    "title": "\n2  What is MOSAIKS?\n",
    "section": "\n2.1 The challenge",
    "text": "2.1 The challenge\nRight now, numerous public satellite systems collect huge amounts of data about the world every day. But there is so much imagery (terabytes per day) that it’s overwhelming to sort through by hand; and it’s too complex and unstructured to be usable in its raw form for most applications.\nThat is why linking satellite imaging to machine learning (SIML) is incredibly powerful. It enables vast amounts of unstructured image data to be transformed into structured information that can immediately be used for planning, research, and decision-making.\nWe believe that people all over the world should be able to access SIML technologies, but we also recognize that most people who would benefit the most from these tools don’t have the time or resources to manage enormous satellite imagery data sets and learn how to apply machine learning to them.\nINSERT IMAGE HERE"
  },
  {
    "objectID": "intro-mosaiks.html#the-solution",
    "href": "intro-mosaiks.html#the-solution",
    "title": "\n2  What is MOSAIKS?\n",
    "section": "\n2.2 The solution",
    "text": "2.2 The solution\nThat’s why we developed MOSAIKS.\nMOSAIKS is designed to work “out of the box” for a wide array of SIML applications, for people with no SIML expertise who work on normal desktop computers. MOSAIKS users never actually have to touch satellite imagery themselves and only need to have basic statistical training.\n\n\n\n\n\n\nIf you can run a regression, you can use MOSAIKS.\n\n\n\nMOSAIKS empowers users to create their own new data sets from satellite imagery. We don’t control what variables users look at, and we never need to know. MOSAIKS is a system that allows users to quickly transform vast amounts of imagery into maps of new variables, using their own training data.\nIf you’ve ever been curious about trying machine learning with satellite imagery, but don’t know anything about machine learning or satellite imagery, MOSAIKS is for you.\nAnd if you know a lot about machine learning and satellite imagery, MOSAIKS might still be for you, since it performs competitively with deep learning methods but is much simpler and cheaper to use.\nINSERT IMAGE HERE"
  },
  {
    "objectID": "intro-mosaiks.html#how-mosaiks-works",
    "href": "intro-mosaiks.html#how-mosaiks-works",
    "title": "\n2  What is MOSAIKS?\n",
    "section": "\n2.3 How MOSAIKS works",
    "text": "2.3 How MOSAIKS works\nThe basic idea of MOSAIKS is to seperate users from the costly and difficult process of transforming imagery into inputs (called “features”) for a machine learning algorithm (images → X). We do that part, so users never have to download or manage imagery. Users then download a table of MOSAIKS features (X), link them to their own geocoded data on the outcome (Y) they are interested in (called “labels”) and run a linear regression to predict their labels using MOSAIKS features (Y = Xβ).\nAll users use the same MOSAIKS features and just match them to their labels based on location. Users can run their analysis on any statistical software they are comfortable with. For most applications, the computing demands will not require users to work with specialized machines, since desktops and laptops work.\nMOSAIKS works because MOSAIKS features capture a huge amount of information about the colors, patterns and textures that show up in satellite imagery. We don’t know what patterns/colors/textures will be important for the application that users have (since we don’t know what applications users will try), so we just try to capture all of them. The purpose of the regression step is to teach the model which patterns/colors/textures predict the labels, and then to use that understanding to make predictions in locations where users don’t have labels. In addition, MOSAIKS encodes image information in a way that allows for nonlinear relationships between labels and images, even though the regression that users implement is a linear regression.\n\n2.3.1 Five steps to using MOSAIKS\nFor users, the procedure for using MOSAIKS has five steps (corresponding figure from Rolf et al. is below):\n\nDownload MOSAIKS features (X) in the areas where you have labels.\nMerge the features with your labels (Y) based on location (so features at position P are linked to labels at position P).\nRun a ridge regression of your labels on the MOSAIKS features (Y = Xβ).\nEvaluate performance.\nUse the results of the regression model (β) to make predictions (Xβ) in a new region of interest where you do not have labels, using only the MOSAIKS features that correspond with those new locations.\n\n\n\nRolf et al. 2021 Figure 1"
  },
  {
    "objectID": "intro-mosaiks.html#what-can-mosaiks-predict",
    "href": "intro-mosaiks.html#what-can-mosaiks-predict",
    "title": "\n2  What is MOSAIKS?\n",
    "section": "\n2.4 What can MOSAIKS predict?",
    "text": "2.4 What can MOSAIKS predict?\n\n\n\n\n\n\nThis question is answered in greater detail in Chapter 5\n\n\n\nMOSAIKS has been successfully used to predict a wide range of outcomes including:\n\nEnvironmental conditions (forest cover, elevation)\nPopulation patterns (density, nighttime lights)\nEconomic indicators (income, house prices)\nInfrastructure (road networks)\n\nThe figure below is from the original MOSAIKS publication (Rolf et al. 2021). The left maps show the input labels. The right map shows the modeled predictions. The scatter plot shows the modeled predictions against the true labels and reports the coefficient of determination (R²) as a measure of performance.\n\n\nRolf et al. 2021 Figure 2\n\nImportantly, all these predictions use the same set of satellite features - there’s no need to reprocess the imagery for different tasks. MOSAIKS achieves accuracy comparable to more complex deep learning methods, but at a fraction of the computational cost. This is the power of MOSAIKS, it removes the need for repreocessing the imagery after the initial encoding.\n\n2.4.1 Building understanding\nIf you are interested in using MOSAIKS, you can also see our tutorial and slide deck. And if you have a little bit of time, we recommend reading the paper we wrote when we introduced the system. We wrote it with users in mind, so we tried to make it as clear and accessible as possible.\nIf you use MOSAIKS, please reference: Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will demonstrate MOSAIKS with real data."
  },
  {
    "objectID": "intro-api.html#api-overview",
    "href": "intro-api.html#api-overview",
    "title": "3  Access MOSAIKS",
    "section": "\n3.1 API Overview",
    "text": "3.1 API Overview\n\n\n\n\n\n\nMOSAIKS API Link\n\n\n\napi.mosaiks.org\n\n\nWe have worked to develop multiple ways to access MOSAIKS features:\n\nGlobal Administrative Units (Planet imagery) You can download features aggregated to country, province, or municipality (ADM0/ADM1/ADM2 boundaries), as described in Sherman et al. (2023). These features are based on Planet imagery. These files are relatively small in size and can be used at these resolutions or to downscale administrative data. To download them, register at api.mosaiks.org and access them via the “Precomputed Files” tab.\nUSA grid from Rolf et al (Google basemap imagery) You can download features for a set of locations across the United States, as described in Rolf et al. (Nature Communications, 2021). These features are based on imagery from the Google Earth base map. You can download the features from the Code Ocean Capsule associated with that manuscript (the capsule will also allow you to replicate the analysis of that paper on a remote machine). The Github repository for that analysis is here.\nGlobal 0.01 x 0.01 degree grid (Planet imagery) You can download features for a complete and dense grid of global land areas via our API. These features are based on on quarterly mosaics from Planet’s Surface Reflectance Basemaps produce from 2019 Q3. Because the complete data set is large (multiple TB), you will need to request custom subsamples of the imagery. To download them, register at api.mosaiks.org and access them via the “Map Query” tool or by uploading a list of locations via the “File Query” tool.\nRecompute MOSAIKS features (Landsat & Sentinel imagery) You can recompute MOSAIKS features yourself using Microsoft’s Planetary Computer (Github repo which currently supports Gaussian random convolutional features). This approach will not provide the benefit of pre-computed features, since you will recompute features on-the-fly every time, but the massive compute power of the Planetary Computer makes this relatively fast and cheap for users.\nRecompute MOSAIKS features (general instruction) You can recompute MOSAIKS features yourself using instructions provided in Chapter 14. By following along with this chapter, you will gain in depth knowledge of how to compute customized features from any input imagery source. The process of dealing with satellite imagery and image featurization are the most challenging aspects of MOSAIKS. This section is only recommended for users with baseline knowledge of remote sensing methodologies, and ideally at least some understanding of convolutional nueral network models.\n\nWe have put together a Resource Page for MOSAIKS users here (registration required), which includes example Python and R notebooks for using the pipeline.\nDon’t forget to see our Tutorial Page here, which has an example Python notebook that we walk through in the video.\nIf you are looking for new data sets that we create using MOSAIKS (not features), we will be posting those here."
  },
  {
    "objectID": "intro-api.html#register-for-an-account",
    "href": "intro-api.html#register-for-an-account",
    "title": "3  Access MOSAIKS",
    "section": "\n3.2 Register for an account",
    "text": "3.2 Register for an account\nVisit api.mosaiks.org.\n\n\nLogin page\n\nSelect Register to create an account. You’ll need to provide:\n\n\nRegistration page\n\nOnce registered, you can log in to access the MOSAIKS features and begin downloading data.\n\n\nLanding page"
  },
  {
    "objectID": "intro-api.html#downloading-features",
    "href": "intro-api.html#downloading-features",
    "title": "3  Access MOSAIKS",
    "section": "\n3.3 Downloading features",
    "text": "3.3 Downloading features\nThe MOSAIKS features are organized using a 0.01 x 0.01 degree latitude-longitude global grid, centered at .005 degree intervals.\nWhen you download features, you’ll receive them in a tabular .csv format where:\n\nEach row represents a unique grid cell\nThe first two columns contain latitude and longitude coordinates\nThe remaining columns represent K features (currently K = 4000 features)\n\nImportant notes about downloads:\n\nFiles remain available for download for 15 days\nAfter 15 days, files are automatically deleted from the system\nThere is a limit of 100,000 records per query\n\n\n3.3.1 Ways to query features\nThere are two main methods to obtain features through the API:\n\nMap Query\n\n\nCreate rectangular boxes by specifying latitude and longitude coordinates\nMultiple boxes can be created\nThe system displays an estimated number of records for each box\nNote that estimates are based on box area and may not reflect actual record numbers, especially for areas containing seas and oceans\n\n\nFile Query\n\n\nSubmit a file with custom latitude and longitude coordinates\nThe API returns features for grid cells closest to your input coordinates\nPoints are allocated to the nearest grid point if they don’t exactly match\nThe output file may have a different number of rows than your input\nPoint ordering may change in the output\n\n\n\nAPI Map Query (left) and File Query (right)"
  },
  {
    "objectID": "intro-api.html#using-mosaiks-features-for-prediction",
    "href": "intro-api.html#using-mosaiks-features-for-prediction",
    "title": "3  Access MOSAIKS",
    "section": "\n3.4 Using MOSAIKS features for prediction",
    "text": "3.4 Using MOSAIKS features for prediction\n\n\n\n\n\n\nThis is a brief overview. Detailed instructions appear later in the manual.\n\n\n\nBasic workflow:\n\nObtain ground truth measurements (“labels”)\nDownload matching features via Map/File Query\nSpatially merge labels and features\nUse regression to model relationship\n\n\n\nUsing MOSAIKS, a simplified workflow design.\n\nYou can experiment with various machine learning approaches in the regression step. For beginners, we recommend starting with our example Jupyter notebook (Chapter 4) that demonstrates a simple ridge regression approach (suitable for both R and Python users).\nThis topic will be covered in greater depth in later chapters. In the next chapter, you will see a simple MOSAIKS workflow which starts from the point of having both features from the API and ground truth labels."
  },
  {
    "objectID": "intro-api.html#citation-requirements",
    "href": "intro-api.html#citation-requirements",
    "title": "3  Access MOSAIKS",
    "section": "\n3.5 Citation requirements",
    "text": "3.5 Citation requirements\nWhen referring to the MOSAIKS methodology or when generating MOSAIKS features, please reference Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\nYou can use the following Bibtex:\n@article{article,\n    author = {Rolf, Esther and Proctor, Jonathan and Carleton, Tamma and Bolliger, Ian and Shankar, Vaishaal and Ishihara, Miyabi and Recht, Benjamin and Hsiang, Solomon},\n    year = {2021},\n    month = {07},\n    pages = {},\n    title = {A generalizable and accessible approach to machine learning with global satellite imagery},\n    volume = {12},\n    journal = {Nature Communications},\n    doi = {10.1038/s41467-021-24638-z}\n}\nIf using features downloaded from this website, please reference, in addition to the publication above, the MOSAIKS API.\nYou can cite the API using the following Bibtex:\n @misc{MOSAIKS API,\n    author = {{Carleton, Tamma and Chong, Trinetta and Druckenmiller, Hannah and Noda, Eugenio and Proctor, Jonathan and Rolf, Esther and Hsiang, Solomon}},\n    title = {{Multi-Task Observation Using Satellite Imagery and Kitchen Sinks (MOSAIKS) API}},\n    howpublished = \"\\url{ https://api.mosaiks.org }\",\n    version = {1.0},\n    year = {2022},\n}\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will have a chance to try MOSAIKS."
  },
  {
    "objectID": "intro-demo.html#overview",
    "href": "intro-demo.html#overview",
    "title": "4  Try MOSAIKS",
    "section": "\n4.1 Overview",
    "text": "4.1 Overview\nThis demo replicates key results from the original MOSAIKS publication (Rolf et al. 2021), using a subset of the data to work within Google Colab’s free tier memory constraints. While MOSAIKS was developed to improve access to satellite-based prediction in data-sparse environments, particularly in developing countries, the original paper focused on demonstrating performance in the United States where high-quality training data was readily available.\nThe US served as an ideal testing ground for several reasons:\n\nExtensive ground truth data available across multiple variables\nReliable spatial referencing of data\nDiverse landscapes and built environments\nAbility to benchmark against existing methods\nSystematic validation of predictions\n\nThis validation in a data-rich environment was crucial for establishing MOSAIKS as a reliable tool before deploying it in contexts where ground truth data is scarce or unreliable. The demo showcases MOSAIKS predicting several variables:\n\nForest cover\nElevation\nPopulation density\nNighttime lights\nIncome\nRoad length\nHousing prices"
  },
  {
    "objectID": "intro-demo.html#demo",
    "href": "intro-demo.html#demo",
    "title": "4  Try MOSAIKS",
    "section": "\n4.2 Demo",
    "text": "4.2 Demo\nThe notebook demonstrates:\n\nLoading pre-computed MOSAIKS features and labels\nMerging the features and labels\nTraining a ridge regression model\nEvaluating predictions\nVisualizing results\n\nThe data used is a 50% random sample of both features (K=4,000 instead of 8,192) and observations (N=50,000 instead of 100,000) compared to the original paper. Despite using this reduced dataset, the demo still achieves strong predictive performance, highlighting MOSAIKS’s efficiency.\n\n\n\n\n\n\nClick the badge to run the demo in Google Colab!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓\n↑↑↑↑↑↑↑↑↑↑↑↑↑\nRemember to click File -&gt; Save a copy in Drive to save any changes you make.\n\n\nFor instruction on how to use Google Colab, please refer to Chapter 1"
  },
  {
    "objectID": "intro-demo.html#dont-want-to-run-code",
    "href": "intro-demo.html#dont-want-to-run-code",
    "title": "4  Try MOSAIKS",
    "section": "\n4.3 Don’t want to run code?",
    "text": "4.3 Don’t want to run code?\nConsider watching this demonstration instead!\n\n\nFigure 4.1: An overview of MOSAIKS and a live demonstration of generating novel predictions using the system. Video recoreded by CIGAR Generalized Planetary Remote Sensing - 2020 Convention session. Presented by Esther Rolf and Tamma Carleton."
  },
  {
    "objectID": "intro-demo.html#whats-next",
    "href": "intro-demo.html#whats-next",
    "title": "4  Try MOSAIKS",
    "section": "\n4.4 What’s next?",
    "text": "4.4 What’s next?\nAfter establishing MOSAIKS’s capabilities in the US context, the MOSAIKS development team have successfully demonstrated the system in many additional settings. This includes on the global scale, or in settings with few or low quality data. In the coming chapters, we will explore some of these applications, showing how MOSAIKS can help address data gaps in regions where traditional data collection is challenging or costly.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section we will take a closer look at the label data that can be used with MOSAIKS."
  },
  {
    "objectID": "labels.html#ground-observations",
    "href": "labels.html#ground-observations",
    "title": "Label Data",
    "section": "Ground observations",
    "text": "Ground observations\nResolution\nThe preparation of label data for MOSAIKS hinges on multiple factors, particularly spatial information such as location, extent, and resolution. If the observations in a dataset have a higher resolution than 1 km², some form of selection or aggregation is required. Label data can come in any native spatial form: raster, point, polygon, or vector. As long as there is spatial information associated with each label data observation, these data can be joined to MOSAIKS imagery features for downstream prediction.\n\n\n\n\n\n\nThe MOSAIKS API is designed to predict outcomes at scales of 1 km² or larger. Customizable solutions are possible for higher resolution problems.\n\n\n\n\n\nFigure 1: Examples of label data formats that can be easily integrated into the MOSAIKS pipeline. Label data of any spatial format that can be aggregated to at least the scale of 1km² (or larger) can be used directly in combination with MOSAIKS imagery features for downstream prediction tasks. Examples shown here are from Rolf et al. (2021) and include: forest cover, elevation, population, and nighttime lights datasets (all raster format); income data (polygon format); road length (vector format); and housing price data (point format).\n\nSample size\nAs with many machine learning algorithms, a large sample size often results in higher performance than low. MOSAIKS has been used and shown to be effective with a wide range of sample sizes (N). The sample size for model training is determined by the spatial and temporal resolution of your label data. For example, when predicting maize yields in the US using ground data from one year of farmer-level surveys in the US, N=3,143 if farmers are geolocated based on their county (even if far more than 3,143 farmers were interviewed, as this is the number of counties in the US). Additional time periods increase sample size for training, but also require more up-front costs, as more imagery need to be “featurized”.\nThe original MOSAIKS publication (Rolf et al., 2021) tested performance for forest cover, income, housing price, population density, nighttime luminosity, and elevation using sample sizes ranging from 60,000 to 100,000, but showed that performance fell only modestly when N shrank to just a few hundred observations. Consistent with this finding, in recent experiments with crop yield, we see high with only around 400 observations (R² = 0.80). It is important to note that the original crop yield dataset included interview data from thousands of farmers across the study country, and this messy data was cleaned and aggregated to the district level prior to modeling. In this case, a clean dataset with a low number of observations was preferred to a large but noisy dataset. Despite the low sample size of the aggregate data, performance was still comparable to more complex CNN models trained specifically on crop yield.\n\n\n\n\n\n\nAs a rule of thumb, we recommend a sample size of at least 300 observations for training a MOSAIKS model, though every application is unique and may require more or fewer observations.\n\n\n\nData types\nMOSAIKS accommodates both continuous (e.g., fraction of area forested) and discrete (e.g., presence/absence of water) labels, with data type influencing model development and testing. Performance metric selection is also determined by data type - for continuous variables, we typically use the coefficient of determination (R²), while for discrete variables, the area under the curve value from the receiver operator characteristic curve (ROC AUC score) is used.\nSummary checklist\nFor optimal use with MOSAIKS, label data should be:\n\n\nConsistently geolocated as point, polygon, vector, or raster data\n\nAggregatable to ≥1km² resolution\n\nObservable in daytime satellite imagery (directly or indirectly)\n\nRecent or slow-changing if using current API features\n\nSample size N≥300 (larger samples generally perform better)\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore more than 100 specific examples of labels that have been tested with MOSAIKS at varying levels of success."
  },
  {
    "objectID": "labels-100-maps.html#overview",
    "href": "labels-100-maps.html#overview",
    "title": "5  What labels work?",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\nMOSAIKS is a designed to be useful for predicting anything one might see in satellite imagery. Some things are easier to predict than others, but the system is designed to be flexible. For instance, some variables can be seen directly in the imagery itself, such as forest cover. Other outcomes can only be seen through proxy variables like income and housing price which are not directly visible in the imagery but can be estimated by looking at objects in imagery that are correlated with them, such as roads, houses, and cars.\n\n\nFigure 5.1: MOSAIKS versatility makes it the perfect tool for a wide range of applications.\n\nIn this chapter we will explore over 100 different outcomes that have been tested with MOSAIKS. We will discuss the results of a selection of these outcomes in detail, and provide a brief overview of the rest."
  },
  {
    "objectID": "labels-100-maps.html#one-hundred-maps",
    "href": "labels-100-maps.html#one-hundred-maps",
    "title": "5  What labels work?",
    "section": "\n5.2 One hundred maps",
    "text": "5.2 One hundred maps\n\n5.2.1 Original publication\nIn Rolf et al. 2021, the authors tested MOSAIKS on 7 outcomes: forest cover, income, housing price, population density, nighttime luminosity, and elevation. The study area is focused in the continental United States. This is an excellent starting point for understanding the capabilities of MOSAIKS as the data quality is high for a diverse set of outcomes. While the results showed significant promise and demonstrated the potential of MOSAIKS, the true test is in the application to new outcomes and new geographies.\n\n5.2.2 Going global\nTo test the global applicability of MOSAIKS, across a diverse set of outcomes, there were 2 primary things that needed to happen:\n1. The creation of a global set of features\n\n\nFigure 5.2: Planet Labs visual basemap imagery from quarter 3 of 2019 (left) and 4 of 4,000 MOSAIKS features downloaded from the API (right).\n\n2. Collecting and currating a large set of outcomes with diversity in spatial structures and categories\n\n\nTable 5.1: The authors selected 115 variables across 10 categories and set to work testing each in the MOSAIKS system.\n\n\n\n\n\n\nCategory\nNumber of Labels\nExample Label\n\n\n\nAgricultural Assets\n5\nAgricultural land ownership\n\n\nAgriculture\n16\nMaize yield\n\n\nBuilt Infrastructure\n9\nBuildings\n\n\nDemographics\n5\nMedian age\n\n\nEducation\n10\nExpected years of schooling\n\n\nHealth\n15\nMalaria in children\n\n\nHousehold Assets\n21\nMobile phones\n\n\nIncome\n9\nHuman development index\n\n\nNatural Systems\n8\nTree cover\n\n\nOccupation\n17\nUnemployment\n\n\n\n\nWith this data in hand, we were able to devise a few simple questions to test:\n\nWhich variables can be effectively measured?\nWhat are the most compelling applications?\nWhat are the modes of failure?"
  },
  {
    "objectID": "labels-100-maps.html#results",
    "href": "labels-100-maps.html#results",
    "title": "5  What labels work?",
    "section": "\n5.3 Results",
    "text": "5.3 Results\n\n5.3.1 Overall performance\nThe results of the 100 maps experiment are shown in the scatter plot below (Figure 5.3). Each point in each scatter sub-plot represents a location in the study for a given label. The x-axis is the observed value of the label, and the y-axis is the MOSAIKS-predicted value. The diagonal 45° line in each sub-plot represents perfect prediction. The coefficient of determination (R²) is used here as the primary measure of accuracy.\nA few broad insights stand out:\n\nSubstantial variation: Even within the same category, we see varying degrees of predictive power. For instance, in the “Agriculture” category, some labels (such as high-level yield averages) are predicted quite accurately, while others (like certain niche crops or management practices) remain more elusive.\nCategory differences: Some categories have consistently higher R² scores. For instance, “Natural Systems” (e.g., tree cover) often score better because the patterns are more directly visible from above—think of large, contiguous forest areas contrasted with open fields or urban centers. On the other hand, “Occupation” or “Demographics” include variables (like unemployment rates) that are largely socio-economic in nature, requiring more indirect and subtle cues.\nFailure cases: A few outcomes show near-random performance, suggesting that the satellite imagery features alone are insufficient to capture their spatial patterns, or that the signals are drowned out by noise (see Failures below).\n\n\n\nFigure 5.3: The results of the 100 maps experiment with the x-axis shows the observed values of the outcome, while the y-axis shows the predicted values. Each point in each scatter is a location from the study. The diagonal line (45°) represents perfect prediction. Performance is measured with the coefficient of determination (R²).\n\nIn the box plot (Figure 5.4) we see the distribution of R² values for each category across all 115 labels. This confirms the wide range of performance. Categories such as “Agriculture,” “Income,” and “Natural Systems” tend to have higher median R² values; categories such as “Health” and “Occupation” show more varied or lower overall performance.\n\n\nFigure 5.4: The results of the 100 maps experiment.\n\nThis heterogeneity underscores an important takeaway: MOSAIKS is not a one-size-fits-all solution. Some phenomena lend themselves to easier detection via satellite data than others. Still, the ability to simultaneously handle over 100 different outcomes from a single feature set is itself a testament to MOSAIKS’ flexibility and global applicability.\n\n5.3.2 Successes\n\n5.3.2.1 Maize yield\nA standout example of a high-performing label is Maize yield (Figure 5.5). This outcome is naturally suited to detection by satellite imagery:\n\n\nDirect visual signal: Agricultural fields have characteristic features, including crop texture, canopy cover, and phenological (growth stage) patterns, all of which can be captured in the spectral and spatial signals from satellite images.\n\n\n\nSpatial contiguity: Large, contiguous fields of maize reduce noise and enable easier extraction of relevant features.\n\nIn the left-hand scatter plot of Figure 5.5, the predicted yield values match well with the observed values, often clustering along the 45° line. On the right, we see that visually identifiable patterns in maize-growing regions are clearly reflected in the predicted maps. This strong alignment highlights how MOSAIKS can quickly yield robust predictions for outcomes that are clearly manifested in the satellite imagery.\n\n\nFigure 5.5: Perfoprmance of MOSAIKS on Maize yield, showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\nCrop yields are a classic use case for remote sensing because farmland is often large, geographically dispersed, and subject to rapid changes from weather and management practices—conditions that satellite imagery can routinely monitor at scale. By measuring vegetation indices (e.g., NDVI, EVI), researchers gain insight into plant health, canopy density, and photosynthetic activity, all of which correlate strongly with agricultural productivity. This non-invasive, timely, and spatially comprehensive approach makes it invaluable for crop forecasting, detecting stress, and guiding resource allocation. Consequently, remote sensing has become a cornerstone in modern yield estimation methods for staple crops around the world. MOSAIKS is a natural extension of this trend, leveraging the latest in machine learning to extract actionable insights from satellite imagery.\n\n\nFigure 5.6: Agricultural fields in the United States Midwest region. This examples shows the clear delineation of fields with varying color intensities, making for easily detectable features in the satellite imagery. Source: NASA\n\n\n5.3.2.2 International wealth index (IWI)\nAnother notable success is the International Wealth Index (IWI; Figure 5.7). This composite measure of household wealth is derived from a variety of indicators, such as housing quality, access to services, and ownership of durable goods. While wealth itself is not directly visible from space, the underlying factors that contribute to it often are. For example, wealthier areas tend to have more developed infrastructure, larger homes, and more vehicles—all of which leave distinct signatures in satellite imagery.\n\n\nFigure 5.7: Performance of MOSAIKS on the International Wealth Index (IWI), showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\nDespite being a composite measure of socioeconomic status, the IWI’s underlying indicators—housing conditions, access to utilities, and asset ownership—often manifest in the built environment as features that satellites capture well. For instance, wealthier neighborhoods typically exhibit a higher density of substantial buildings, paved roads, formal layouts, and visible amenities (e.g., swimming pools, parked vehicles). These cues translate into distinctive patterns in the spectral and spatial data extracted by MOSAIKS’ features. Furthermore, infrastructure development and housing materials (like metal roofs versus thatch) can produce detectable differences in reflectance, making it easier for the algorithm to discern socioeconomic gradients.\n\n\n\n\n\n\nThis highlights one of MOSAIKS’ core advantages: even when the target variable isn’t directly “visible,” the system can tease out its proxies from wide-ranging visual cues, leading to robust predictions of wealth indices around the globe.\n\n\n\n\n5.3.3 Failures\nIn contrast, certain labels show extremely low R² values, effectively indicating no predictive power under this approach. One notable example is the presence of underground pipelines Figure 5.8. Unlike forests or agricultural fields, pipeline infrastructure is typically hidden from direct visual inspection—often located entirely underground or obscured in ways that do not provide surface indicators distinguishable in imagery (Figure 5.9).\n\nLack of visible features: There is no spectral or structural cue (e.g., coloration, texture, shape) that reliably indicates the presence of a pipeline.\nIndirect clues Are unreliable: One might speculate that pipelines could follow roads or distinct corridors, but these correlations vary widely by region and do not consistently appear in the imagery.\nSignal-to-noise ratio: In many areas, the pipeline corridor may appear visually indistinguishable from farmland or other vegetation, leaving little to no unique satellite signature.\n\nAs a result, MOSAIKS has little chance to identify and learn features predictive of such hidden infrastructure. This stands in stark contrast to more visually prominent targets like maize fields or tree cover.\n\n\nFigure 5.8: Where it fails\n\n\n\nFigure 5.9: Why it fails - buried"
  },
  {
    "objectID": "labels-over-time.html",
    "href": "labels-over-time.html",
    "title": "\n6  Time series data\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\nFor time series data, the frequency of observation and rate of change should also be considered when selecting satellite imagery to create features. For instance, during a region’s rainy season it may be much more difficult to obtain high quality cloud free images. This has the potential to leave large gaps in the feature data and may require imputation to fill these gaps.\nTake crop yield for example. The data is typically annual, but the seasonal conditions can have a massive impact on yield. To capture seasonal variation, monthly or quarterly satellite features can be used with yearly labels. This would mean that each observation has K features for each time step t (K*t features). Let’s say we have 1,000 features computed monthly, then our crop yield would be predicted by 12,000 features. An important point of this data is that there are consistent locations with regular yield measurements."
  },
  {
    "objectID": "labels-data-prep.html#joining-data",
    "href": "labels-data-prep.html#joining-data",
    "title": "\n7  Preparing labels\n",
    "section": "\n7.1 Joining data",
    "text": "7.1 Joining data\nBefore model training, label data must be joined to imagery features. The joined data should be tabular with each row containing:\n\n\nLabel - the observed value\n\n\nGeographic location - such as a MOSAIKS grid cell or larger geographic area\n\n\nTime (optional) - useful for time series data (year, mont, or day)\n\n\nK feature columns - column for every random satellite feature (typically k = 4,000)\n\nFor example, a dataset with crop yeild data could look like:\n\n\nObservation\nDistrict\nYear\nCrop Yield\n\n\n\n1\nChibombo\n2019\n1.520\n\n\n2\nKabwe\n2019\n1.878\n\n\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n\n\n\nAnd a dataset with district level aggregate features could look like:\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n10.6\n1.1\n…\n2.2\n\n\n\nFinally, the joined dataset would look like:\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n1.520\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n1.878\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n10.6\n1.1\n…\n2.2\n\n\n\nIn the above example, our geographic location is the district and our label is the crop yield (mt/ha). We then have K columns containing the features and N observations.\nIn this example, our features started at 1 km² resolution and were aggregated to the district level to match the crop yield data. To join this data, we first found all of the feature locations that fall within the district boundaries using a spatial join. Then we averaged the features within each district. This allowed us to have a single feature vector for each district. The resulting tabular data is ready for modeling.\nNote: Add example code for joining data"
  },
  {
    "objectID": "labels-survey.html",
    "href": "labels-survey.html",
    "title": "8  Survey Data",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\nNotes:\n\nWhy survey data needs it own section\nTypes of survey data\nHow to access survey data\nWorking with survey data\n\nLSMS data\nDHS data\n\n\nRemote sensing informed survey design"
  },
  {
    "objectID": "mosaiks-demo-2.html",
    "href": "mosaiks-demo-2.html",
    "title": "\n9  Demo Number 2\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\nIDK yet"
  },
  {
    "objectID": "interactive-agriculture.html",
    "href": "interactive-agriculture.html",
    "title": "\n10  Agriculture example\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "satellite.html#overview",
    "href": "satellite.html#overview",
    "title": "Satellite Imagery",
    "section": "Overview",
    "text": "Overview\nThis part of the book provides some measure of direction on how to navigate the overwhelming world of satellite imagery. The main focus is how to choose imagery and considerations for proceessing with a focus on using the data in the MOSAIKS framework. However many of the concepts covered will be broadly applicapble to geospatial information science that utilizes remote sensing technologies."
  },
  {
    "objectID": "satellite.html#moving-past-the-mosaiks-api",
    "href": "satellite.html#moving-past-the-mosaiks-api",
    "title": "Satellite Imagery",
    "section": "Moving past the MOSAIKS API",
    "text": "Moving past the MOSAIKS API\nBlurb about when to think about getting satellite imagery (i.e. when are API features no longer enough).\nBit of info on why EO satellites are needed, what they have been used for, and what they could be used for."
  },
  {
    "objectID": "satellite.html#section-outline",
    "href": "satellite.html#section-outline",
    "title": "Satellite Imagery",
    "section": "Section Outline",
    "text": "Section Outline\n\nWhat considerations need to be made when choosing imagery.\nHow to process satellite imagery"
  },
  {
    "objectID": "satellite-imagery.html#time-range",
    "href": "satellite-imagery.html#time-range",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.1 Time range",
    "text": "11.1 Time range\nHow long a satellite or satellite constellation have been operating.\n\n\nSource: NASA"
  },
  {
    "objectID": "satellite-imagery.html#sensor-resolution-pixel-size",
    "href": "satellite-imagery.html#sensor-resolution-pixel-size",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.2 Sensor resolution (pixel size)",
    "text": "11.2 Sensor resolution (pixel size)\nAnother important part of picking the best imagery is the resolution of the satellite sensor. This should be guided by intuition of what scale is necessary for the satellite to detect your labels, as well as availability of imagery. For example, tree cover may not require the same resolution of imagery to detect effectively as crop yield.\n\n\nSource: Radiant Earth"
  },
  {
    "objectID": "satellite-imagery.html#spectral-resolution-em-range",
    "href": "satellite-imagery.html#spectral-resolution-em-range",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.3 Spectral resolution (EM range)",
    "text": "11.3 Spectral resolution (EM range)\nThe avaialable electromagnetic radiation spectrum of the sensor\n\n\nSource: Radiant Earth"
  },
  {
    "objectID": "satellite-imagery.html#temporal-resolution-revisit-rate",
    "href": "satellite-imagery.html#temporal-resolution-revisit-rate",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.4 Temporal resolution (revisit rate)",
    "text": "11.4 Temporal resolution (revisit rate)\nSingle satellite revisit vs constellations\n\n\nSource: Radiant Earth"
  },
  {
    "objectID": "satellite-imagery.html#cloud-cover",
    "href": "satellite-imagery.html#cloud-cover",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.5 Cloud cover",
    "text": "11.5 Cloud cover\nData quality can also be affected by cloud cover. This may affect your choice of satellite. For instance, if you require monthly features, the slower revisit time of Landsat may mean many months will lack coverage due to cloud cover limiting your ability to create high quality features."
  },
  {
    "objectID": "satellite-imagery.html#sensor-type",
    "href": "satellite-imagery.html#sensor-type",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.6 Sensor Type",
    "text": "11.6 Sensor Type\nActive vs passive sensors\nVisual range, NIR, etc - surface reflectance\nSAR, night/day clouds ok\n\n\nSource: Radiant Earth"
  },
  {
    "objectID": "satellite-imagery.html#sec-satellite-public",
    "href": "satellite-imagery.html#sec-satellite-public",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.7 Public Satellites",
    "text": "11.7 Public Satellites\n\nSentinel-1\nSentinel-2\nLandsat 8\nMODIS\nVIIRS\nASTER"
  },
  {
    "objectID": "satellite-imagery.html#sec-satellite-private",
    "href": "satellite-imagery.html#sec-satellite-private",
    "title": "\n11  Choosing Imagery\n",
    "section": "\n11.8 Private Satellites",
    "text": "11.8 Private Satellites\n\nWorldView\n\nPlanet\n\nSkySat\nRapidEye\n\n\n\nnote: find a place for this text, taken out of labels section from Togo doc\nHowever, satellite features can be created at varying temporal scales. Publicly available satellite imagery such as Landsat and Sentinel offer a rich time series, but their use with MOSAIKS will require custom feature generation. The label data’s timestep will play an important role in determining the satellite imagery required for computing features.\nThe simplest implementation uses composite satellite images that are highly processed to remove clouds and low quality pixels, and is often normalized and color balanced. This is available directly on the MOSAIKS API."
  },
  {
    "objectID": "satellite-processing.html",
    "href": "satellite-processing.html",
    "title": "\n12  Processing Imagery\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\nDownload vs cloud processing"
  },
  {
    "objectID": "features.html#kitchen-sinks",
    "href": "features.html#kitchen-sinks",
    "title": "Satellite Features",
    "section": "Kitchen sinks?",
    "text": "Kitchen sinks?\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. Whenever we present this system, people always ask us, “ah, what about kitchen sinks?” The answer stems from the phrase “everything but the kitchen sink,” which means “almost everything imaginable.” If you were to hear someone describe making a sandwhich with “everything but the kitchen sink,” you would know that they used every ingredient at their disposal. You might also hear the phrase “everything and the kitchen sink,” which means “everything imaginable.” For MOSAIKS, I like the idea of “everything but the kitchen sink” because we are not extracting every last drop of information from the imagery, but it implies that we are getting a lot out of the imagery. Additionally, if it helps, you can think of the imagery as something we are not taking along with us, rather just the useful bits we extracted.\n\n\nFigure 1: Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by Grace Lewin.\n\nThis idea of leaving behind the imagery is one of the reasons MOSAIKS is so powerful. It means that for most users, they never have to sufer through the issues that arise from dealing with large amounts of satellite imagery. We (the MOSAIKS team) take the imagery, extract a bunch of random convolutional features, and then we leave the imagery behind. We don’t need to know what the imagery looks like, we don’t need to interpret what any feature is, we just need to know that we have a bunch of features that we can use to predict some outcome. Then users can download these features and use them to predict their outcomes.\nIn this section of the book, we leave this simplistic aspect of MOSAIKS behind and focus on what that imagery looks like, how we extract features from it, and attempt to provide some intuition for what these features are.\n\n\n\n\n\n\nFrom here on out, we will refer to the random convolutional features as either RCFs or features. You should consider these terms interchangeable in the context of MOSAIKS."
  },
  {
    "objectID": "features.html#why-use-rcfs",
    "href": "features.html#why-use-rcfs",
    "title": "Satellite Features",
    "section": "Why use RCFs?",
    "text": "Why use RCFs?\nTraditional convolutional neural networks (CNNs)\nTo learn why we use random convolutional features (RCFs), we need to understand how traditional convolutional neural networks (CNNs) work. CNNs are a type of neural network that are designed to work with images. They are made up of layers of neurons that are connected in a way that allows them to learn features from the images. In the case of CNNs, these features are learned through a process called backpropagation. Backpropagation is a method for training neural networks that uses the chain rule of calculus to calculate the gradient of the loss function with respect to the weights of the network. This gradient is then used to update the weights of the network in a way that minimizes the loss function. This means that the network is learning to recognize features in the images that are important for the task it is trying to solve. It is therfore using minimization to learn the features that are important for the task.\n\n\nFigure 2: A simplified diagram showing a typical convolutional nueral network model architecture used for\n\nReplacing minimization with randomization in learning\nMOSAIKS takes a different approach to learning features from images. Instead of using backpropagation to learn the features, we use random convolutional filters to extract features from the images. The resultant features are therfore as random as the filters applied to the images. This means that the features are not optimized for any particular task, but are instead a random sample of the information in the images. This is a key difference between RCFs and traditional CNNs. Traditional CNNs learn features that are optimized for the task they are trying to solve, while RCFs extract features that are random samples of the information in the images. This means that RCFs are much faster to compute and can be used to extract features from large amounts of imagery in a short amount of time.\n\n\nFigure 3: Rolf et al. 2021 Figure 1: MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context."
  },
  {
    "objectID": "features.html#random-convolutional-features-rcfs",
    "href": "features.html#random-convolutional-features-rcfs",
    "title": "Satellite Features",
    "section": "Random convolutional features (RCFs)",
    "text": "Random convolutional features (RCFs)\nThe core of MOSAIKS is the random convolutional features (RCFs) that we extract from satellite imagery. The extraction process is simple: we take a bunch of random convolutional filters and apply them to the imagery. Each convolutional filter can be a tensor of randomly generated values (gaussian patches), or they can be sampled from the imagery itself (empirical patches; Figure 4).\n\n\nFigure 4: Rolf et al. 2021 Figure 1 C: A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. K patches are randomly sampled from across the N images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image.\n\nLets stay with the idea of empiricial features for a moment. If we take a convolutional filter and apply it to the imagery, we get a new image. This new image is the result of the convolutional filter “looking” at the imagery and “seeing” something. The filter might be looking for edges, or colors, or textures, or patterns, or anything else. The filter might be looking for something that we can’t even see with our eyes. After the convolution is applied, a non-linear activation function transforms the result. In this case, we use the rectified linear unit (ReLU) activation function. ReLU is a simple function that takes the maximum of the input and zero. This means that if the input is negative, the output is zero, and if the input is positive, the output is the input. After this, we use an adaptive average pooling layer to reduce the size of the image. This is a simple operation that takes the average of the pixels in a region of the image and outputs a single value. This value is our feature for this filter.\nIntuition for RCFs\nHere we will take a look at a simplified cartoon representation of what a RCF might be looking for in the imagery. We will use a few simple 3x3 filters drawn directly from a single image."
  },
  {
    "objectID": "features-api.html#feature-data",
    "href": "features-api.html#feature-data",
    "title": "13  API Features",
    "section": "\n13.1 Feature data",
    "text": "13.1 Feature data\n\n\n\n\n\n\nCreating satellite image features is covered in later chapters.\n\n\n\n\n13.1.1 RCF brief overview\nMOSAIKS relies on random convolutional features (RCFs) computed from satellite imagery. The random nature of the features means that they are task-agnostic. These features can be reused multiple times to model various tasks. Features are generally created over a standard grid of 0.01 by 0.01 degree cells (~1 km², depending on latitude). The output is a feature vector of length K for every grid location.\n\n\n\n\n\n\nFeatures can be computed at higher or lower resolutions than 1 km². This is discussed in Chapter 14.\n\n\n\n\n13.1.2 Feature aggrgegation\nThe default 1 km² resolution of MOSAIKS is generally the maximum resolution the label data should be in. If the labels are in lower resolution, the satellite features can be aggregated up to larger areas to match. Typical aggregation might be to larger grid cell, census tract, county/district, or state/province levels. The exact aggregation level is contingent on the spatial resolution of the label data.\n\n\nFigure 13.1: Example showing of 3 representative random convolutional features (rows). Features are downloaded from the MOSAIKS API at 0.01° resolution (the native resolution) and aggregated to 3 levels, including (A) larger grid cells (0.1°), (B) counties, and (C) states.\n\n\n13.1.3 API Features\nCurrently the MOSAIKS API has a single set of global features ready to download. The features are publicly available for download; this is the fastest and easiest way to begin using MOSAIKS. They come from Planet Labs, Inc. Basemaps product Global Quarterly 2019 (qaurter 3) which has a native resolution of 4.77 m in the red, green, and blue bands of the visual spectrum. Given this, the easiest way to get started with MOSAIKS is to have label data for a recent time period (ideally from 2019 for fast changing labels, or a close year for more steady labels). To use MOSAIKS for time series data is possible, just not currently with precomputed and publicly available features. To access API features see Chapter 3\n\n\nTable 13.1: The MOSAIKS API offers features at 0.01°, 0.1°, and 1° resolution for the entire globe. Additionally, we provide features aggregated to the ADM0 (country), ADM1 (state/province), and ADM2 (county/municipality) levels. All features share the same 4,000 feature columns, and are all created from the same Planet Labs imagery, the only difference is the resolution/aggregation level. At each level of aggregation, we offer area weighted features and population weighted features. Population weighted features use weights from the Gridded Population of the World (GPWv4) population density dataset.\n\nResolution\nTime Period\nPopulation Weighted\nArea Weighted\nDownload\n\n\n\n0.01°\n2019 Q3\n\n\nQuery\n\n\n0.1°\n2019 Q3\n✓\n✓\nChunked files\n\n\n1°\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM2\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM1\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM0\n2019 Q3\n✓\n✓\nSingle file"
  },
  {
    "objectID": "features-computing.html",
    "href": "features-computing.html",
    "title": "14  Computing Features",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\nDepending on the spatial scale and resolution of the label data, subsampling the MOSAIKS grid may be appropriate to reduce computation time and cost."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Task Modeling",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "model-choice.html",
    "href": "model-choice.html",
    "title": "\n15  Model Choice\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "model-spatial.html",
    "href": "model-spatial.html",
    "title": "\n16  Filling spatial data gaps\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "model-temporal.html",
    "href": "model-temporal.html",
    "title": "\n17  Filling temporal data gaps\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Model Uncertainty",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "uncertainty-intro.html",
    "href": "uncertainty-intro.html",
    "title": "\n18  Overview of uncertainty\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "uncertainty-ethics.html",
    "href": "uncertainty-ethics.html",
    "title": "\n19  Ethical implications\n",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete."
  }
]