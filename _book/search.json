[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIKS Training Manual",
    "section": "",
    "text": "Welcome\nThis is the first iteration of the MOSAIKS Training Manual! The manual serves as a comprehensive reference for understanding MOSAIKS, its capabilities, and practical implementation guidance.\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. It is a framework designed to simplify the use of satellite imagery and machine learning for analyzing socioeconomic and environmental outcomes across different geographic contexts and time periods.\nThis comprehensive two-week program is designed for academics, professionals, and policymakers interested in leveraging MOSAIKS to better understand socioeconomic and environmental challenges. The course is particularly valuable for those working in:\nThroughout this course, you’ll learn practical applications through a combination of:\nThe curriculum covers the complete MOSAIKS workflow:\nWhether you’re new to MOSAIKS or looking to deepen your expertise, this course provides the tools and knowledge needed to effectively utilize this powerful framework.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "MOSAIKS Training Manual",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMOSAIKS was developed and is supported by researchers across multiple partner organizations:\nCore Team:\nBenjamin Recht, Esther Rolf, Tamma Carleton, Eugenio Noda, Hikari Murayama, Hannah Druckenmiller, Ian Bolliger, Jeanette Tseng, Jessica Katz, Jonathan Proctor, Luke Sherman, Miyabi Ishihara, Simon Greenhill, Solomon Hsiang, Taryn Fransen, Trinetta Chong, Vaishaal Shankar, Graeme Blair, Darin Christensen, Shopnavo Biswas, Karena Yan, Cullen Molitor, Grace Lewin, Steven Cognac, Juliet Cohen\nPartner Organizations:\n\nCenter for Effective Global Action (CEGA; UCB)\nEnvironmental Markets Lab (emLab; UCSB)\nGlobal Policy Lab (GPL; Stanford University)\nHarvard Data Science Initiative (HDSI; Harvard University)\nProject on Resources and Governance (PRG; UCLA)\nMaster of Environmental Data Science Program (MEDS; UCSB)\n\nFunding Support:\n\nThe Patrick J. McGovern Foundation\nThe United States Agency for International Development (USAID)\nUnited Nations Development Programme (UNDP)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html",
    "href": "part-00-intro/00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Course structure\nThis course is designed as an intensive two-week program that combines lectures, demonstrations, and hands-on sessions. Each day is structured as follows:\nEach day concludes with a Q&A and feedback session from 4:30-5:00, providing opportunities to clarify concepts and share ideas. It is expected that this first course will spur many new ideas and concepts which should be included in the future trainings. Please remember to take notes throughout each day with particular emphasis in areas you think could be explained better or that need additional topics covered. These can be areas that you struggled with or that you would anticipate could be difficult for others.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#course-structure",
    "href": "part-00-intro/00-intro.html#course-structure",
    "title": "Introduction",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n9:00 - 10:30\nMorning Session 1\n\n\n10:30 - 11:00\nBreak\n\n\n11:00 - 12:30\nMorning Session 2\n\n\n12:30 - 1:30\nLunch\n\n\n1:30 - 3:00\nAfternoon Session 1\n\n\n3:00 - 3:30\nBreak\n\n\n3:30 - 4:30\nAfternoon Session 2\n\n\n4:30 - 5:00\nFeedback and Development",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#schedule-overview",
    "href": "part-00-intro/00-intro.html#schedule-overview",
    "title": "Introduction",
    "section": "Schedule overview",
    "text": "Schedule overview\n\nCourse schedule by week\n\n\n\n\n\n\n\nDay\nWeek 1\nWeek 2\n\n\n\n\nMonday\nOrientation and introduction to MOSAIKS\nHoliday - No Class\n\n\nTuesday\nGround labels and data processing fundamentals\nTask modeling and machine learning applications\n\n\nWednesday\nAgriculture applications and MOSAIKS API\nUnderstanding uncertainty in MOSAIKS\n\n\nThursday\nSatellite imagery fundamentals and processing\nSurvey data processing and design\n\n\nFriday\nDeep dive into random convolutional features (RCFs)\nFuture directions and advanced applications\n\n\n\nMain topics covered each day:\n\nWeek 1\n\nDay 1: MOSAIKS framework, accessing features, basic workflow\nDay 2: Understanding ground truth data, data cleaning, spatial resolution\nDay 3: Agricultural yield prediction, downloading features via API\nDay 4: Types of satellite imagery, processing considerations, quality control\nDay 5: Feature computation, theory behind RCFs, hands-on implementation\n\n\n\nWeek 2\n\nDay 1: Martin Luther King Jr. Day - No Class\nDay 2: Model selection, hyperparameter tuning, cross-validation\nDay 3: Error analysis, confidence intervals, reporting uncertainty\nDay 4: Survey design principles, geolocation methods, sampling strategies\nDay 5: Advanced topics, emerging applications, future development",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#training-expectations",
    "href": "part-00-intro/00-intro.html#training-expectations",
    "title": "Introduction",
    "section": "Training expectations",
    "text": "Training expectations\n\nWhat you will learn\n\nUnderstanding of MOSAIKS framework and capabilities\nPractical skills in satellite imagery processing\n\nExperience with machine learning applications\nHands-on practice with real-world datasets\nKnowledge of survey data integration\nBest practices for model implementation\n\n\n\nPrerequisites\nThere are no explicit prerequisites, though this course does cover some advanced topics in:\n\nThe Python programming language\nMachine learning\nGeospatial data\n\n\n\nParticipant expectations\n\nActive participation in discussions and hands-on sessions\nCompletion of assigned homework (particularly the Week 1 Friday assignment)\n\nEngagement in Q&A sessions\nContribution to feedback sessions for course improvement\n\n\n\nComputing requirements\nThe course includes hands-on computing sessions. You will need:\n\nA computer with access to the internet\nA Google account\nAccess to Google Colaboratory\nAccess to necessary data (details to be provided)\n\n\n\nHomework and presentations\nThere will be a homework assignment at the end of Week 1, which participants will present on Tuesday of Week 2. This assignment is designed to reinforce learning and provide practical experience with MOSAIKS tools.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html",
    "href": "part-00-intro/01-intro-compute.html",
    "title": "1  Compute setup",
    "section": "",
    "text": "1.1 Requirements\nThis course will primarily use Google Colaboratory (Colab) for our computational needs. Colab is a free, cloud-based platform that allows you to write and execute Python code through your browser. It comes with many pre-installed libraries and provides free access to computing resources, including GPUs.\nTo participate in the coding portions of this course, you’ll need:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#requirements",
    "href": "part-00-intro/01-intro-compute.html#requirements",
    "title": "1  Compute setup",
    "section": "",
    "text": "A laptop or desktop computer\nReliable internet connection\nA Google account (if you don’t have one, create one at accounts.google.com)\nA modern web browser (Chrome recommended)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#getting-started-with-google-colab",
    "href": "part-00-intro/01-intro-compute.html#getting-started-with-google-colab",
    "title": "1  Compute setup",
    "section": "1.2 Getting started with Google Colab",
    "text": "1.2 Getting started with Google Colab\n\n1.2.1 Accessing Colab\n\nGo to colab.research.google.com\nSign in with your Google account\nClick “New Notebook” to create your first Colab notebook\n\n\n\n1.2.2 Understanding the interface\nThe Colab interface is similar to Jupyter notebooks, with a few key components:\n\nMenu Bar: Contains File, Edit, View, Insert, Runtime, Tools, and Help options\nToolbar: Quick access to common actions like adding code/text cells\nCell Area: Where you write and execute code or text\nRuntime Status: Shows the state of your notebook’s connection to Google’s servers\n\n\n\n1.2.3 Basic operations\n\nCreating Cells:\n\nCode cells: Click “+ Code” or use Ctrl+M B\nText cells: Click “+ Text” or use Ctrl+M M\n\nRunning Cells:\n\nClick the play button next to the cell\nUse Shift+Enter\nSelect Runtime &gt; Run all from the menu\n\nCell Types:\n\nCode cells: For Python code execution\nText cells: For documentation (supports Markdown)\n\n\n\n\n1.2.4 Important features\n\nRuntime Type:\n\nClick Runtime &gt; Change runtime type\nSelect Python 3 as the runtime\nFor GPU access: Change hardware accelerator to GPU when needed\n\nFile Management:\n\nFiles uploaded to Colab are temporary\nConnect to Google Drive for persistent storage:\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nPackage Installation:\n\nInstall additional packages using:\n\ncondapip\n\n\n# add a -c conda-forge to select the conda-forge channel\n# add a -q flag to install quietly (reduced output)\n# add a -y flag to prememptively accept other changes\n!conda install package_name\n\n\n!pip install package_name\n\n\n\n\n\n1.2.5 Best practices\n\nSave Your Work:\n\nRegularly save to Google Drive (File &gt; Save a copy in Drive)\nDownload important notebooks locally as backups\n\nResource Management:\n\nClose unused notebooks to free up resources\nBe aware of idle timeouts (notebooks disconnect after extended inactivity)\n\nMemory Usage:\n\nMonitor memory usage through Runtime &gt; Resource usage\nUse Runtime &gt; Factory reset runtime if you run into memory issues\n\n\n\n\n1.2.6 Keyboard shortcuts\nHere are the most useful keyboard shortcuts for working in Colab:\n\nWindows/LinuxMac\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nRun current cell\nCtrl+Enter\n\n\nRun cell and move to next\nShift+Enter\n\n\nRun cell and insert below\nAlt+Enter\n\n\nInsert code cell above\nCtrl+M A\n\n\nInsert code cell below\nCtrl+M B\n\n\nConvert to text cell\nCtrl+M M\n\n\nConvert to code cell\nCtrl+M Y\n\n\nDelete current cell\nCtrl+M D\n\n\nToggle line numbers\nCtrl+M L\n\n\nToggle output\nCtrl+M O\n\n\nCut cell\nCtrl+M X\n\n\nCopy cell\nCtrl+M C\n\n\nPaste cell below\nCtrl+M V\n\n\nSelect multiple cells\nShift+Up/Down\n\n\nFind and replace\nCtrl+F\n\n\nSave notebook\nCtrl+S\n\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nRun current cell\n⌘+Enter\n\n\nRun cell and move to next\nShift+Enter\n\n\nRun cell and insert below\nOption+Enter\n\n\nInsert code cell above\n⌘+M A\n\n\nInsert code cell below\n⌘+M B\n\n\nConvert to text cell\n⌘+M M\n\n\nConvert to code cell\n⌘+M Y\n\n\nDelete current cell\n⌘+M D\n\n\nToggle line numbers\n⌘+M L\n\n\nToggle output\n⌘+M O\n\n\nCut cell\n⌘+M X\n\n\nCopy cell\n⌘+M C\n\n\nPaste cell below\n⌘+M V\n\n\nSelect multiple cells\nShift+Up/Down\n\n\nFind and replace\n⌘+F\n\n\nSave notebook\n⌘+S\n\n\n\n\n\n\nYou can view all available shortcuts in Colab by pressing Ctrl+M H (⌘+M H on Mac) or through Help &gt; Keyboard shortcuts in the menu.\n\n\n1.2.7 Common issues and solutions\n\nRuntime Disconnections:\n\nClick “Reconnect” when prompted\nYour variables will be reset, but saved code remains\n\nPackage Installation Issues:\n\nRestart runtime after installing new packages\nUse Runtime &gt; Restart runtime\n\nMemory Errors:\n\nClear unnecessary variables\nRestart runtime\nConsider using smaller data samples during development\n\n\n\n\n\n\n\n\nMemory errors are common when working with large datasets or complex models on the free tier of Colab. If you encounter these issues, consider using a paid version of Colab or connecting a Google Cloud Platform vitual machine (VM).\n\n\n\n\n\n1.2.8 Getting help\n\nAccess Colab’s built-in documentation: Help &gt; Colab Overview\nView keyboard shortcuts: Help &gt; Keyboard shortcuts\nSearch the Help menu for specific topics\nUse the Help &gt; Search Solutions feature",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#ai-assistance-in-colab",
    "href": "part-00-intro/01-intro-compute.html#ai-assistance-in-colab",
    "title": "1  Compute setup",
    "section": "1.3 AI assistance in Colab",
    "text": "1.3 AI assistance in Colab\nGoogle Gemini is a powerful AI assistant that is seemlesly integrated with Google Colab. You can use it to generate code, comments, or markdown text to improve your notebooks. Gemini can be accessed in several ways in Colab to help you with your work, all start by selecting the Gemini icon in different parts of the notebook editor.\n\n\n\n\n\n\nGemini icon\n\n\n\n\nLook for this icon to indicate where you can click to access Gemini in Colab.\n\n\nHere are a few ways you can use Google Gemini effectively in Colab:\n\n1.3.1 Chat support\nClick the “Gemini” button in the top-right corner to open a chat interface where you can ask questions about your code, debug issues, or get explanations of concepts. This option is especially useful for beginners or for complex problems.\n\n\n1.3.2 Code generation\nUse the “Generate code” option (the sparkle icon) above any empty code cell to generate new code based on your description. You can ask it to many different things including:\n\nLoading a dataset called my_data.csv\nPlot a histogram of the data\nBuild a model to predict y from X\n\n\n\n1.3.3 Code explanation\nUse the “Explain code” option (the sparkle icon) above any complete code cell to open a chat interface that will automatically explain the code in the cell. This is useful for understanding code written by someone else, learning new concepts you are not familiar with, or getting a second opinion on your work.\n\n\n1.3.4 Code completion\nColab provides intelligent autocomplete as you type:\n\nPress Tab to accept suggestions\nUse Ctrl+Space (Cmd+Space on Mac) to manually trigger suggestions\nGet real-time documentation and parameter hints\n\n\n\n\n\n\n\nWhile these AI tools are helpful, always review and understand the code they suggest before using it in your work.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#accessing-course-notebooks",
    "href": "part-00-intro/01-intro-compute.html#accessing-course-notebooks",
    "title": "1  Compute setup",
    "section": "1.4 Accessing course notebooks",
    "text": "1.4 Accessing course notebooks\nAll course notebooks are hosted on GitHub and can be accessed directly in Google Colab. There are two ways to access the notebooks:\n\n1.4.1 Method 1: Direct links\nEach section of this book includes direct “Open in Colab” links for relevant notebooks. Simply click the badge to open the notebook:\nExample \n\n\n1.4.2 Method 2: Clone the notebook\nTo choose a notebook from the repository (Add link to demo/interactive notebooks here):\n\nOpen Google Colab (colab.research.google.com)\nClick File &gt; Open Notebook\nSelect the GitHub tab\nEnter the repository URL: https://github.com/[username]/[repo] (UPDATE WITH REPO)\nSelect the notebook you want to open\n\n\n\n1.4.3 Saving your work\nWhen you open a notebook from GitHub in Colab, it creates a temporary copy. To save your work:\n\nClick File &gt; \"Save a copy in Drive\"\nThis creates your own editable copy in your Google Drive\nAll future changes will be saved to your copy\n\n\n\n1.4.4 Notebook organization\nThe course notebooks are organized into:\n\ndemos/: Complete demonstration notebooks\nexercises/: Interactive notebooks with exercises to complete\nsolutions/: Complete versions of exercise notebooks\n\nEach notebook includes:\n\nClear instructions in markdown cells\nCode cells with examples or exercises\nTO DO sections for exercises\nValidation cells to check your work",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#data-access-and-management",
    "href": "part-00-intro/01-intro-compute.html#data-access-and-management",
    "title": "1  Compute setup",
    "section": "1.5 Data access and management",
    "text": "1.5 Data access and management\nThere are several ways to access data in Colab notebooks. Here are the main approaches:\n\n1.5.1 Direct downloads\nFor data hosted on repositories like Zenodo, you can download directly using wget:\n# Set the data directory\ndata_dir = \"LSMS-ISA-data\"\n\n# Download the data\n!wget https://zenodo.org/records/14040658/files/Data.zip\n\n# Unzip and organize\n!unzip Data.zip\n!mv Data {data_dir}\n!rm Data.zip\n\n\n1.5.2 Google Drive integration\nFor data stored in Google Drive:\n\nFirst mount your Google Drive:\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nAccess your data using the mounted path:\n\ndrive_path = \"/content/drive/MyDrive/project_folder\"\n\nFor better performance, copy data to local VM storage:\n\n\n\n\n\n\n\nRemember that the VM’s storage is temporary - files will be deleted when the runtime disconnects. Always keep a backup of your data in Drive or another permanent storage location.\n\n\n\nimport os\nimport shutil\n\n# Create local directory\nlocal_dir = \"/content/data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Copy data from Drive to VM\ndrive_data = os.path.join(drive_path, \"my_data\") \nshutil.copytree(drive_data, local_dir, dirs_exist_ok=True)\n\n\n1.5.3 Why copy data to the VM?\nWhen working with data in Colab, copying files from Google Drive to the virtual machine (VM) can significantly improve performance:\n\nFaster Access: Reading directly from Google Drive requires data to be transferred over the network for each operation. Local VM storage provides much faster read/write speeds.\nReduced Latency: Network latency between Colab and Google Drive can slow down operations that require multiple data accesses. Local data eliminates this latency.\nMore Reliable: Network connectivity issues or Drive access problems won’t interrupt your analysis once data is copied locally.\nBetter for Iterative Processing: If your code needs to read the same data multiple times (like in machine learning training loops), local access is much more efficient.\n\nFor example, reading a 1GB dataset from Drive might take 30 seconds, while reading from local VM storage could take just a few seconds. The time spent copying data once at the start of your session can save significant time during analysis. This can be especially true in a notebook environment when a user is developing code and may need to access and reaccess the data multiple times.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will take a closer look at MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html",
    "href": "part-00-intro/02-intro-mosaiks.html",
    "title": "2  What is MOSAIKS?",
    "section": "",
    "text": "2.1 The challenge\nRight now, numerous public satellite systems collect huge amounts of data about the world every day. But there is so much imagery (terabytes per day) that it’s overwhelming to sort through by hand; and it’s too complex and unstructured to be usable in its raw form for most applications.\nThat is why linking satellite imaging to machine learning (SIML) is incredibly powerful. It enables vast amounts of unstructured image data to be transformed into structured information that can immediately be used for planning, research, and decision-making.\nWe believe that people all over the world should be able to access SIML technologies, but we also recognize that most people who would benefit the most from these tools don’t have the time or resources to manage enormous satellite imagery data sets and learn how to apply machine learning to them.\nINSERT IMAGE HERE",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#the-solution",
    "href": "part-00-intro/02-intro-mosaiks.html#the-solution",
    "title": "2  What is MOSAIKS?",
    "section": "2.2 The solution",
    "text": "2.2 The solution\nThat’s why we developed MOSAIKS.\nMOSAIKS is designed to work “out of the box” for a wide array of SIML applications, for people with no SIML expertise who work on normal desktop computers. MOSAIKS users never actually have to touch satellite imagery themselves and only need to have basic statistical training.\n\n\n\n\n\n\nIf you can run a regression, you can use MOSAIKS.\n\n\n\nMOSAIKS empowers users to create their own new data sets from satellite imagery. We don’t control what variables users look at, and we never need to know. MOSAIKS is a system that allows users to quickly transform vast amounts of imagery into maps of new variables, using their own training data.\nIf you’ve ever been curious about trying machine learning with satellite imagery, but don’t know anything about machine learning or satellite imagery, MOSAIKS is for you.\nAnd if you know a lot about machine learning and satellite imagery, MOSAIKS might still be for you, since it performs competitively with deep learning methods but is much simpler and cheaper to use.\nINSERT IMAGE HERE",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#how-mosaiks-works",
    "href": "part-00-intro/02-intro-mosaiks.html#how-mosaiks-works",
    "title": "2  What is MOSAIKS?",
    "section": "2.3 How MOSAIKS works",
    "text": "2.3 How MOSAIKS works\nThe basic idea of MOSAIKS is to seperate users from the costly and difficult process of transforming imagery into inputs (called “features”) for a machine learning algorithm (images → X). We do that part, so users never have to download or manage imagery. Users then download a table of MOSAIKS features (X), link them to their own geocoded data on the outcome (Y) they are interested in (called “labels”) and run a linear regression to predict their labels using MOSAIKS features (Y = Xβ).\n\n\n\nRolf et al. 2021 Figure 1\n\n\nAll users use the same MOSAIKS features and just match them to their labels based on location. Users can run their analysis on any statistical software they are comfortable with. For most applications, the computing demands will not require users to work with specialized machines, since desktops and laptops work.\nMOSAIKS works because MOSAIKS features capture a huge amount of information about the colors, patterns and textures that show up in satellite imagery. We don’t know what patterns/colors/textures will be important for the application that users have (since we don’t know what applications users will try), so we just try to capture all of them. The purpose of the regression step is to teach the model which patterns/colors/textures predict the labels, and then to use that understanding to make predictions in locations where users don’t have labels. In addition, MOSAIKS encodes image information in a way that allows for nonlinear relationships between labels and images, even though the regression that users implement is a linear regression.\n\n\n\n\n\n\nFigure 2.1: Example of 4 (of 4,000) MOSAIKS feature maps (right) computed from satellite imagery (left). These features were chosen at random from what is available on the MOSAIKS API.\n\n\n\n\n2.3.1 Five steps to using MOSAIKS\nFor users, the procedure for using MOSAIKS has five steps (corresponding figure from Rolf et al. is below):\n\nDownload MOSAIKS features (X) in the areas where you have labels.\nMerge the features with your labels (Y) based on location (so features at position P are linked to labels at position P).\nRun a ridge regression of your labels on the MOSAIKS features (Y = Xβ).\nEvaluate performance.\nUse the results of the regression model (β) to make predictions (Xβ) in a new region of interest where you do not have labels, using only the MOSAIKS features that correspond with those new locations.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#what-can-mosaiks-predict",
    "href": "part-00-intro/02-intro-mosaiks.html#what-can-mosaiks-predict",
    "title": "2  What is MOSAIKS?",
    "section": "2.4 What can MOSAIKS predict?",
    "text": "2.4 What can MOSAIKS predict?\n\n\n\n\n\n\nThis question is answered in greater detail in Chapter 5\n\n\n\nMOSAIKS has been successfully used to predict a wide range of outcomes including:\n\nEnvironmental conditions (forest cover, elevation)\nPopulation patterns (density, nighttime lights)\nEconomic indicators (income, house prices)\nInfrastructure (road networks)\n\nThe figure below is from the original MOSAIKS publication (Rolf et al. 2021). The left maps show the input labels. The right map shows the modeled predictions. The scatter plot shows the modeled predictions against the true labels and reports the coefficient of determination (R²) as a measure of performance.\n\n\n\nRolf et al. 2021 Figure 2\n\n\nImportantly, all these predictions use the same set of satellite features - there’s no need to reprocess the imagery for different tasks. MOSAIKS achieves accuracy comparable to more complex deep learning methods, but at a fraction of the computational cost. This is the power of MOSAIKS, it removes the need for repreocessing the imagery after the initial encoding.\n\n2.4.1 Building understanding\nIf you are interested in using MOSAIKS, you can also see our tutorial and slide deck. And if you have a little bit of time, we recommend reading the paper we wrote when we introduced the system. We wrote it with users in mind, so we tried to make it as clear and accessible as possible.\nIf you use MOSAIKS, please reference: Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will demonstrate MOSAIKS with real data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html",
    "href": "part-00-intro/03-intro-api.html",
    "title": "3  Access MOSAIKS",
    "section": "",
    "text": "3.1 Introduction\nAt its core, MOSAIKS requires two main inputs: satellite features and ground truth data. Our aim is to make these features as accessible as possible so that the majority of users do not have to worry about the technical details of satellite imagery processing.\nTo this end, we have worked to develop multiple ways to access MOSAIKS features:\nThe MOSAIKS API should be considered the primary way to access features. It is a user-friendly interface that allows you to download features for any location on Earth. The API is designed to be accessible to users with a range of technical backgrounds, from beginners to experts.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#introduction",
    "href": "part-00-intro/03-intro-api.html#introduction",
    "title": "3  Access MOSAIKS",
    "section": "",
    "text": "Option\nImagery Source\nSpatial Coverage\nSpatial Resolution\nTemporal Resolution\nLink\n\n\n\n\nMOSAIKS API\nPlanet imagery\nGlobal land areas\n0.01°, 0.1°, 1°, ADM2, ADM1, ADM0\n2019 Q3 (static layer)\nMOSAIKS API\n\n\nRolf et al 2021\nGoogle Earth basemap\nContinental United States (~100k locations)\n0.01°\n2019 (static layer)\nCode Ocean Capsule\n\n\nCompute features\nAny imagery source\nUser-defined\nUser-defined\nUser-defined\nChapter 13\n\n\n\n\n\n\n\n\n\nThe first part of this book focuses on the MOSAIKS API and publicly available features. The second part will cover how to compute features from your own imagery.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#api-overview",
    "href": "part-00-intro/03-intro-api.html#api-overview",
    "title": "3  Access MOSAIKS",
    "section": "3.2 API Overview",
    "text": "3.2 API Overview\n\n\n\n\n\n\nMOSAIKS API Link\n\n\n\napi.mosaiks.org\n\n\nWe have put together a [Resource Page](https://api.mosaiks.org/portal/resources/ for MOSAIKS users (registration required), which includes example Python and R notebooks for using the pipeline.\nDon’t forget to see our Tutorial Page, which has an example Python notebook that we walk through in the video.\nIf you are looking for new data sets that we create using MOSAIKS (not features), we will be posting those here as they become available.\n\n3.2.1 Register for an account\nVisit api.mosaiks.org.\n\n\n\nLogin page\n\n\nSelect Register to create an account. You’ll need to provide:\n\n\n\nRegistration page\n\n\nOnce registered, you can log in to access the MOSAIKS features and begin downloading data.\n\n\n\nLanding page\n\n\n\n\n3.2.2 Downloading features\nThe MOSAIKS features are organized using a 0.01 x 0.01 degree latitude-longitude global grid, centered at .005 degree intervals.\nWhen you download features, you’ll receive them in a tabular .csv format where:\n\nEach row represents a unique grid cell\nThe first two columns contain latitude and longitude coordinates\nThe remaining columns represent K features (currently K = 4000 features)\n\nImportant notes about downloads:\n\nFiles remain available for download for 15 days\nAfter 15 days, files are automatically deleted from the system\nThere is a limit of 100,000 records per query\n\n\n3.2.2.1 Ways to query features\nThere are two main methods to obtain features through the API:\n\nMap Query\n\n\nCreate rectangular boxes by specifying latitude and longitude coordinates\nMultiple boxes can be created\nThe system displays an estimated number of records for each box\nNote that estimates are based on box area and may not reflect actual record numbers, especially for areas containing seas and oceans\n\n\nFile Query\n\n\nSubmit a file with custom latitude and longitude coordinates\nThe API returns features for grid cells closest to your input coordinates\nPoints are allocated to the nearest grid point if they don’t exactly match\nThe output file may have a different number of rows than your input\nPoint ordering may change in the output\n\n\n\n\nAPI Map Query (left) and File Query (right)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#using-mosaiks-features-for-prediction",
    "href": "part-00-intro/03-intro-api.html#using-mosaiks-features-for-prediction",
    "title": "3  Access MOSAIKS",
    "section": "3.3 Using MOSAIKS features for prediction",
    "text": "3.3 Using MOSAIKS features for prediction\n\n\n\n\n\n\nThis is a brief overview. Detailed instructions appear later in the manual (#sec-model-choice).\n\n\n\nBasic workflow:\n\nObtain ground truth measurements (“labels”)\nDownload matching features via Map/File Query\nSpatially merge labels and features\nUse regression to model relationship\n\nYou can experiment with various machine learning approaches in the regression step. For beginners, we recommend starting with our example Jupyter notebook (Chapter 4) that demonstrates a simple ridge regression approach (suitable for both R and Python users).\n\n\n\nUsing MOSAIKS, a simplified workflow design.\n\n\nThis topic will be covered in greater depth in later chapters. In the next chapter, you will see a simple MOSAIKS workflow which starts from the point of having both features from the API and ground truth labels.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#citation-requirements",
    "href": "part-00-intro/03-intro-api.html#citation-requirements",
    "title": "3  Access MOSAIKS",
    "section": "3.4 Citation requirements",
    "text": "3.4 Citation requirements\nWhen referring to the MOSAIKS methodology or when generating MOSAIKS features, please reference Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\nYou can use the following Bibtex:\n@article{article,\n    author = {Rolf, Esther and Proctor, Jonathan and Carleton, Tamma and Bolliger, Ian and Shankar, Vaishaal and Ishihara, Miyabi and Recht, Benjamin and Hsiang, Solomon},\n    year = {2021},\n    month = {07},\n    pages = {},\n    title = {A generalizable and accessible approach to machine learning with global satellite imagery},\n    volume = {12},\n    journal = {Nature Communications},\n    doi = {10.1038/s41467-021-24638-z}\n}\nIf using features downloaded from this website, please reference, in addition to the publication above, the MOSAIKS API.\nYou can cite the API using the following Bibtex:\n @misc{MOSAIKS API,\n    author = {{Carleton, Tamma and Chong, Trinetta and Druckenmiller, Hannah and Noda, Eugenio and Proctor, Jonathan and Rolf, Esther and Hsiang, Solomon}},\n    title = {{Multi-Task Observation Using Satellite Imagery and Kitchen Sinks (MOSAIKS) API}},\n    howpublished = \"\\url{ https://api.mosaiks.org }\",\n    version = {1.0},\n    year = {2022},\n}\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will have a chance to try MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html",
    "href": "part-00-intro/04-intro-demo.html",
    "title": "4  Try MOSAIKS",
    "section": "",
    "text": "4.1 Overview\nThis demo replicates key results from the original MOSAIKS publication (Rolf et al. 2021). While MOSAIKS great potential to improve access to satellite-based prediction in data-sparse environments, the original paper focused on demonstrating performance in the United States where high-quality training data was readily available.\nThe US served as an ideal testing ground for several reasons:\nThis validation in a data-rich environment was crucial for establishing MOSAIKS as a reliable tool before deploying it in contexts where ground truth data is scarce or unreliable.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#overview",
    "href": "part-00-intro/04-intro-demo.html#overview",
    "title": "4  Try MOSAIKS",
    "section": "",
    "text": "Extensive ground truth data available across multiple variables\nReliable spatial referencing of data\nDiverse landscapes and built environments\nAbility to benchmark against existing methods\nSystematic validation of predictions",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#demonstration-code",
    "href": "part-00-intro/04-intro-demo.html#demonstration-code",
    "title": "4  Try MOSAIKS",
    "section": "4.2 Demonstration code",
    "text": "4.2 Demonstration code\nBelow is a link to a Jupyter notebook intented to demonstrate practical use of MOSAIKS with real data. In fact, this notebook uses the original input data and features from Rolf et al. 2021. The code demonstrates:\n\nLoading pre-computed MOSAIKS features and labels\nMerging the features and labels\nTraining a ridge regression model\nEvaluating predictions\nVisualizing results The demo showcases MOSAIKS predicting several variables:\n\n\nForest cover, elevation, population density, nighttime lights, income, & road length\n\nA user simply needs to select which variable they would like to predict, and no other changes need to be made to the code.\nTo stay within the Colab free tier limits of memory usage, we subset the data. We take a 50% random sample of both features (K=4,000 instead of 8,192) and observations (N=50,000 instead of 100,000) compared to the original paper. Despite using this reduced dataset, the demo still achieves strong predictive performance, highlighting MOSAIKS’s efficiency.\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n\n Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#dont-want-to-run-code",
    "href": "part-00-intro/04-intro-demo.html#dont-want-to-run-code",
    "title": "4  Try MOSAIKS",
    "section": "4.3 Don’t want to run code?",
    "text": "4.3 Don’t want to run code?\nConsider watching this demonstration instead!\n\n\n\n\n\n\nFigure 4.1: An overview of MOSAIKS and a live demonstration of generating novel predictions using the system. Video recoreded by CIGAR Generalized Planetary Remote Sensing - 2020 Convention session. Presented by Esther Rolf and Tamma Carleton.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#whats-next",
    "href": "part-00-intro/04-intro-demo.html#whats-next",
    "title": "4  Try MOSAIKS",
    "section": "4.4 What’s next?",
    "text": "4.4 What’s next?\nAfter establishing MOSAIKS’s capabilities in the US context, the MOSAIKS development team have successfully demonstrated the system in many additional settings. This includes on the global scale, or in settings with few or low quality data. In the coming chapters, we will explore some of these applications, showing how MOSAIKS can help address data gaps in regions where traditional data collection is challenging or costly.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section we will take a closer look at the label data that can be used with MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html",
    "href": "part-01-labels/00-labels.html",
    "title": "Label data",
    "section": "",
    "text": "Overview\nThis section explores the ground truth data (labels) used with MOSAIKS. While the system is designed to be flexible in the types of outcomes it can predict, understanding what makes good label data and how to prepare it properly is crucial for success.\nLabel data represents the “truth” that MOSAIKS attempts to predict - whether that’s crop yields, population density, economic indicators, or any other variable that might be visible (directly or indirectly) in satellite imagery. The quality and characteristics of this label data significantly influence model performance.",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#what-makes-good-label-data",
    "href": "part-01-labels/00-labels.html#what-makes-good-label-data",
    "title": "Label data",
    "section": "What makes good label data?",
    "text": "What makes good label data?\nFor optimal performance with MOSAIKS, label data should have several key characteristics:\n\nAccurate geographic location information\nAppropriate spatial resolution (typically ≥1km²)\nReasonable temporal alignment with imagery\nSufficient sample size (generally ≥300 observations)\nObservable connection to surface features",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#section-outline",
    "href": "part-01-labels/00-labels.html#section-outline",
    "title": "Label data",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key considerations for working with label data in MOSAIKS:\n5  What labels work?\n\nExamples of successful applications\nUnderstanding performance patterns\nLimits of predictability\n\n6  Survey data\n\nWorking with survey data\nSampling considerations\nPrivacy and ethical concerns\nIntegration challenges\n\n7  Preparing labels\n\nData cleaning procedures\nSpatial aggregation methods\nQuality control steps\nJoining with features\n\n8  Label data demo\n\nHands-on examples\nCommon pitfalls\nBest practices\nPerformance evaluation\n\nThese chapters provide both practical guidance for preparing your own label data and deeper understanding of what types of outcomes MOSAIKS can effectively predict.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore over 100 different outcomes that have been tested with MOSAIKS, examining what works well and what doesn’t.",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html",
    "href": "part-01-labels/01-labels-100-maps.html",
    "title": "5  What labels work?",
    "section": "",
    "text": "5.1 Overview\nMOSAIKS is a designed to be useful for predicting anything one might see in satellite imagery. Some things are easier to predict than others, but the system is designed to be flexible. For instance, some variables can be seen directly in the imagery itself, such as forest cover. Other outcomes can only be seen through proxy variables like income and housing price which are not directly visible in the imagery but can be estimated by looking at objects in imagery that are correlated with them, such as roads, houses, and cars.\nIn this chapter we will explore over 100 different outcomes that have been tested with MOSAIKS. We will discuss the results of a selection of these outcomes in detail, and provide a brief overview of the rest.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#overview",
    "href": "part-01-labels/01-labels-100-maps.html#overview",
    "title": "5  What labels work?",
    "section": "",
    "text": "Figure 5.1: MOSAIKS versatility makes it the perfect tool for a wide range of applications.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "href": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "title": "5  What labels work?",
    "section": "5.2 One hundred maps",
    "text": "5.2 One hundred maps\n\n5.2.1 Original publication\nIn Rolf et al. 2021, the authors tested MOSAIKS on 7 outcomes: forest cover, income, housing price, population density, nighttime luminosity, and elevation. The study area is focused in the continental United States. This is an excellent starting point for understanding the capabilities of MOSAIKS as the data quality is high for a diverse set of outcomes. While the results showed significant promise and demonstrated the potential of MOSAIKS, the true test is in the application to new outcomes and new geographies.\n\n\n5.2.2 Going global\nTo test the global applicability of MOSAIKS, across a diverse set of outcomes, there were 2 primary things that needed to happen:\n1. The creation of a global set of features\n\n\n\n\n\n\nFigure 5.2: Planet Labs visual basemap imagery from quarter 3 of 2019 (left) and 4 of 4,000 MOSAIKS features downloaded from the API (right).\n\n\n\n2. Collecting and currating a large set of outcomes with diversity in spatial structures and categories\n\n\n\n\n\n\n\n\n\n\n\nCategory\nNumber of Labels\nExample Label\n\n\n\n\nAgricultural Assets\n5\nAgricultural land ownership\n\n\nAgriculture\n16\nMaize yield\n\n\nBuilt Infrastructure\n9\nBuildings\n\n\nDemographics\n5\nMedian age\n\n\nEducation\n10\nExpected years of schooling\n\n\nHealth\n15\nMalaria in children\n\n\nHousehold Assets\n21\nMobile phones\n\n\nIncome\n9\nHuman development index\n\n\nNatural Systems\n8\nTree cover\n\n\nOccupation\n17\nUnemployment\n\n\n\n\n\nTable 5.1: The authors selected 115 variables across 10 categories and set to work testing each in the MOSAIKS system.\n\n\n\nWith this data in hand, we were able to devise a few simple questions to test:\n\nWhich variables can be effectively measured?\nWhat are the most compelling applications?\nWhat are the modes of failure?",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#results",
    "href": "part-01-labels/01-labels-100-maps.html#results",
    "title": "5  What labels work?",
    "section": "5.3 Results",
    "text": "5.3 Results\n\n5.3.1 Overall performance\nThe results of the 100 maps experiment are shown in the scatter plot below (Figure 5.3). Each point in each scatter sub-plot represents a location in the study for a given label. The x-axis is the observed value of the label, and the y-axis is the MOSAIKS-predicted value. The diagonal 45° line in each sub-plot represents perfect prediction. The coefficient of determination (R²) is used here as the primary measure of accuracy.\nA few broad insights stand out:\n\nSubstantial variation: Even within the same category, we see varying degrees of predictive power. For instance, in the “Agriculture” category, some labels (such as high-level yield averages) are predicted quite accurately, while others (like certain niche crops or management practices) remain more elusive.\nCategory differences: Some categories have consistently higher R² scores. For instance, “Natural Systems” (e.g., tree cover) often score better because the patterns are more directly visible from above—think of large, contiguous forest areas contrasted with open fields or urban centers. On the other hand, “Occupation” or “Demographics” include variables (like unemployment rates) that are largely socio-economic in nature, requiring more indirect and subtle cues.\nFailure cases: A few outcomes show near-random performance, suggesting that the satellite imagery features alone are insufficient to capture their spatial patterns, or that the signals are drowned out by noise (see Failures below).\n\n\n\n\n\n\n\nFigure 5.3: The results of the 100 maps experiment with the x-axis shows the observed values of the outcome, while the y-axis shows the predicted values. Each point in each scatter is a location from the study. The diagonal line (45°) represents perfect prediction. Performance is measured with the coefficient of determination (R²).\n\n\n\nIn the box plot (Figure 5.4) we see the distribution of R² values for each category across all 115 labels. This confirms the wide range of performance. Categories such as “Agriculture,” “Income,” and “Natural Systems” tend to have higher median R² values; categories such as “Health” and “Occupation” show more varied or lower overall performance.\n\n\n\n\n\n\nFigure 5.4: The results of the 100 maps experiment.\n\n\n\nThis heterogeneity underscores an important takeaway: MOSAIKS is not a one-size-fits-all solution. Some phenomena lend themselves to easier detection via satellite data than others. Still, the ability to simultaneously handle over 100 different outcomes from a single feature set is itself a testament to MOSAIKS’ flexibility and global applicability.\n\n\n5.3.2 Successes\n\n5.3.2.1 Maize yield\nA standout example of a high-performing label is Maize yield (Figure 5.5). This outcome is naturally suited to detection by satellite imagery:\n\nDirect visual signal: Agricultural fields have characteristic features, including crop texture, canopy cover, and phenological (growth stage) patterns, all of which can be captured in the spectral and spatial signals from satellite images.\n\n\n\nSpatial contiguity: Large, contiguous fields of maize reduce noise and enable easier extraction of relevant features.\n\nIn the left-hand scatter plot of Figure 5.5, the predicted yield values match well with the observed values, often clustering along the 45° line. On the right, we see that visually identifiable patterns in maize-growing regions are clearly reflected in the predicted maps. This strong alignment highlights how MOSAIKS can quickly yield robust predictions for outcomes that are clearly manifested in the satellite imagery.\n\n\n\n\n\n\nFigure 5.5: Perfoprmance of MOSAIKS on Maize yield, showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\nCrop yields are a classic use case for remote sensing because farmland is often large, geographically dispersed, and subject to rapid changes from weather and management practices—conditions that satellite imagery can routinely monitor at scale. By measuring vegetation indices (e.g., NDVI, EVI), researchers gain insight into plant health, canopy density, and photosynthetic activity, all of which correlate strongly with agricultural productivity. This non-invasive, timely, and spatially comprehensive approach makes it invaluable for crop forecasting, detecting stress, and guiding resource allocation. Consequently, remote sensing has become a cornerstone in modern yield estimation methods for staple crops around the world. MOSAIKS is a natural extension of this trend, leveraging the latest in machine learning to extract actionable insights from satellite imagery.\n\n\n\n\n\n\nFigure 5.6: Agricultural fields in the United States Midwest region. This examples shows the clear delineation of fields with varying color intensities, making for easily detectable features in the satellite imagery. Source: NASA\n\n\n\n\n\n5.3.2.2 International wealth index (IWI)\nAnother notable success is the International Wealth Index (IWI; Figure 5.7). This composite measure of household wealth is derived from a variety of indicators, such as housing quality, access to services, and ownership of durable goods. While wealth itself is not directly visible from space, the underlying factors that contribute to it often are. For example, wealthier areas tend to have more developed infrastructure, larger homes, and more vehicles—all of which leave distinct signatures in satellite imagery.\n\n\n\n\n\n\nFigure 5.7: Performance of MOSAIKS on the International Wealth Index (IWI), showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\nDespite being a composite measure of socioeconomic status, the IWI’s underlying indicators—housing conditions, access to utilities, and asset ownership—often manifest in the built environment as features that satellites capture well. For instance, wealthier neighborhoods typically exhibit a higher density of substantial buildings, paved roads, formal layouts, and visible amenities (e.g., swimming pools, parked vehicles). These cues translate into distinctive patterns in the spectral and spatial data extracted by MOSAIKS’ features. Furthermore, infrastructure development and housing materials (like metal roofs versus thatch) can produce detectable differences in reflectance, making it easier for the algorithm to discern socioeconomic gradients.\n\n\n\n\n\n\nThis highlights one of MOSAIKS’ core advantages: even when the target variable isn’t directly “visible,” the system can tease out its proxies from wide-ranging visual cues, leading to robust predictions of wealth indices around the globe.\n\n\n\n\n\n\n5.3.3 Failures\nIn contrast, certain labels show extremely low R² values, effectively indicating no predictive power under this approach. One notable example is the presence of underground pipelines Figure 5.8. Unlike forests or agricultural fields, pipeline infrastructure is typically hidden from direct visual inspection—often located entirely underground or obscured in ways that do not provide surface indicators distinguishable in imagery (Figure 5.9).\n\nLack of visible features: There is no spectral or structural cue (e.g., coloration, texture, shape) that reliably indicates the presence of a pipeline.\nIndirect clues Are unreliable: One might speculate that pipelines could follow roads or distinct corridors, but these correlations vary widely by region and do not consistently appear in the imagery.\nSignal-to-noise ratio: In many areas, the pipeline corridor may appear visually indistinguishable from farmland or other vegetation, leaving little to no unique satellite signature.\n\nAs a result, MOSAIKS has little chance to identify and learn features predictive of such hidden infrastructure. This stands in stark contrast to more visually prominent targets like maize fields or tree cover.\n\n\n\n\n\n\nFigure 5.8: Where it fails\n\n\n\n\n\n\n\n\n\nFigure 5.9: Why it fails - buried\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore survey label data in more detail.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html",
    "href": "part-01-labels/02-labels-survey.html",
    "title": "6  Survey data",
    "section": "",
    "text": "6.1 Why does survey data needs its own chapter?\nNotes:\nSurvey data presents unique challenges and opportunities when working with MOSAIKS. Unlike many other data sources that may be consistently gathered through automated systems or administrative records, survey data:",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "href": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "title": "6  Survey data",
    "section": "",
    "text": "Captures detailed household and individual-level information that’s otherwise unobservable\nOften follows complex sampling designs that need special handling\nMay have inconsistent geographic referencing across different surveys\nRequires careful consideration of privacy and ethical concerns\nCan be expensive and time-consuming to collect, making validation of MOSAIKS predictions particularly valuable",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "href": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "title": "6  Survey data",
    "section": "6.2 Types of survey data",
    "text": "6.2 Types of survey data\nSeveral major categories of surveys are commonly used with MOSAIKS:\n\n6.2.1 Household surveys\n\nLiving Standards Measurement Study (LSMS)\nDemographic and Health Surveys (DHS)\nMultiple Indicator Cluster Surveys (MICS)\nLabor force surveys\nNational census data\n\n\n\n6.2.2 Agricultural surveys\n\nAgricultural censuses\nCrop cutting surveys\nFarm management surveys\nAgricultural household surveys\n\n\n\n6.2.3 Economic surveys\n\nEnterprise surveys\nMarket price surveys\nConsumer expenditure surveys",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "href": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "title": "6  Survey data",
    "section": "6.3 Accessing survey data",
    "text": "6.3 Accessing survey data\nSurvey data access varies by source and type:\n\n6.3.1 Public repositories\n\nWorld Bank Microdata Library\nIPUMS International\nDHS Program website\nFAO statistical databases\n\n\n\n6.3.2 National statistical offices\n\nCensus bureaus\nAgricultural ministries\nEconomic agencies\n\n\n\n6.3.3 Research institutions\n\nUniversities\nThink tanks\nResearch organizations",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "href": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "title": "6  Survey data",
    "section": "6.4 Working with survey data",
    "text": "6.4 Working with survey data\n\n6.4.1 LSMS data\nThe Living Standards Measurement Study (LSMS) requires specific considerations:\n\nComplex multi-topic household surveys\nDetailed geographic information\nPanel structure in some countries\nIntegration with agricultural data\nVarying spatial referencing methods\n\nExample of loading and processing LSMS data:\nimport pandas as pd\nfrom pathlib import Path\n\ndef load_lsms_data(data_path, year):\n    \"\"\"\n    Load and process LSMS survey data\n    \n    Parameters:\n    -----------\n    data_path : str\n        Path to LSMS data files\n    year : int\n        Survey year to process\n        \n    Returns:\n    --------\n    DataFrame with processed LSMS data\n    \"\"\"\n    # Load household data\n    hh_data = pd.read_csv(Path(data_path) / f\"household_{year}.csv\")\n    \n    # Load geographic data\n    geo_data = pd.read_csv(Path(data_path) / f\"geographic_{year}.csv\")\n    \n    # Merge datasets\n    merged_data = pd.merge(\n        hh_data, \n        geo_data,\n        on='household_id',\n        how='inner'\n    )\n    \n    return merged_data\n\n\n6.4.2 DHS data\nThe Demographic and Health Surveys (DHS) present unique characteristics:\n\nStandardized across countries\nCluster-based sampling\nDisplaced GPS coordinates\nRich health and demographic indicators\nRegular update cycle\n\nExample of handling DHS displacement:\nimport geopandas as gpd\nimport numpy as np\n\ndef adjust_dhs_coordinates(gdf, displacement=2000):\n    \"\"\"\n    Adjust for DHS coordinate displacement\n    \n    Parameters:\n    -----------\n    gdf : GeoDataFrame\n        DHS data with point geometry\n    displacement : float\n        Buffer distance in meters\n        \n    Returns:\n    --------\n    GeoDataFrame with uncertainty buffers\n    \"\"\"\n    # Create uncertainty buffers\n    gdf['geometry'] = gdf.geometry.buffer(displacement)\n    \n    return gdf",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "href": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "title": "6  Survey data",
    "section": "6.5 Remote sensing informed survey design",
    "text": "6.5 Remote sensing informed survey design\nMOSAIKS can enhance survey design in several ways:\n\n6.5.1 Pre-survey planning\n\nOptimize sampling frame using satellite-derived information\nIdentify areas of interest based on physical characteristics\nStratify sampling based on predicted characteristics\n\n\n\n6.5.2 During survey implementation\n\nValidate location information\nGuide field teams with up-to-date imagery\nMonitor survey progress\n\n\n\n6.5.3 Post-survey analysis\n\nValidate survey responses against satellite indicators\nFill data gaps in hard-to-reach areas\nCreate high-resolution predictions from survey samples\n\nExample of using MOSAIKS features for survey planning:\ndef optimize_sampling_locations(features_df, n_samples, strata_col='predicted_class'):\n    \"\"\"\n    Optimize survey sampling locations using MOSAIKS features\n    \n    Parameters:\n    -----------\n    features_df : DataFrame\n        MOSAIKS features with coordinates\n    n_samples : int\n        Number of sampling locations needed\n    strata_col : str\n        Column name for stratification\n        \n    Returns:\n    --------\n    DataFrame with selected sampling locations\n    \"\"\"\n    # Stratified random sampling\n    samples = features_df.groupby(strata_col).apply(\n        lambda x: x.sample(n=n_samples // len(features_df[strata_col].unique()))\n    )\n    \n    return samples\nThis integration of MOSAIKS with survey data represents a powerful approach for both enhancing traditional survey methods and extending their reach through satellite-based prediction.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll look at practical guidance for preparing label data for use with MOSAIKS, including data cleaning, agregation, and joining to satellite features.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html",
    "href": "part-01-labels/03-labels-data-prep.html",
    "title": "7  Preparing labels",
    "section": "",
    "text": "7.1 Overview\nIn this chapter, we’ll cover the process of preparing labels for use with MOSAIKS features. Labels are the observed values that we want to predict using our model. These could be crop yields, disease incidence, or any other variable of interest. Label data can come in many forms, including point observations, polygon aggregates, or gridded rasters, but the key is that they must have a sptaial component. We take advantage of this spatial information to join the labels with the MOSAIKS features, which are also spatially explicit.\nWhile MOSAIKS can involve many optional steps, its effective utilization necessitates two core components:\nEnsuring the spatial resolution is shared across both datasets and that they contain geographical data suitable for merging is crucial.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#overview",
    "href": "part-01-labels/03-labels-data-prep.html#overview",
    "title": "7  Preparing labels",
    "section": "",
    "text": "Ground observations (labels)\nSatellite features",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "href": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "title": "7  Preparing labels",
    "section": "7.2 Ground observations",
    "text": "7.2 Ground observations\n\n7.2.1 Resolution\n\n\n\n\n\n\nThe MOSAIKS API is designed to predict outcomes at scales of 1 km² or larger. Customizable solutions are possible for higher resolution problems.\n\n\n\nThe preparation of label data for MOSAIKS hinges on multiple factors, particularly spatial information such as location, extent, and resolution. If the observations in a dataset have a higher resolution than 1 km², some form of selection or aggregation is required. Label data can come in any native spatial form: raster, point, polygon, or vector. As long as there is spatial information associated with each label data observation, these data can be joined to MOSAIKS imagery features for downstream prediction.\n\n\n\n\n\n\nFigure 7.1: Examples of label data formats that can be easily integrated into the MOSAIKS pipeline. Label data of any spatial format that can be aggregated to at least the scale of 1km² (or larger) can be used directly in combination with MOSAIKS imagery features for downstream prediction tasks. Examples shown here are from Rolf et al. (2021) and include: forest cover, elevation, population, and nighttime lights datasets (all raster format); income data (polygon format); road length (vector format); and housing price data (point format).\n\n\n\n\n\n7.2.2 Sample size\nAs with many machine learning algorithms, a large sample size often results in higher performance than low. MOSAIKS has been used and shown to be effective with a wide range of sample sizes (N). The sample size for model training is determined by the spatial and temporal resolution of your label data. For example, when predicting maize yields in the US using ground data from one year of farmer-level surveys in the US, N=3,143 if farmers are geolocated based on their county (even if far more than 3,143 farmers were interviewed, as this is the number of counties in the US). Additional time periods increase sample size for training, but also require more up-front costs, as more imagery need to be “featurized”.\nThe original MOSAIKS publication (Rolf et al., 2021) tested performance for forest cover, income, housing price, population density, nighttime luminosity, and elevation using sample sizes ranging from 60,000 to 100,000, but showed that performance fell only modestly when N shrank to just a few hundred observations. Consistent with this finding, in recent experiments with crop yield, we see high with only around 400 observations (R² = 0.80). It is important to note that the original crop yield dataset included interview data from thousands of farmers across the study country, and this messy data was cleaned and aggregated to the district level prior to modeling. In this case, a clean dataset with a low number of observations was preferred to a large but noisy dataset. Despite the low sample size of the aggregate data, performance was still comparable to more complex CNN models trained specifically on crop yield.\n\n\n\n\n\n\nAs a rule of thumb, we recommend a sample size of at least 300 observations for training a MOSAIKS model, though every application is unique and may require more or fewer observations.\n\n\n\n\n\n7.2.3 Data types\nMOSAIKS accommodates both continuous (e.g., fraction of area forested) and discrete (e.g., presence/absence of water) labels, with data type influencing model development and testing. Performance metric selection is also determined by data type - for continuous variables, we typically use the coefficient of determination (R²), while for discrete variables, the area under the curve value from the receiver operator characteristic curve (ROC AUC score) is used.\n\n\n7.2.4 Checklist\nFor optimal use with MOSAIKS, label data should be:\n\nConsistently geolocated as point, polygon, vector, or raster data\nAggregatable to ≥1km² resolution\nObservable in daytime satellite imagery (directly or indirectly)\nRecent or slow-changing if using current API features\nSample size N≥300 (larger samples generally perform better)",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#joining-data",
    "href": "part-01-labels/03-labels-data-prep.html#joining-data",
    "title": "7  Preparing labels",
    "section": "7.3 Joining data",
    "text": "7.3 Joining data\nBefore model training, label data must be joined to imagery features. The joined data should be tabular with each row containing:\n\nLabel - the observed value\n\nGeographic location - such as a MOSAIKS grid cell or larger geographic area\n\nTime (optional) - useful for time series data (year, mont, or day)\n\nK feature columns - column for every random satellite feature (typically k = 4,000)\n\nFor example, a dataset with crop yeild data could look like:\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\n\n\n\n\n1\nChibombo\n2019\n1.520\n\n\n2\nKabwe\n2019\n1.878\n\n\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n\n\n\nAnd a dataset with district level aggregate features could look like:\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n10.6\n1.1\n…\n2.2\n\n\n\nFinally, the joined dataset would look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n1.520\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n1.878\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n10.6\n1.1\n…\n2.2\n\n\n\nIn the above example, our geographic location is the district and our label is the crop yield (mt/ha). We then have K columns containing the features and N observations.\nIn this example, our features started at 1 km² resolution and were aggregated to the district level to match the crop yield data. To join this data, we first found all of the feature locations that fall within the district boundaries using a spatial join. Then we averaged the features within each district. This allowed us to have a single feature vector for each district. The resulting tabular data is ready for modeling.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "href": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "title": "7  Preparing labels",
    "section": "7.4 Data cleaning considerations",
    "text": "7.4 Data cleaning considerations\nBefore joining label data with MOSAIKS features, several key preparation steps should be considered:\n\n7.4.1 Geographic information\n\nCoordinate systems: Ensure coordinates are in a projection consistent with the features (API uses decimal degrees - WGS84) (Figure 7.2)\nSpatial units: Convert all geographic boundaries to same format (e.g., administrative regions)\nResolution matching: Aggregate or disaggregate data to match MOSAIKS grid\nDuplicate checking: Remove duplicate locations within same time period\nMissing coordinates: Handle missing or invalid geographic information\n\n\n\n\n\n\n\nFigure 7.2: Nine small-scale map projections\n\n\n\n\n\n7.4.2 Label quality\n\nOutliers: Identify and handle extreme values\nMissing values: Decide on approach for handling NA/NULL values\nUnits: Convert all measurements to consistent units\nData types: Ensure numeric fields are properly formatted\nRange checks: Verify values fall within expected bounds\n\n\n\n7.4.3 Temporal alignment\n\nTime periods: Match observation dates with feature timestamps\nSeasonality: Account for seasonal patterns in data collection\nAggregation: Consider temporal aggregation needs (e.g., annual averages)\nGaps: Handle missing time periods appropriately",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#data-formats",
    "href": "part-01-labels/03-labels-data-prep.html#data-formats",
    "title": "7  Preparing labels",
    "section": "7.5 Data formats",
    "text": "7.5 Data formats\n\n\n\n\n\n\nWhen working with large datasets, consider using efficient data formats like parquet, feather, GeoTIFF, or Zarr to reduce memory usage and speed up processing.\n\n\n\nMOSAIKS can work with several common spatial data formats:\n\n7.5.1 Point data\nCommon for survey locations or specific measurement sites:\n# Example point data format\npoint_data = pd.DataFrame({\n    'longitude': [-15.416667, -15.308333, -15.200000],\n    'latitude': [28.316667, 28.441667, 28.550000],\n    'year': [2019, 2019, 2019],\n    'yield': [1.52, 1.88, 2.38]\n})\nPoint data is typically joined to the nearest grid cell centroid or aggregated to larger polygons like administrative units (districts, provinces, etc.).\n\n\n7.5.2 Polygon data\nUsed for administrative boundaries or management units:\n# Example polygon data format (GeoJSON-like)\npolygon_data = {\n    'type': 'Feature',\n    'properties': {\n        'district': 'Chibombo',\n        'year': 2019,\n        'yield': 1.52\n    },\n    'geometry': {\n        'type': 'Polygon',\n        'coordinates': [[[...coordinates...]]]\n    }\n}\n\n\n7.5.3 Raster data\nCommon for gridded observations or model outputs:\n# Example raster metadata\nraster_meta = {\n    'driver': 'GTiff',\n    'dtype': 'float32',\n    'nodata': -9999,\n    'width': 1000,\n    'height': 1000,\n    'crs': 'EPSG:4326',\n    'transform': [0.01, 0, -15.5, 0, -0.01, 28.6]\n}",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#joining-procedures",
    "href": "part-01-labels/03-labels-data-prep.html#joining-procedures",
    "title": "7  Preparing labels",
    "section": "7.6 Joining procedures",
    "text": "7.6 Joining procedures\nThe process of joining labels to features depends on your data format:\n\n7.6.1 Point to grid matching\nFor point observations that don’t align perfectly with the MOSAIKS grid:\ndef match_points_to_grid(points_df, grid_size=0.01):\n    \"\"\"\n    Match point observations to MOSAIKS grid cells\n    \n    Parameters:\n    -----------\n    points_df : pandas DataFrame\n        Contains 'longitude' and 'latitude' columns\n    grid_size : float\n        Size of grid cells in degrees (default 0.01 for ~1km)\n        \n    Returns:\n    --------\n    DataFrame with points matched to grid cell centroids\n    \"\"\"\n    # Round coordinates to nearest grid cell center\n    points_df['grid_lon'] = (np.floor(points_df['longitude'] / grid_size) * \n                            grid_size + grid_size/2)\n    points_df['grid_lat'] = (np.floor(points_df['latitude'] / grid_size) * \n                            grid_size + grid_size/2)\n    return points_df\n\n\n7.6.2 Polygon aggregation\nFor labels defined over administrative regions or other polygons:\ndef aggregate_features_to_polygons(features_df, polygons_gdf):\n    \"\"\"\n    Aggregate grid cell features to polygon level\n    \n    Parameters:\n    -----------\n    features_df : pandas DataFrame\n        Grid cell features with 'longitude' and 'latitude'\n    polygons_gdf : geopandas GeoDataFrame\n        Polygon boundaries\n        \n    Returns:\n    --------\n    DataFrame with features averaged within each polygon\n    \"\"\"\n    # Convert features to GeoDataFrame\n    features_gdf = gpd.GeoDataFrame(\n        features_df, \n        geometry=gpd.points_from_xy(features_df.longitude, \n                                  features_df.latitude)\n    )\n    \n    # Spatial join\n    joined = gpd.sjoin(features_gdf, polygons_gdf)\n    \n    # Average features within polygons\n    return joined.groupby('index_right').mean()\n\n\n7.6.3 Raster alignment\nFor gridded data that needs to be aligned with MOSAIKS grid:\ndef align_raster_to_mosaiks(raster_array, raster_transform, \n                           target_transform):\n    \"\"\"\n    Align raster data to MOSAIKS grid\n    \n    Parameters:\n    -----------\n    raster_array : numpy array\n        Source raster data\n    raster_transform : affine.Affine\n        Source raster geotransform\n    target_transform : affine.Affine\n        MOSAIKS grid geotransform\n        \n    Returns:\n    --------\n    Numpy array aligned to MOSAIKS grid\n    \"\"\"\n    # Implement resampling logic here\n    # Could use rasterio.warp.reproject or similar\n    pass",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#final-data-format",
    "href": "part-01-labels/03-labels-data-prep.html#final-data-format",
    "title": "7  Preparing labels",
    "section": "7.7 Final data format",
    "text": "7.7 Final data format\nAfter joining, your data should be in a tabular format ready for modeling:\n# Example final format\nmodel_data = pd.DataFrame({\n    'id': range(n_obs),\n    'longitude': [...],  # Grid cell or polygon centroid\n    'latitude': [...],   # Grid cell or polygon centroid\n    'year': [...],       # Time period if relevant\n    'label': [...],      # Target variable\n    'feature_0': [...],  # First MOSAIKS feature\n    'feature_1': [...],  # Second MOSAIKS feature\n    # ... additional features ...\n    'feature_3999': [...] # Last MOSAIKS feature\n})\nNote: Add example notebook focusing on joining data with different data type examples\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we’ll look at working specifically with survey data, which presents unique challenges for spatial joining and aggregation.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html",
    "href": "part-01-labels/04-labels-demo.html",
    "title": "8  Label data demo",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\nIDK yet\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next part, we will take a look at considerations for choosing and processing satellite imagery.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html",
    "href": "part-02-satellite/00-satellite.html",
    "title": "Satellite imagery",
    "section": "",
    "text": "Overview\nThis section of the book aims to help navigate the complex world of satellite imagery, with a focus on applying these data within the MOSAIKS framework. While MOSAIKS API features are sufficient for many applications, some use cases require working directly with satellite imagery. Understanding how to select and process appropriate imagery becomes crucial in these situations.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#overview",
    "href": "part-02-satellite/00-satellite.html#overview",
    "title": "Satellite imagery",
    "section": "",
    "text": "The skills covered in these chapters are primarily relevant for users who need to generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can skip this section.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "href": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "title": "Satellite imagery",
    "section": "When to look beyond the MOSAIKS API",
    "text": "When to look beyond the MOSAIKS API\nThe MOSAIKS API provides pre-computed features from 2019 Planet Labs imagery. While these features enable a wide range of applications, you may need to work directly with satellite imagery when:\n\nYour analysis requires data from a different time period\nYou need higher spatial or temporal resolution\nYour application requires specific spectral bands\nYou want to validate or compare MOSAIKS features\nYou’re developing new methodologies",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "href": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "title": "Satellite imagery",
    "section": "Earth observation satellites: Past, present and future",
    "text": "Earth observation satellites: Past, present and future\nSatellite-based Earth observation has revolutionized our understanding of the planet since the launch of the first Landsat satellite in 1972. Today, hundreds of Earth observation satellites collect terabytes of imagery daily, supporting applications from weather forecasting to precision agriculture.\nThe variety of available satellite data has grown dramatically, with options including:\n\nFree public satellites (Landsat, Sentinel)\nCommercial providers (Planet Labs, Maxar)\nSpecialized sensors (radar, hyperspectral)\nSatellite constellations providing frequent coverage\nVery high resolution imagery (&lt;1m pixels)\n\nThis wealth of options creates both opportunities and challenges in selecting appropriate imagery for your specific needs.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#section-outline",
    "href": "part-02-satellite/00-satellite.html#section-outline",
    "title": "Satellite imagery",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key considerations when working with satellite imagery:\n9  Choosing imagery\n\nSpatial, spectral, and temporal resolution\nPublic vs commercial options\nCost considerations\nData quality and availability\n\n10  Processing imagery\n\nAccessing imagery\nPre-processing steps\nQuality control\nComputing requirements\n\nThese chapters provide practical guidance for incorporating satellite imagery into your MOSAIKS workflow while highlighting important technical and logistical considerations.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will take a look at how to choose the right satellite imagery for your application.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html",
    "href": "part-02-satellite/01-satellite-imagery.html",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "9.1 Time range\nLINK: https://eds-223-geospatial.github.io/course-materials/week6.html\nWhen selecting satellite imagery for use with MOSAIKS, several key factors need to be considered. The choice of imagery should be guided by your specific research needs, label characteristics, and practical constraints like cost and processing requirements.\nBased on your label resolution, a user needs to decide on:\nThe operational lifespan of satellites is a crucial consideration when choosing imagery sources. Different satellite programs have varying launch dates and operational periods, which affects the historical depth of available data.\nFor instance, the Landsat program offers the longest continuous space-based record of Earth’s land surface, with missions dating back to 1972. In contrast, more recent satellite programs like Sentinel (launched in 2014) offer higher resolution imagery but have a shorter historical record.\nKey considerations for time range include:",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#time-range",
    "href": "part-02-satellite/01-satellite-imagery.html#time-range",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "Historical depth needed for your analysis\nConsistency of sensor technology over time\nAvailability of calibration across different satellite generations\nPlanned mission duration and data continuity\n\n\n\n\nSource: NASA",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sensor-resolution-pixel-size",
    "href": "part-02-satellite/01-satellite-imagery.html#sensor-resolution-pixel-size",
    "title": "9  Choosing imagery",
    "section": "9.2 Sensor resolution (pixel size)",
    "text": "9.2 Sensor resolution (pixel size)\nSpatial resolution refers to the size of each pixel in the satellite imagery, which determines the smallest object that can be detected. The choice of spatial resolution should be guided by:\n\nThe physical size of features relevant to your labels\nThe spatial scale of your analysis\nStorage and processing constraints\nCost considerations (higher resolution typically means higher cost)\n\nCommon spatial resolutions:\n\nVery high (&lt; 1m): Individual trees, buildings, vehicle\nHigh (1-10m): Small agricultural fields, urban infrastructure\nMedium (10-30m): Large agricultural fields, forest stands\nLow (&gt; 30m): Regional land cover, climate patterns\n\n\n\n\nSource: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#spectral-resolution-em-range",
    "href": "part-02-satellite/01-satellite-imagery.html#spectral-resolution-em-range",
    "title": "9  Choosing imagery",
    "section": "9.3 Spectral resolution (EM range)",
    "text": "9.3 Spectral resolution (EM range)\nSpectral resolution describes the number and width of electromagnetic wavelength bands that a sensor can detect. Different features on Earth’s surface reflect or absorb radiation differently across these bands.\nKey Spectral Bands and Applications\n\n\n\n\n\n\n\n\nSpectral Region\nWavelength Range\nKey Applications\n\n\n\n\nVisible light (RGB)\n0.4-0.7 μm\nNatural color imagery, built environment\n\n\nNear-infrared (NIR)\n0.7-1.1 μm\nVegetation health, biomass\n\n\nShort-wave infrared (SWIR)\n1.1-3.0 μm\nMineral mapping, moisture content\n\n\nThermal infrared\n3.0-14 μm\nTemperature, urban heat islands\n\n\nMicrowave\n&gt; 1 mm\nCloud penetration, soil moisture\n\n\n\n\n\n\nSource: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#temporal-resolution-revisit-rate",
    "href": "part-02-satellite/01-satellite-imagery.html#temporal-resolution-revisit-rate",
    "title": "9  Choosing imagery",
    "section": "9.4 Temporal resolution (revisit rate)",
    "text": "9.4 Temporal resolution (revisit rate)\nTemporal resolution refers to how frequently a satellite revisits the same location. This can range from multiple times per day to several weeks between observations.\nFactors affecting temporal resolution include:\n\nSingle satellite vs constellation systems\nOrbital parameters\nSensor swath width\nCloud cover frequency\nLatitude of study area\n\nSome trade-offs to consider:\n\nHigher temporal resolution often means lower spatial resolution\nCost increases with temporal frequency\nData storage and processing requirements grow with frequency\n\n\n\n\nSource: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "href": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "title": "9  Choosing imagery",
    "section": "9.5 Cloud cover",
    "text": "9.5 Cloud cover\nCloud cover presents a significant challenge in optical satellite imagery. Cloud cover can introduce several issues, including reduced data availability, temporal inconsistency, and the need for quality control and processing overhead.\nSeveral strategies exist for handling cloud contamination include:\n\nCloud masking algorithms\nComposite images over time periods\nUse of radar satellites (which penetrate clouds)\nStrategic timing of image acquisition\nGap-filling techniques",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "href": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "title": "9  Choosing imagery",
    "section": "9.6 Sensor type",
    "text": "9.6 Sensor type\nSatellite sensors broadly fall into two categories:\n\n9.6.1 Passive sensors\n\nDetect natural radiation (reflected sunlight or emitted heat)\nInclude optical and thermal sensors\nPros: Typically easier to interpret\nCons: Limited by atmospheric conditions\n\n\n\n9.6.2 Active sensors\n\nEmit their own energy and measure the return signal\nInclude RADAR (SAR) and LiDAR\nPros: Can penetrate clouds and operate at night\nCons: Often require more complex processing\n\n\n\n\nSource: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sec-satellite-public",
    "href": "part-02-satellite/01-satellite-imagery.html#sec-satellite-public",
    "title": "9  Choosing imagery",
    "section": "9.7 Public satellites",
    "text": "9.7 Public satellites\nPublic satellite programs offer free or low-cost imagery with global coverage. These programs are often funded by government agencies and provide a wealth of data for scientific research and environmental monitoring.\n\n9.7.1 Optical sensors\n\nSentinel-2: 10-60m resolution, 5-day revisit, 13 spectral bands\nLandsat 8/9: 15-100m resolution, 16-day revisit, 11 spectral bands\nMODIS: 250m-1km resolution, daily revisit, 36 spectral bands\n\n\n\n9.7.2 Radar sensors\n\nSentinel-1: C-band SAR, 5-40m resolution, 6-12 day revisit\nNISAR: L-band and S-band SAR (launching 2024)\n\n\n\n9.7.3 Other\n\nVIIRS: 375m-750m resolution, daily global coverage\nASTER: 15-90m resolution, 16-day revisit, 14 spectral bands",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sec-satellite-private",
    "href": "part-02-satellite/01-satellite-imagery.html#sec-satellite-private",
    "title": "9  Choosing imagery",
    "section": "9.8 Private satellites",
    "text": "9.8 Private satellites\nPrivate satellites often provide higher resolution imagery, greater specialization, and more frequent revisits than public programs, but at a higher cost.\n\n9.8.1 Very high resolution\n\nMaxar WorldView: 31cm panchromatic, 1.24m multispectral\nPlanet SkySat: 50cm panchromatic, 1m multispectral\nAirbus Pleiades: 50cm panchromatic, 2m multispectral\n\n\n\n9.8.2 High resolution\n\nPlanet PlanetScope: 3-5m resolution, daily revisit\nPlanet RapidEye: 5m resolution, 5.5-day revisit\nSPOT: 1.5-6m resolution, 26-day revisit\n\n\n\n9.8.3 Specialized\n\nICEYE: X-band SAR, &lt;1m resolution\nCapella Space: X-band SAR, 50cm resolution\nGHGSat: Greenhouse gas monitoring",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "href": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "title": "9  Choosing imagery",
    "section": "9.9 Processing levels",
    "text": "9.9 Processing levels\nSatellite imagery is typically available at different processing levels. Understanding these levels is crucial for selecting appropriate data for your analysis.\n\n9.9.1 Common processing levels\n\n9.9.1.1 Level 0: Raw data\n\nUnprocessed instrument data\nRequires extensive processing\nNot typically used for analysis\nOnly available from some providers\n\n\n\n9.9.1.2 Level 1: Radiometric corrections\n\nBasic radiometric calibration\nMay include systematic geometric corrections\nCommon formats:\n\nLevel 1A: Raw data with radiometric calibration\nLevel 1B: Includes geometric corrections\nLevel 1C: Orthorectified and radiometrically calibrated\n\n\n\n\n9.9.1.3 Level 2: Surface reflectance\n\nAtmospheric corrections applied\nConverted to physical units\nCloud and shadow masks\nReady for most analyses\n\n\n\n9.9.1.4 Level 3: Derived products\n\nTemporal composites\nMosaics\nSpecific indices (e.g., NDVI)\nGap-filled products\n\n\n\n\n9.9.2 Key processing considerations\n\nOrthorectification is the process of removing geometric distortions from satellite imagery. This correction accounts for terrain effects, satellite position, and Earth curvature, ensuring accurate spatial analysis.\nAtmospheric correction is essential for multi-temporal analysis of satellite imagery. This process removes atmospheric effects like aerosols, water vapor, and scattering, ensuring consistency across images.\nRadiometric calibration is the process of converting raw sensor values to physical units. This step ensures that imagery from different sensors and time periods can be compared accurately.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "href": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "title": "9  Choosing imagery",
    "section": "9.10 Choosing the right combination",
    "text": "9.10 Choosing the right combination\nWhen selecting imagery for your application, consider these practical guidelines:\n\n9.10.1 For agricultural applications\n\nResolution: 10-30m\nTemporal frequency: Weekly or biweekly\nKey bands: NIR, Red, SWIR\nRecommended sensors: Sentinel-2, Landsat 8/9\n\n\n\n9.10.2 For urban applications\n\nResolution: 0.5-10m\nTemporal frequency: Monthly to annual\nKey bands: RGB, NIR\nRecommended sensors: WorldView, Planet, Sentinel-2\n\n\n\n9.10.3 For forest monitoring\n\nResolution: 10-30m\nTemporal frequency: Monthly to annual\nKey bands: NIR, SWIR, Red\nRecommended sensors: Landsat, Sentinel-2\n\n\n\n9.10.4 For water resources\n\nResolution: 10-30m\nTemporal frequency: Weekly to monthly\nKey bands: NIR, SWIR, Blue\nRecommended sensors: Sentinel-2, Landsat 8/9",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#cost-considerations",
    "href": "part-02-satellite/01-satellite-imagery.html#cost-considerations",
    "title": "9  Choosing imagery",
    "section": "9.11 Cost considerations",
    "text": "9.11 Cost considerations\n\n\n\n\n\n\nRemember that the most expensive or highest resolution imagery isn’t always the best choice. The optimal choice balances your scientific needs with practical constraints.\n\n\n\n\n9.11.1 Free options\n\nLandsat program (30m)\nSentinel program (10m)\nMODIS (250m-1km)\nVIIRS (375m-750m)\n\n\n\n9.11.2 Commercial options\n\nVery high resolution (sub-meter): $15-25/km²\nHigh resolution (1-5m): $5-15/km²\nMedium resolution (5-30m): $1-5/km²\nSubscription services: Variable pricing\n\n\n\n9.11.3 Hidden costs\nThere are several hidden costs associated with satellite imagery that should be considered:\n\nStorage requirements\nProcessing time\nTechnical expertise needed\nSoftware licenses\nCloud computing resources",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "href": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "title": "9  Choosing imagery",
    "section": "9.12 Making the final decision",
    "text": "9.12 Making the final decision\nConsider these questions when making your final imagery selection:\n\nWhat is the minimum spatial resolution needed?\nHow frequent do observations need to be?\nWhat spectral bands are required?\nWhat is the time period of interest?\nWhat is your budget?\nWhat processing level is needed?\nHow will you handle clouds and data gaps?\nWhat are your storage and computing resources?\n\nThe answers to these questions will guide you toward the most appropriate imagery source for your specific application.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will explore how to access and process satellite imagery for use with MOSAIKS.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html",
    "href": "part-02-satellite/02-satellite-processing.html",
    "title": "10  Processing imagery",
    "section": "",
    "text": "10.1 Overview\nAfter selecting appropriate satellite imagery (as discussed in Chapter 9), the next step is accessing and processing that imagery for use with MOSAIKS. This chapter covers key considerations and practical approaches for working with satellite data.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#cloud-vs-local-processing",
    "href": "part-02-satellite/02-satellite-processing.html#cloud-vs-local-processing",
    "title": "10  Processing imagery",
    "section": "10.2 Cloud vs local processing",
    "text": "10.2 Cloud vs local processing\nThere are two main approaches to accessing and processing satellite imagery:\n\n10.2.1 Cloud-based platforms\nModern cloud platforms offer several advantages for satellite image processing:\n\nNo need to download raw imagery\nScalable computing resources\nPre-configured environments\nOften include common datasets\nPay only for what you use\n\nPopular platforms include:\n\nGoogle Earth Engine\nMicrosoft Planetary Computer\nAmazon Web Services\nPlanet Platform\nEuro Data Cube\n\n\n\n10.2.2 Local processing\nTraditional local processing may be preferred when:\n\nInternet connectivity is limited\nData privacy is paramount\nWorking with proprietary algorithms\nComputing needs are modest\nFrequent reuse of same imagery\n\nConsider these factors when choosing:\n\nData volume\nProcessing complexity\nBudget constraints\nTime requirements\nSecurity needs",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#accessing-imagery",
    "href": "part-02-satellite/02-satellite-processing.html#accessing-imagery",
    "title": "10  Processing imagery",
    "section": "10.3 Accessing imagery",
    "text": "10.3 Accessing imagery\n\n10.3.1 Cloud platforms\n\n10.3.1.1 Google Earth Engine\nimport ee\nee.Initialize()\n\n# Get Sentinel-2 imagery\ns2 = ee.ImageCollection('COPERNICUS/S2_SR')\n  .filterDate('2019-01-01', '2019-12-31')\n  .filterBounds(region)\n\n\n10.3.1.2 Microsoft Planetary Computer\nimport planetary_computer as pc\nimport pystac_client\n\n# Access STAC API\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=pc.sign_inplace,\n)\n\n\n\n10.3.2 Local downloads\n\n10.3.2.1 Sentinel-2\nfrom sentinelsat import SentinelAPI\n\n# Connect to Copernicus Open Access Hub\napi = SentinelAPI('user', 'password')\n\n# Search and download\nproducts = api.query(\n    area,\n    date=('20190101', '20191231'),\n    platformname='Sentinel-2'\n)\n\n\n10.3.2.2 Landsat\nimport landsatxplore.api\n\n# Initialize API\napi = landsatxplore.api.API('user', 'password')\n\n# Search scenes\nscenes = api.search(\n    dataset='landsat_ot_c2_l2',\n    bbox=bbox,\n    start_date='2019-01-01',\n    end_date='2019-12-31'\n)",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "href": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "title": "10  Processing imagery",
    "section": "10.4 Processing steps",
    "text": "10.4 Processing steps\n\n10.4.1 1. Quality assessment\n\nCloud detection\nShadow masking\nBad pixel identification\nSensor artifacts removal\n\n\n\n10.4.2 2. Atmospheric correction\n\nConvert to surface reflectance\nAccount for atmospheric effects\nStandardize across images\n\n\n\n10.4.3 3. Geometric correction\n\nOrthorectification\nCo-registration\nProjection alignment\n\n\n\n10.4.4 4. Mosaicking\n\nImage stitching\nFeathering/blending\nGap filling\nColor balancing\n\n\n\n10.4.5 5. Temporal compositing\n\nBest-pixel selection\nWeighted averaging\nGap filling\nSmoothing",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-workflows",
    "href": "part-02-satellite/02-satellite-processing.html#processing-workflows",
    "title": "10  Processing imagery",
    "section": "10.5 Processing workflows",
    "text": "10.5 Processing workflows\n\n10.5.1 Example cloud-based workflow\ndef process_sentinel2(aoi, start_date, end_date):\n    \"\"\"\n    Process Sentinel-2 imagery on Google Earth Engine\n    \n    Parameters:\n    -----------\n    aoi : ee.Geometry\n        Area of interest\n    start_date : str\n        Start date (YYYY-MM-DD)\n    end_date : str\n        End date (YYYY-MM-DD)\n        \n    Returns:\n    --------\n    ee.Image\n        Processed composite image\n    \"\"\"\n    s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date)\n    \n    # Cloud masking\n    def mask_clouds(img):\n        clouds = img.select('QA60').bitwiseAnd(1 &lt;&lt; 10)\n        return img.updateMask(clouds.Not())\n    \n    # Apply cloud mask\n    s2_masked = s2.map(mask_clouds)\n    \n    # Create median composite\n    composite = s2_masked.median()\n    \n    return composite\n\n\n10.5.2 Example local workflow\ndef process_local_images(image_dir):\n    \"\"\"\n    Process downloaded satellite imagery\n    \n    Parameters:\n    -----------\n    image_dir : str\n        Directory containing images\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Processed image array\n    \"\"\"\n    import rasterio\n    from rasterio.merge import merge\n    \n    # List images\n    images = []\n    for file in os.listdir(image_dir):\n        if file.endswith('.tif'):\n            with rasterio.open(os.path.join(image_dir, file)) as src:\n                images.append(src)\n    \n    # Merge images\n    mosaic, transform = merge(images)\n    \n    return mosaic",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#best-practices",
    "href": "part-02-satellite/02-satellite-processing.html#best-practices",
    "title": "10  Processing imagery",
    "section": "10.6 Best practices",
    "text": "10.6 Best practices\n\nDocument everything\n\nProcessing steps\nParameter choices\nQuality control decisions\nSoftware versions\n\nValidate outputs\n\nVisual inspection\nStatistical checks\nGround truth comparison\nCross-platform verification\n\nOptimize resources\n\nBatch processing\nParallel computing\nMemory management\nStorage efficiency\n\nVersion control\n\nTrack code changes\nArchive key datasets\nDocument dependencies\nMaintain reproducibility",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "href": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "title": "10  Processing imagery",
    "section": "10.7 Common challenges",
    "text": "10.7 Common challenges\n\n10.7.1 Storage requirements\n\nRaw imagery can be massive\nMultiple processing steps multiply storage needs\nIntermediate products management\nBackup considerations\n\n\n\n10.7.2 Computing resources\n\nProcessing can be CPU/GPU intensive\nMemory limitations\nI/O bottlenecks\nNetwork bandwidth\n\n\n\n10.7.3 Quality issues\n\nCloud contamination\nAtmospheric effects\nSensor artifacts\nGeometric distortions\n\n\n\n10.7.4 Time management\n\nProcessing can be slow\nDownload times\nQuality checking\nIteration cycles",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "href": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "title": "10  Processing imagery",
    "section": "10.8 Future considerations",
    "text": "10.8 Future considerations\nAs you develop your processing pipeline, consider:\n\nScalability needs\nAutomation opportunities\nQuality control requirements\nResource constraints\nTime limitations\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will explore how to extract MOSAIKS features from processed imagery.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html",
    "href": "part-03-features/00-features.html",
    "title": "Satellite features",
    "section": "",
    "text": "Overview\nThis section explores MOSAIKS features - the compressed representations of satellite imagery that enable efficient prediction across diverse tasks. While many users can rely on pre-computed API features, understanding how these features work and how to generate them provides valuable context and enables customization when needed.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#overview",
    "href": "part-03-features/00-features.html#overview",
    "title": "Satellite features",
    "section": "",
    "text": "The technical details covered in these chapters are primarily relevant for users who need to understand or generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can focus on 12  API features.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "href": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "title": "Satellite features",
    "section": "When to look beyond the API features",
    "text": "When to look beyond the API features\nThe MOSAIKS API provides pre-computed features derived from 2019 Planet Labs imagery. While these features enable many applications, you may need to work directly with feature generation when:\n\nYour analysis requires data from a different time period\nYou need features at a different spatial resolution\nYou want to experiment with different feature parameters\nYou’re developing new methodological approaches\nYou need to validate or compare feature types",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#feature-types-and-computation",
    "href": "part-03-features/00-features.html#feature-types-and-computation",
    "title": "Satellite features",
    "section": "Feature types and computation",
    "text": "Feature types and computation\nMOSAIKS features transform raw satellite imagery into a concise tabular format that captures essential patterns while dramatically reducing data volume. The system currently supports:\n\nRandom convolutional features (RCFs)\nGaussian random features\nEmpirical patch features\nHybrid approaches\n\nThese different feature types offer various tradeoffs in terms of computation time, storage requirements, and predictive performance across tasks.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#section-outline",
    "href": "part-03-features/00-features.html#section-outline",
    "title": "Satellite features",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key aspects of MOSAIKS features:\n11  Understanding features\n\nTechnical foundation\nImplementation details\n\nParameter choices\nPerformance characteristics\n\n12  API features\n\nAccessing pre-computed features\nFeature specifications\nData formats and structure\nUsage guidelines\n\n13  Computing features\n\nProcessing requirements\nComputational workflows\nStorage considerations\nQuality control\n\nThese chapters provide both practical guidance for working with MOSAIKS features and deeper technical understanding of how the feature extraction process works.\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will attemppt to provide some context and intuition for the random convolutional features (RCFs) that are at the core of MOSAIKS.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html",
    "href": "part-03-features/01-features-rcf.html",
    "title": "11  Understanding features",
    "section": "",
    "text": "11.1 Kitchen sinks?\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. Whenever we present this system, people always ask us, “where does kitchen sinks come from?” The answer stems from the phrase “everything but the kitchen sink,” which means “almost everything imaginable.” If you were to hear someone describe making a sandwhich with “everything but the kitchen sink,” you would know that they used every ingredient at their disposal. You might also hear the phrase “everything and the kitchen sink,” which means “everything imaginable.” For MOSAIKS, I think “everything but the kitchen sink” is more appropriate as we are not extracting every last drop of information from the imagery. However we are getting a lot out of the imagery. Additionally, if it helps, you can think of the imagery as something we are not taking along with us, rather just the useful bits we extracted.\nThis idea of leaving behind the imagery is one of the reasons MOSAIKS is so powerful. It means that for most users, they never have to sufer through the issues that arise from dealing with large amounts of satellite imagery. We (the MOSAIKS team) take the imagery, extract a bunch of random convolutional features, and then we leave the imagery behind. We don’t need to know what the imagery looks like, we don’t need to interpret what any feature is, we just need to know that we have a bunch of features that we can use to predict different outcomes. Then users can download these features and use them to predict their outcomes.\nIn this section of the book, we leave this simplistic aspect of MOSAIKS behind. We go back to the root of the problem and focus on how we extract features from imagery. We also attempt to provide some intuition for what these features are.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "href": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "title": "11  Understanding features",
    "section": "",
    "text": "Figure 11.1: Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by Grace Lewin.\n\n\n\n\n\n\n\n\n\n\n\nIn this book, we refer to the random convolutional features as RCFs, features, satellite features, MOSAIKS features, and possibly other terms. You should consider these terms interchangeable in this context.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "href": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "title": "11  Understanding features",
    "section": "11.2 Why use RCFs?",
    "text": "11.2 Why use RCFs?\n\n11.2.1 Traditional convolutional neural networks (CNNs)\nTo learn why we use random convolutional features (RCFs), we need to understand how traditional convolutional neural networks (CNNs) work. CNNs are a type of neural network that are designed to work with images. They are made up of layers of neurons that are connected in a way that allows them to learn features from the images. In the case of CNNs, these features are learned through a process called backpropagation. Backpropagation is a method for training neural networks that uses the chain rule of calculus to calculate the gradient of the loss function with respect to the weights of the network. This gradient is then used to update the weights of the network in a way that minimizes the loss function. This means that the network is learning to recognize features in the images that are important for the task it is trying to solve. It is therfore using minimization to learn the features that are important for the task.\n\n\n\n\n\n\nFigure 11.2: A simplified diagram showing a typical convolutional nueral network model architecture used for\n\n\n\n\n\n11.2.2 Replacing minimization with randomization in learning\nMOSAIKS takes a different approach to learning features from images. Instead of using backpropagation to learn the features, we use random convolutional filters to extract features from the images. The resultant features are therfore as random as the filters applied to the images. This means that the features are not optimized for any particular task, but are instead a random sample of the information in the images. This is a key difference between RCFs and traditional CNNs. Traditional CNNs learn features that are optimized for the task they are trying to solve, while RCFs extract features that are random samples of the information in the images. This means that RCFs are much faster to compute and can be used to extract features from large amounts of imagery in a short amount of time.\n\n\n\n\n\n\nFigure 11.3: Rolf et al. 2021 Figure 1: MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#random-convolutional-features-rcfs",
    "href": "part-03-features/01-features-rcf.html#random-convolutional-features-rcfs",
    "title": "11  Understanding features",
    "section": "11.3 Random convolutional features (RCFs)",
    "text": "11.3 Random convolutional features (RCFs)\n\n11.3.1 Turning an image into a feature vector\nThe core of MOSAIKS is the random convolutional features (RCFs) that we extract from satellite imagery. The extraction process is simple: we take a bunch of random convolutional filters and apply them to the imagery. Each convolutional filter can be a tensor of randomly generated values (gaussian patches), or they can be sampled from the imagery itself (empirical patches; Figure 11.4).\n\n\n\n\n\n\nFigure 11.4: Rolf et al. 2021 Figure 1 C: A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. K patches are randomly sampled from across the N images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image.\n\n\n\nIf we take a convolutional filter and apply it to the imagery, we get a new image. This new image is the result of the convolutional filter “looking” at the imagery and “seeing” something. The filter might be looking for edges, colors, textures, patterns, or anything else. The filter might be looking for something that we can’t even see with our eyes. After the convolution is applied, a non-linear activation function transforms the result. In this case, we use the rectified linear unit (ReLU) activation function. ReLU is a simple function that takes the maximum of the input and zero. This means that if the input is negative, the output is zero, and if the input is positive, the output is the input. After this, we use an adaptive average pooling layer to reduce the size of the activation map to a single number. This summary value is the feature for this filter.\n\n\n\n\n\n\nAfter repeating this process for all the filters, we end up with a feature vector for the image. This feature vector is a representation of the image that.\n\n\n\n\n\n11.3.2 Intuition for RCFs\nHere we will take a look at a cartoon representation of what a RCF vector might be looking for in the imagery. We will use a few simple 3 pixel by 3 pixel filters in the convolution step. The filters can have a few common names including weights, kernels, or patches. In the cartoon, we will use the term patch because these filters are drawn from the imagery (i.e. they are a small “patch” of the imagery).\n\n\n\n\n\n\nFigure 11.5: A single image with 3 random convolutional patches, highlighting 3 different feature activations of the imagery.\n\n\n\nIn the cartoon above (Figure 11.5), our first patch is drawn from the forest canopy and so we see a 3x3 set of green pixels. Patch 2 is a 3x3 patch of grey pixels from a road. Patch 3 is a 3x3 patch of blue pixels from a river. When we apply these patches to the imagery, we get a new image that highlights the features that the patch is looking for. For example, when we apply the forest patch to the imagery, we get a new image that highlights the forest canopy. When we apply the road patch to the imagery, we get a new image that highlights the road. When we apply the river patch to the imagery, we get a new image that highlights the river.\n\n\n\nVideo\n\n\nFigure 11.6: Two cartoon images with the same patches used in the convolution step. We see that we have several labels for each image and that we can model each outcome with the same set of features.\n\n\n\nHaving more trees in the image will increase the value of the feature that represents the forest patch. In Figure 11.6, image 1 has more trees than image 2, so we see the feature in the first place in the vector has a higher value. Having more roads in the image will increase the value of the feature that represents the road patch, which is seen with image 2 having a higher second value. This is how we can use the features to predict different outcomes from the imagery. We can use the same set of features to predict the presence or levels of different outcomes in the imagery.\n\n\n\nVideo\n\n\nFigure 11.7: A cartoon image projected into random feature space. If we look at just 2 dimensions, how grey on the x-axis and how green on the y-axis, we can find the vector through this feature space which represents our outcome.\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will look at what is publicly available in the MOSAIKS API.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html",
    "href": "part-03-features/02-features-api.html",
    "title": "12  API features",
    "section": "",
    "text": "12.1 Feature data",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#feature-data",
    "href": "part-03-features/02-features-api.html#feature-data",
    "title": "12  API features",
    "section": "",
    "text": "Creating satellite image features is covered in later chapters.\n\n\n\n\n12.1.1 RCF brief overview\nMOSAIKS relies on random convolutional features (RCFs) computed from satellite imagery. The random nature of the features means that they are task-agnostic. These features can be reused multiple times to model various tasks. Features are generally created over a standard grid of 0.01 by 0.01 degree cells (~1 km², depending on latitude). The output is a feature vector of length K for every grid location.\n\n\n\n\n\n\nFeatures can be computed at higher or lower resolutions than 1 km². This is discussed in Chapter 13.\n\n\n\n\n\n12.1.2 Feature aggrgegation\nThe default 1 km² resolution of MOSAIKS is generally the maximum resolution the label data should be in. If the labels are in lower resolution, the satellite features can be aggregated up to larger areas to match. Typical aggregation might be to larger grid cell, census tract, county/district, or state/province levels. The exact aggregation level is contingent on the spatial resolution of the label data.\n\n\n\n\n\n\nFigure 12.1: Example showing of 3 representative random convolutional features (rows). Features are downloaded from the MOSAIKS API at 0.01° resolution (the native resolution) and aggregated to 3 levels, including (A) larger grid cells (0.1°), (B) counties, and (C) states.\n\n\n\n\n\n12.1.3 API Features\nCurrently the MOSAIKS API has a single set of global features ready to download. The features are publicly available for download; this is the fastest and easiest way to begin using MOSAIKS. They come from Planet Labs, Inc. Basemaps product Global Quarterly 2019 (qaurter 3) which has a native resolution of 4.77 m in the red, green, and blue bands of the visual spectrum. Given this, the easiest way to get started with MOSAIKS is to have label data for a recent time period (ideally from 2019 for fast changing labels, or a close year for more steady labels). To use MOSAIKS for time series data is possible, just not currently with precomputed and publicly available features. To access API features see Chapter 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nResolution\nTime Period\nPopulation Weighted\nArea Weighted\nDownload\n\n\n\n\n0.01°\n2019 Q3\n\n\nQuery\n\n\n0.1°\n2019 Q3\n✓\n✓\nChunked files\n\n\n1°\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM2\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM1\n2019 Q3\n✓\n✓\nSingle file\n\n\nADM0\n2019 Q3\n✓\n✓\nSingle file\n\n\n\n\n\nTable 12.1: The MOSAIKS API offers features at 0.01°, 0.1°, and 1° resolution for the entire globe. Additionally, we provide features aggregated to the ADM0 (country), ADM1 (state/province), and ADM2 (county/municipality) levels. All features share the same 4,000 feature columns, and are all created from the same Planet Labs imagery, the only difference is the resolution/aggregation level. At each level of aggregation, we offer area weighted features and population weighted features. Population weighted features use weights from the Gridded Population of the World (GPWv4) population density dataset.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html",
    "href": "part-03-features/03-features-computing.html",
    "title": "13  Computing features",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\nDepending on the spatial scale and resolution of the label data, subsampling the MOSAIKS grid may be appropriate to reduce computation time and cost.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-04-models/00-model.html",
    "href": "part-04-models/00-model.html",
    "title": "Task modeling",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "Task modeling"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html",
    "href": "part-04-models/01-model-choice.html",
    "title": "14  Model choice",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Model choice</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html",
    "href": "part-04-models/02-model-spatial.html",
    "title": "15  Modeling over space",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter,",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Modeling over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html",
    "href": "part-04-models/03-model-temporal.html",
    "title": "16  Modeling over time",
    "section": "",
    "text": "16.1 Understanding temporal resolution\nTime series analysis in satellite-based ML applications requires careful attention to the temporal resolution of both your labels and imagery. In the context of MOSAIKS, you may opt for different strategies depending on the frequency of satellite acquisitions and how quickly your outcome of interest changes.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "href": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "title": "16  Modeling over time",
    "section": "",
    "text": "Figure 16.1: ESRI visualization of pixel values for a given location changing over time.\n\n\n\n\n16.1.1 Temporal alignment\nWhen aligning data in time, consider:\n\nLabel frequency: How often are ground measurements collected? For instance, annual crop yields, monthly economic indicators, or even daily weather observations. The granularity of these data points will guide how to match them to your satellite-derived features.\nImagery availability: How frequently can you obtain clear satellite images of your area of interest? High revisit rates enable more frequent observations, but factors like cloud cover or sensor anomalies may reduce the actual number of usable images.\nChange detection: How quickly does your variable of interest change? Urban development may unfold over months or years, whereas flood events can occur within days. Matching the temporal granularity of your features to the dynamics of your phenomenon is crucial.\n\n\n\n16.1.2 Seasonal patterns\nBoth natural and human-driven processes often exhibit strong seasonal signals:\n\nNatural cycles: Vegetation growth, snow cover, and water extent are all influenced by seasons. For example, NDVI metrics might be more informative during peak growing seasons (Figure 16.2).\nHuman activities: Cropping cycles, holiday travel, and heating or cooling demand are just a few examples of how human behavior can vary throughout the year. These temporal rhythms can introduce systematic patterns in your data.\nFeature extraction: Because satellite observations reflect surface conditions, different times of the year may require distinct feature sets. For example, reflectance values change when vegetation is senescent vs. when it is fully grown. Similarly, atmospheric conditions (like haze) may be more prevalent in certain seasons.\n\n\n\n\n\n\n\nFigure 16.2: EOS Data Analytics time series visualization of the Normalized Difference Vegetation Index (NDVI) showing seasonal trends.\n\n\n\n\n\n16.1.3 Challenges in time series applications\nAlthough the potential benefits of time series analysis are significant, there are a few common pitfalls:\n\n16.1.3.1 Data gaps\n\nCloud cover: In regions with high cloud coverage, usable imagery can be sparse, leading to irregular time intervals between valid observations.\nSatellite maintenance or sensor dropout: Even short interruptions in satellite operations can reduce data availability.\nOrbital patterns: Some satellites have specific revisit schedules, meaning certain areas might not be imaged as frequently as others, leading to patchy time series data.\n\n\n\n\n\n\n\nFigure 16.3: Visualization of cloud cover over the Amazon rainforest.\n\n\n\n\n\n\n\n\n\nRecommended reading\n\n\n\nFor more information on cloud cover and its impact on satellite data, see Flores-Anderson et al. 2023.\n\n\n\n\n\n16.1.4 Pre-processed satellite products\nSeveral satellite providers offer pre-processed data products specifically designed for time series analysis. These products handle common challenges like cloud cover and normalization:\n\n16.1.4.1 MODIS vegetation indices\n\n16-day or monthly composites of vegetation indices (e.g., NDVI, EVI)\nAutomated cloud masking and quality control\nSurface reflectance values normalized for atmospheric effects\nGlobal coverage at 250m-1km resolution since 2000\nIdeal for monitoring seasonal vegetation dynamics\n\n\n\n16.1.4.2 Planet basemaps\n\nQuarterly visual composites from multiple PlanetScope satellites\nCloud-free mosaics using best available pixels\nColor-balanced and radiometrically calibrated\nGlobal coverage at ~4.7m resolution\nSuitable for tracking gradual land use changes\n\n\n\n16.1.4.3 Harmonized Landsat-Sentinel (HLS)\n\nCombined product using Landsat 8-9 and Sentinel-2 imagery\n2-3 day revisit frequency at 30m resolution\nAtmospherically corrected and co-registered\nConsistent surface reflectance values between sensors\nEnables dense time series from 2013-present\n\nThese products reduce the preprocessing burden for users but may not capture rapid changes that occur between compositing periods. The choice between using raw imagery or pre-processed products depends on the temporal resolution needed for your specific application. These data products and how to use them will be covered in greater detail in Chapter 9 and Chapter 10.\n\n\n16.1.4.4 Temporal consistency\n\nSensor drift: Over time, satellite sensors can degrade or drift, influencing the consistency of your data. Proper calibration can mitigate these issues.\nMulti-sensor calibration: If you are combining data from multiple satellites in a constellation, ensure that differences in sensor sensitivity or bands do not introduce spurious signals in your time series.\n\n\n\nVideo\nTime lapse imagery of North Platte, Nebraska. Imagery and visualization from planet Labs Inc.\n\n\n\n\n16.1.4.5 Storage and computation\n\nData volume: Each additional time step in your analysis increases the storage and processing requirements.\nTemporal correlation: Many time series phenomena exhibit autocorrelation, which can influence how you design and train models. Standard ML algorithms assume independence between samples, so specialized methods or features (such as lagged features) may be required to handle temporal dependencies.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "href": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "title": "16  Modeling over time",
    "section": "16.2 Approaches to time series modeling",
    "text": "16.2 Approaches to time series modeling\nBelow are three common strategies for incorporating time series data within a MOSAIKS-like framework. Each approach has trade-offs in terms of complexity, computational cost, and interpretability.\n\n16.2.1 Feature stacking\nIn a feature-stacking approach, you generate MOSAIKS features for multiple time steps and then concatenate them into a single feature vector. This is straightforward but can lead to very large feature spaces if you have many time points.\n\nCompute features for each time period: Run your MOSAIKS feature extraction for each quarter, month, or year—whatever time granularity is relevant.\nConcatenate features: Combine them into a single vector, ensuring that naming conventions keep time steps distinguishable.\nModel training: Input the stacked features into your preferred machine learning algorithm (e.g., linear regression, random forest, or neural network).\n\nExample (quarterly stacking):\n# Create feature names for each quarter\nfeatures_Q1 = [f'X_{i}_Q1' for i in range(1000)]  # X_0_Q1, X_1_Q1, ..., X_999_Q1\nfeatures_Q2 = [f'X_{i}_Q2' for i in range(1000)]  # X_0_Q2, X_1_Q2, ..., X_999_Q2\nfeatures_Q3 = [f'X_{i}_Q3' for i in range(1000)]  # X_0_Q3, X_1_Q3, ..., X_999_Q3\nfeatures_Q4 = [f'X_{i}_Q4' for i in range(1000)]  # X_0_Q4, X_1_Q4, ..., X_999_Q4\n\n# Combine features names for all quarters\nfeatures_annual = features_Q1 + features_Q2 + features_Q3 + features_Q4\n\n# Total length = 4,000 features for four quarters\nfeatures_df = pd.DataFrame(data=features, columns=features_annual)\nThis approach is typically suitable for annual or seasonal data where the number of time steps remains manageable. However, it may be less practical if you have daily or weekly observations over several years.\n\n\n16.2.2 Temporal aggregation\nIn temporal aggregation, you compute features at a higher frequency but then summarize them over a time window:\n\nExtract features at a high frequency: This provides a rich temporal view.\nAggregate: Compute statistical summaries such as the mean, max, or variance of each feature across the chosen time window. Common windows include daily, weekly, monthly, and quarterly aggregations.\nModel with aggregated features: The aggregated features can represent dynamic processes while controlling the dimensionality of your dataset.\n\nThis method captures general trends and reduces noise from cloud cover or other transient factors. However, important temporal nuances (like specific short-lived events) might be lost in the aggregation.\n\n\n16.2.3 Sequential modeling\nFor phenomena where temporal order is crucial and frequent observations exist, sequential modeling can be more powerful:\n\nFeature extraction: Maintain the time dimension in your feature matrix (e.g., one matrix per location, with time as rows and features as columns).\nApply time series modeling: Techniques like LSTM (Long Short-Term Memory) networks, temporal convolutional networks, or classical state-space models (e.g., ARIMA) can handle temporal dependencies explicitly.\nLagged relationships: Incorporate features from previous time steps to capture delayed effects (e.g., precipitation from last month affecting vegetation today).\n\nAlthough these methods provide a more nuanced view of temporal processes, they require additional modeling expertise and computational resources.\n\n\n\n\n\n\nStart with a simpler approach like feature stacking or temporal aggregation. If you find that your phenomenon has rapid or complex temporal dynamics that aren’t well-captured by these methods, then explore more advanced sequential models.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "href": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "title": "16  Modeling over time",
    "section": "16.3 Hands on with time series data",
    "text": "16.3 Hands on with time series data\nIn lieu of time series MOSAIKS features, we will instead demonstrate similar examples using MODIS NDVI data. This will allow us to explore the challenges and opportunities of time series analysis without the need for custom feature extraction.\nNOTE: UPDATE ME with new notebook link.\n\n\n\n\n\n\nClick the badge to run the demo in Google Colab!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n\n↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\nRemember to click File -&gt; Save a copy in Drive to save any changes you make.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "href": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "title": "16  Modeling over time",
    "section": "16.4 Filling temporal gaps",
    "text": "16.4 Filling temporal gaps\nWhen working with time series data, you may encounter missing values due to cloud cover, sensor issues, or other factors. Here are some common strategies for filling these gaps:",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#summary",
    "href": "part-04-models/03-model-temporal.html#summary",
    "title": "16  Modeling over time",
    "section": "16.5 Summary",
    "text": "16.5 Summary\nHandling time series in satellite-based ML workflows requires balancing data volume, temporal alignment, and modeling complexity. While feature stacking can be effective for low-frequency or seasonal processes, more sophisticated techniques may be needed to capture high-frequency changes or long-range temporal dependencies. Ultimately, the “best” approach depends on the nature of your target variable, data availability, and the resources at your disposal.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will look at",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling over time</span>"
    ]
  },
  {
    "objectID": "part-05-uncertainty/00-uncertainty.html",
    "href": "part-05-uncertainty/00-uncertainty.html",
    "title": "Model uncertainty",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "Model uncertainty"
    ]
  },
  {
    "objectID": "part-05-uncertainty/01-uncertainty-intro.html",
    "href": "part-05-uncertainty/01-uncertainty-intro.html",
    "title": "17  Overview of uncertainty",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "Model uncertainty",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Overview of uncertainty</span>"
    ]
  },
  {
    "objectID": "part-05-uncertainty/02-uncertainty-ethics.html",
    "href": "part-05-uncertainty/02-uncertainty-ethics.html",
    "title": "18  Ethical implications",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "Model uncertainty",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ethical implications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "References"
    ]
  }
]