# Understanding features {#sec-features-rcf} 

```{r}
#| echo: false
#| results: "asis"

source("../_common.R")
status("review")
```

::: {.callout-tip}
# Who should read this chapter?

This section of the book is not for everyone. If you want to use the publicly available MOSAIKS features to predict outcomes, you can skip this section. If you plan on computing your own features, this section is for you. 
:::

## Kitchen sinks?

MOSAIKS stands for **<ins>M</ins>ulti-task <ins>O</ins>bservation using <ins>SA</ins>tellite <ins>I</ins>magery & <ins>K</ins>itchen <ins>S</ins>inks**. Whenever we present MOSAIKS, we get the question, “Where does ‘kitchen sinks’ come from?” The answer stems from the phrase *“everything but the kitchen sink,”* which means *“almost everything imaginable.”* In the context of MOSAIKS, *everything but the kitchen sink* emphasizes that we take a *huge amount* of information out of the raw imagery—though, of course, we’re not capturing every last pixel or every possible relationship. It’s as if we’re taking the most useful “ingredients” (i.e., features) from the imagery and leaving the rest behind.

![Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by [Grace Lewin](https://young-lab.eemb.ucsb.edu/people/grace-lewin).](../images/Everything%20but%20the%20kitchen%20sink.gif){#fig-everything-but-the-kitchen-sink}

This idea of leaving behind the raw imagery is key to MOSAIKS’s power. It means that *most* users never have to handle the massive amounts of satellite imagery directly. Instead, the MOSAIKS team takes on that burden—extracting a large set of random convolutional features—and then discards the imagery. End users do not need to *see* the raw imagery or interpret *what* each individual feature means; they can just use the numerical representation. As a result, users can simply download these features and apply them to their own predictive tasks.

In this section, however, we lift the hood to see what’s going on. We focus on how we extract the features from satellite imagery and attempt to provide *some* intuition for what these features represent.

::: {.callout-tip}
In this book, the terms *random convolutional features*, *RCFs*, *features*, *satellite features*, and *MOSAIKS features* are used interchangeably.
:::

---

## Random convolutional features (RCFs)

### Understanding convolutions

In simple terms, applying a *convolution* to an image can highlight certain patterns, like edges, textures, or colors. Convolutional filters “scan” across the image, computing element-wise products with small patches of the image. Many computer vision models stack multiple convolutional and other layers into a convolutional neural network (CNN). Deep neural networks can have many layers, which is why the approach is often called deep learning.

![A gif showing a convolutional layer with no padding and no strides. Source: [A guide to convolution arithmetic for deep learning](https://github.com/vdumoulin/conv_arithmetic).](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/refs/heads/master/gif/no_padding_no_strides.gif){#fig-convolution-gif}

MOSAIKS, on the other hand, uses a *single* convolutional layer. These layer weights are randomly initialized and remain *fixed*; they aren’t updated during training. This is why the features we get are essentially “random samples” of the spatial and spectral information in the images. 

::: {.callout-tip}
There are many great resources for visualizing convolutions, including [Convolutional Neural Networks (CNNs) explained](https://poloclub.github.io/cnn-explainer/).
:::

### Turning an image into a feature vector

#### Overview

The MOSAIKS featurization process produces a *fixed-length* feature representation for each patch of satellite imagery. Practically, this means we end up with a numerical vector for each image. The process has three main steps:

1. **Convolution**  
2. **Activation**  
3. **Average pooling**  

Let’s illustrate these steps with a simple example. Imagine we have 10,000 satellite images, each in the visual spectrum (red, green, and blue channels). Each image is a 3-dimensional tensor of shape (3, 256, 256)—representing the *color channels* (3) by *height* (256) by *width* (256).

![A collection of satellite images. Source: [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)](../images/mpc_gallery.jpg){#fig-mpc-gallery}

#### Convolution

1. **Initialize convolutional weights.**  
   We can draw these weights from a normal (Gaussian) distribution or from random patches of our image dataset. Either way, each *filter* has the same number of color channels as the original images and a specified kernel size (e.g., 3×3). So if our kernel size is 3, each filter might have shape (3, 3, 3).

2. **Perform the convolution.**  
   We compute the dot product of each filter with *every* portion of the image as the filter slides across it. The result is a new image (a “feature map”) that highlights regions similar to that filter’s pattern.

::: {.callout-tip}   
We use pytorch’s [torch.nn.functional.conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) function to perform this operation.
:::

#### Activation

Next, we apply a non-linear activation function—[ReLU (Rectified Linear Unit)](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)—to the feature map. ReLU outputs `max(0, x)`, meaning it zeroes out negative values. This step helps capture non-linearities in the data.

![Three randomly generated feature activation maps plotted from the [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/catalog) Landsat Collection thumbnail image.](../images/RCF_patch_sampling_activation_maps.png){#fig-RCF-patch-sampling-activation-maps}

#### Average pooling

We then apply an adaptive average pooling layer to each activation map, collapsing the 2D spatial grid into a single number per filter. Essentially, this “global average” is a single summary value for how strongly that filter responded to the image. 

::: {.callout-tip}
We use pytorch’s [torch.nn.functional.adaptive_avg_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html) function to perform this operation.
:::

#### Putting it all together

We repeat this for all filters. If we have *K* filters, we end up with a *K*-dimensional feature vector.

![A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. *K* patches are randomly sampled from across the *N* images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image. Source: [Rolf et al. 2021 Figure 1 C](https://www.nature.com/articles/s41467-021-24638-z/figures/1)](../images/rolf_et_al_2021-Fig_1-c.jpg){#fig-Rolf-et-al-2021-Fig-1-C}

#### The 2-for-1 feature special

When we define our model, one of the parameters that needs to be specified is the number of features. This number needs to be positive because we actually generate 2 features for each filter. 

For each filter, we create two features:

1. **Feature A** – ReLU activation on the convolution output.  
2. **Feature B** – ReLU activation on the *inverted* convolution output (i.e., multiplication by –1, or “negative” of the same filter output).

We then average-pool both maps, producing two features per filter. So if you specify 200 filters, you are actually drawing 100 weights and getting 200 features. This improves computational efficiency and ensures that each filter captures both positive and negative cues.

---

### Intuition for RCFs

Below is a cartoon that illustrates what a random convolutional feature vector might be looking for. Suppose we randomly draw three 3×3 patches from different parts of an image: a *forest* patch, a *road* patch, and a *river* patch.

![A single image with 3 random convolutional patches, highlighting 3 different feature activations of the imagery.](../images/cartoon_features-1.jpg){#fig-cartoon-features-1}

When we convolve the forest patch across the entire image, the resulting map “lights up” places that look like forest. Likewise, the road patch lights up roads, and the river patch lights up the river. After the ReLU and average-pooling steps, we get a set of summary values—one for each filter.

![Two cartoon images with the same patches used in the convolution step. We see that we have several labels for each image and that we can model each outcome with the same set of features.](../images/cartoon_features.mp4){#fig-cartoon-label-video autoplay="true"}

If Image 1 has more trees than Image 2, the “forest” feature’s value is higher in Image 1. If Image 2 has more roads, its “road” feature will be higher. Because these features each capture a different (random) spatial pattern, we can then combine them for many downstream prediction tasks—whether it’s predicting tree cover, paved roads, or even something else that correlates with these visual patterns.

![A cartoon image projected into random feature space. If we look at just 2 dimensions, how grey on the x-axis and how green on the y-axis, we can find the vector through this feature space which represents our outcome.](../images/cartoon_vectors.mp4){#fig-cartoon-vect-video autoplay="true"}

---

## Why use RCFs?

### Traditional convolutional neural networks (CNNs)

To appreciate RCFs, let’s contrast them with traditional CNNs. In a standard CNN, filters are learned via *backpropagation*. The network sees many examples, calculates errors, and updates the filter weights so that over time they become good at extracting *task-specific* features. This makes CNNs powerful, but only for what they have been trained to do.

![A simplified diagram showing a typical convolutional neural network model architecture.](../images/CNN.jpg){#fig-CNN}

### Replacing minimization with randomization in learning

MOSAIKS takes a radically simpler approach: *random* filters are used to sample the imagery’s information. Because they are random, they are not “tailored” to any one task. This sounds counterintuitive at first—surely you’d want to *learn* your filters! But the virtue of randomization is speed and *broad* applicability. Since no training is required to set these filters, we can produce features at planet-scale *very* quickly. These features can then be shared with many users, each of whom can apply them to their own predictive tasks (e.g., estimating forest cover, housing density, crop yields, etc.).

![[Rolf et al. 2021 Figure 1](https://www.nature.com/articles/s41467-021-24638-z/figures/1): MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.](../images/rolf_et_al_2021-Fig_1.jpg){#fig-Rolf-et-al-2021-Fig-1}

---

## Summary

Random Convolutional Features (RCFs) form the backbone of MOSAIKS’s ability to handle massive amounts of satellite imagery at scale. Instead of learning tailored filters via backpropagation (like a traditional convolutional neural network), MOSAIKS uses randomly initialized filters that remain fixed. This single-layer approach may seem counterintuitive, but it has several advantages:

1. **Lightweight and Task-Agnostic**

    - Because the filters are not tied to any particular outcome, the same feature set can be applied to countless downstream tasks. This drastically reduces the need to reprocess raw imagery for every new prediction goal.

1. **Planet-Scale Featurization**

    - Massive amounts of imagery can be processed quickly because no iterative training is needed to refine filters. After a one-time generation of RCFs, they can be stored, shared, and reused—removing the bottleneck of handling petabytes of raw data.

1. **Broad Feature Capture**

    - Random filters effectively “sample” a wide variety of spatial and spectral patterns, capturing edges, textures, colors, and more. The “2-for-1” feature approach ensures that each filter captures both positive and negative cues, doubling the dimensionality (and potential information) without doubling compute time.

1. **Easily Distributed**

    - The resulting feature vectors (rather than raw images) are small enough to download and manipulate on standard hardware. Researchers and practitioners can thus apply these features to varied applications, from ecological monitoring to socioeconomic modeling.

By converting raw imagery into a rich but compact numerical representation, RCFs offer a robust, flexible, and scalable gateway to satellite-based insights—without the headache of storing or manually interpreting vast image archives.

---

::: {.callout-note}
# Looking forward

In the next section, we’ll look at what is publicly available in the MOSAIKS API and how you can access these features without featurizing data yourself.
:::
