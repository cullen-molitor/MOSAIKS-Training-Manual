[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIKS Training Manual",
    "section": "",
    "text": "Welcome\nThis is the first edition of the MOSAIKS Training Manual! The manual serves as a comprehensive reference for understanding MOSAIKS, its capabilities, and guidance on practical implementation. You will learn what MOSAIKS is, what can be done with it, and how to use it effectively in a variety of applications.\nThe skills and knowledge you gain from this manual will enable you to leverage satellite imagery and machine learning to address complex socioeconomic and environmental challenges. This can be a self taught manual or used in conjunction with a training course.\nMany of the concepts and examples are broadly applicable to the world of remote sensing and machine learning, so even if you are not using MOSAIKS, you may find the content useful.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-mosaiks",
    "href": "index.html#what-is-mosaiks",
    "title": "MOSAIKS Training Manual",
    "section": "What is MOSAIKS?",
    "text": "What is MOSAIKS?\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. It is a framework designed to simplify the use of satellite imagery and machine learning for predicting socioeconomic and environmental outcomes across different geographic contexts and time periods. MOSAIKS relis on random convolutions (developed in Rahimi and Recht (2008)) applied to satellite imagery, which extract task-agnostic features and enable researchers and practitioners to easily and flexibly predict a diversity of outcomes from raw imagery.\n\n\n\n\n\n\nFigure 1: MOSAIKS spelled out with imagery from the Landsat Satellite Constellation data catalog. Made with: Your Name in Landsat",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-is-this-for",
    "href": "index.html#who-is-this-for",
    "title": "MOSAIKS Training Manual",
    "section": "Who is this for?",
    "text": "Who is this for?\nThis comprehensive two-week program is designed for academics, professionals, and practitioners interested in leveraging MOSAIKS to better understand socioeconomic and environmental challenges. The course is particularly valuable for those working in:\n\nRemote sensing and satellite imagery analysis\nMachine learning applications with geospatial data\nAgricultural and environmental monitoring and assessment\nDevelopment research and policy making",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-will-i-learn",
    "href": "index.html#what-will-i-learn",
    "title": "MOSAIKS Training Manual",
    "section": "What will I learn?",
    "text": "What will I learn?\nThroughout this course, you’ll learn practical applications through a combination of:\n\nTheoretical foundations and conceptual understanding\nHands-on exercises with real-world data\nBest practices for implementation\nStrategies for analyzing and interpreting results\n\nThe curriculum covers the complete MOSAIKS workflow, including:\n\nAccessing and processing satellite imagery\nUnderstanding MOSAIKS feature extraction\nWorking with the MOSAIKS API\nImplementing machine learning models\nQuantifying and communicating uncertainty\nApplying models to various contexts\nWorking with survey data\n\nWhether you’re new to MOSAIKS or looking to deepen your expertise, this course provides the tools and knowledge needed to effectively utilize this framework.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "MOSAIKS Training Manual",
    "section": "Course structure",
    "text": "Course structure\nThis course is designed as an intensive two-week program that combines lectures, demonstrations, and hands-on sessions. Each day is structured as follows:\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n9:00 - 10:30\nMorning Session 1\n\n\n10:30 - 11:00\nBreak\n\n\n11:00 - 12:30\nMorning Session 2\n\n\n12:30 - 1:30\nLunch\n\n\n1:30 - 3:00\nAfternoon Session 1\n\n\n3:00 - 3:30\nBreak\n\n\n3:30 - 4:30\nAfternoon Session 2\n\n\n4:30 - 5:00\nFeedback and Development\n\n\n\n\n\nTable 1: Daily time divisions showing 2 morning sessions, 2 afternoon sessions, and a final time slot alloted for developing the course further.\n\n\n\nEach day concludes with a Q&A and feedback session from 4:30-5:00, providing opportunities to clarify concepts and share ideas. It is expected that this first course will spur many new ideas and concepts which should be included in the future trainings. Please remember to take notes throughout each day with particular emphasis in areas you think could be explained better or that need additional topics covered. These can be areas that you struggled with or that you would anticipate could be difficult for others.\nWhile the in-person version of this course was designed with the schedule outlined above, other users can easily navigate throughout the course content as desired, noting that the chapter content is designed for sequential learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "MOSAIKS Training Manual",
    "section": "Course schedule",
    "text": "Course schedule\n\nWeek 1\n\nDay 1: MOSAIKS framework, accessing features, basic workflow\nDay 2: Understanding ground truth data, data cleaning, spatial resolution\nDay 3: Agricultural yield prediction, downloading features via API\nDay 4: Types of satellite imagery, processing considerations, quality control\nDay 5: Feature computation, theory behind RCFs, hands-on implementation\n\n\n\nWeek 2\n\nDay 1: Martin Luther King Jr. Day - No Class\nDay 2: Model selection, hyperparameter tuning, cross-validation\nDay 3: Error analysis, confidence intervals, reporting uncertainty\nDay 4: Survey design principles, geolocation methods, sampling strategies\nDay 5: Advanced topics, emerging applications, future development",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#training-expectations",
    "href": "index.html#training-expectations",
    "title": "MOSAIKS Training Manual",
    "section": "Training expectations",
    "text": "Training expectations\n\nWhat you will learn\n\nUnderstanding of MOSAIKS framework and capabilities\nPractical skills in satellite imagery processing\n\nExperience with machine learning applications\nHands-on practice with real-world datasets\nKnowledge of survey data integration\nBest practices for model implementation\n\n\n\nPrerequisites\nThere are no explicit prerequisites, though this course does cover some advanced topics in:\n\nThe Python programming language\nMachine learning\nGeospatial data\n\n\n\nParticipant expectations\n\nActive participation in discussions and hands-on sessions\nCompletion of assigned homework (particularly the Week 1 Friday assignment)\n\nEngagement in Q&A sessions\nContribution to feedback sessions for course improvement\n\n\n\nComputing requirements\nThe course includes hands-on computing sessions. You will need:\n\nA computer with access to the internet\nA Google account\nAccess to Google Colaboratory\nAccess to necessary data (details to be provided)\n\n\n\nHomework and presentations\nThere will be a homework assignment at the end of Week 1, which participants will present on Tuesday of Week 2. This assignment is designed to reinforce learning and provide practical experience with MOSAIKS tools.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#book-structure-and-content",
    "href": "index.html#book-structure-and-content",
    "title": "MOSAIKS Training Manual",
    "section": "Book structure and content",
    "text": "Book structure and content\nThis manual is organized into six main parts, each focusing on a critical aspect of MOSAIKS. We begin with foundational concepts and gradually progress to more advanced topics in modeling and uncertainty quantification.\n\n\n\n\n\n\n\n\n\n\nPart\nDescription\n\n\n\n\nIntroduction\nComputing setup, MOSAIKS overview, API access, initial demonstration\n\n\nLabel data\nUnderstanding suitable labels, survey integration, data preparation\n\n\nSatellite imagery\nSelecting appropriate imagery, processing considerations\n\n\nFeatures\nRandom convolutional features, API access, feature computation\n\n\nTask modeling\nModel selection, spatial analysis, temporal considerations\n\n\nModel uncertainty\nUncertainty quantification, ethical considerations\n\n\n\n\n\nTable 2: Overview of the MOSAIKS Training Manual contents\n\n\n\nThe content is designed to be both comprehensive and practical, with each part building upon previous concepts while remaining relatively self-contained. This structure allows readers to either progress through the manual sequentially or focus on specific topics of interest. Throughout each section, we provide practical examples, code demonstrations, and best practices drawn from real-world applications.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "MOSAIKS Training Manual",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMOSAIKS was developed and is supported by a large team of researchers across multiple partner organizations:\nDevelopment Team:\nBenjamin Recht, Cullen Molitor, Darin Christensen, Esther Rolf, Eugenio Noda, Grace Lewin, Graeme Blair, Hannah Druckenmiller, Hikari Murayama, Ian Bolliger, Jean Tseng, Jessica Katz, Jonathan Proctor, Juliet Cohen, Karena Yan, Luke Sherman, Miyabi Ishihara, Shopnavo Biswas, Simon Greenhill, Solomon Hsiang, Steven Cognac, Tamma Carleton, Taryn Fransen, Trinetta Chong, Vaishaal Shankar\nMOSAIKS Training Manual Team: Cullen Molitor, Tamma Carleton, Esther Rolf, Sean Luna McAdams, Heather Lahr\nPartner Organizations:\n\nCenter for Effective Global Action (CEGA; UCB)\nEnvironmental Markets Lab (emLab; UCSB)\nGlobal Policy Lab (GPL; Stanford University)\nProject on Resources and Governance (PRG; UCLA)\nMaster of Environmental Data Science Program (MEDS; UCSB)\n\nFunding Support:\n\nThe Patrick J. McGovern Foundation\nThe Fund for Innovation in Development (FID)\nThe United States Agency for International Development (USAID)\nUnited Nations Development Programme (UNDP)\n\nWe are grateful for the support and contributions of all team members and partner organizations in making MOSAIKS a reality. We hope to continue expanding the framework and its applications to address pressing global challenges.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the first part of this book, we will cover the basics of MOSAIKS, including its framework, capabilities, and practical applications. This section is focused on exploring the original MOSAIKS publication (Rolf et al. 2021) and understanding the core concepts behind the framework.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html",
    "href": "part-00-intro/00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Overview\nThis section introduces the fundamental concepts of MOSAIKS (Multi-task Observation using Satellite Imagery & Kitchen Sinks) and provides practical guidance for getting started with the system. Whether you’re new to satellite imagery analysis or an experienced practitioner, understanding these foundations is crucial for effectively utilizing MOSAIKS in your work.\nMOSAIKS bridges the gap between the vast potential of satellite imagery and practical applications by providing:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#overview",
    "href": "part-00-intro/00-intro.html#overview",
    "title": "Introduction",
    "section": "",
    "text": "Accessible machine learning tools for non-experts\nComputationally efficient predictions\nFlexible applications across diverse tasks\nScalable solutions for global challenges",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#when-to-use-mosaiks",
    "href": "part-00-intro/00-intro.html#when-to-use-mosaiks",
    "title": "Introduction",
    "section": "When to use MOSAIKS",
    "text": "When to use MOSAIKS\nMOSAIKS is particularly valuable when you need to:\n\nGenerate predictions across large geographic areas\nWork with limited computational resources\nAnalyze multiple outcomes using the same imagery\nCreate predictions without deep learning expertise\nScale analysis from local to global applications\n\nHowever, MOSAIKS may not be the best choice when:\n\nYou need predictions at sub-kilometer resolution\nYour outcome requires specific spectral bands\nReal-time predictions are essential\nYour application requires interpretable features\n\n\n\n\n\n\n\nThere may be an existing tool that meets your needs. MOSAIKS is not the best choice for every application.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#section-outline",
    "href": "part-00-intro/00-intro.html#section-outline",
    "title": "Introduction",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through getting started with MOSAIKS:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n1  Compute setup\nGoogle Colab, data management, implementation practices\n\n\n2  What is MOSAIKS?\nCore concepts, system architecture, capabilities\n\n\n3  Access MOSAIKS\nAPI access, data products, authentication\n\n\n4  Try MOSAIKS\nBasic workflow, example analysis, common pitfalls\n\n\n\n\n\nTable 1: Outline of the introduction section\n\n\nThese chapters provide both theoretical understanding and practical skills needed to begin working with MOSAIKS. The focus is on making satellite-based prediction accessible while maintaining scientific rigor and computational efficiency.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll set up our computing environment using Google Colab. This course uses Colab to demonstrate various aspects of MOSAIKS, in a free and accessible environment.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html",
    "href": "part-00-intro/01-intro-compute.html",
    "title": "1  Compute setup",
    "section": "",
    "text": "1.1 Overview\nThis course primarily uses Google Colaboratory (Colab) for our computational needs. Colab is a free, cloud-based platform that allows you to write and execute Python code through your browser. It comes with many pre-installed libraries and provides free access to computing resources, including GPUs.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#requirements",
    "href": "part-00-intro/01-intro-compute.html#requirements",
    "title": "1  Compute setup",
    "section": "1.2 Requirements",
    "text": "1.2 Requirements\nTo participate in the coding portions of this course, you’ll need:\n\nA laptop or desktop computer\nA reliable internet connection\nA Google account (if you don’t have one, create one at accounts.google.com)\nA web browser (Chromium based browsers recommended)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#getting-started-with-google-colab",
    "href": "part-00-intro/01-intro-compute.html#getting-started-with-google-colab",
    "title": "1  Compute setup",
    "section": "1.3 Getting started with Google Colab",
    "text": "1.3 Getting started with Google Colab\n\n1.3.1 Accessing Colab\n\nGo to colab.research.google.com\nSign in with your Google account\nClick “New Notebook” to create your first Colab notebook\n\n\n\n1.3.2 Understanding the interface\nThe Colab interface is similar to Jupyter notebooks, with a few key components:\n\nMenu Bar: Contains options for File, Edit, View, Insert, Runtime, Tools, and Help.\nToolbar: Quick access to common actions like adding code/text cells.\nCell Area: Where you write and execute code or text.\nRuntime Status: Shows the state of your notebook’s connection to Google’s servers.\n\n\n\n1.3.3 Basic operations\n\nCreating Cells:\n\nCode cells: Click + Code. Supports Python or R code depending on the selected runtime\nText cells: Click + Text. Supports Markdown and HTML tags for documentation\n\nRunning Cells:\n\nClick the play button next to the cell or use Shift+Enter\nCan also select Runtime &gt; Run the focused cell (or another Run option) from the menu\n\n\n\n\n1.3.4 Important features\n\nRuntime Type:\n\nClick Runtime &gt; Change runtime type\nSelect Python 3 as the runtime\nFor GPU access: Change the hardware accelerator to one of the offered GPU types when needed\n\nFile Management:\n\nFiles uploaded to Colab are temporary and will be lost when the runtime disconnects\nConnect to Google Drive and save outputs there for persistent storage:\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nPackage Installation:\n\nInstall additional packages using:\n\ncondapip\n\n\n# Warning: using \"!conda install\" is not recommended. \n# As a general rule use the magic command \"%conda install\" instead.\n%conda install &lt;package_name&gt;\n\n\n# Warning: using \"!pip install\" is not recommended. \n# As a general rule use the magic command \"%pip install\" instead.\n%pip install &lt;package_name&gt;\n\n\n\n\n\n1.3.5 Best practices\n\nSave Your Work:\n\nThe links in this book will make a fresh copy of a notebook as they are saved on GitHub.\nTo save any changes you make, click File &gt; Save a copy in Drive\nDownload important notebooks locally as backups\n\nResource Management:\n\nClose unused notebooks to free up resources\nBe aware of idle timeouts (notebooks disconnect after extended inactivity)\n\nMemory Usage:\n\nMonitor memory usage through Runtime &gt; View resources\nThe free tier of Colab provides very limited memory (12GB) and may not be sufficient for large datasets or complex models\n\n\n\n\n1.3.6 Keyboard shortcuts\nHere are some useful keyboard shortcuts for working in Colab:\n\nWindows/LinuxMac\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+M H\nView keyboard shortcuts\n\n\nCtrl+Enter\nRun current cell\n\n\nShift+Enter\nRun cell and move to next\n\n\nAlt+Enter\nRun cell and insert below\n\n\nCtrl+M A\nInsert code cell above\n\n\nCtrl+M B\nInsert code cell below\n\n\nCtrl+M M\nConvert to text cell\n\n\nCtrl+M Y\nConvert to code cell\n\n\nCtrl+M D\nDelete current cell\n\n\nCtrl+M L\nToggle line numbers\n\n\nCtrl+M O\nToggle output\n\n\nCtrl+M X\nCut cell\n\n\nCtrl+M C\nCopy cell\n\n\nCtrl+M V\nPaste cell below\n\n\nShift+Up/Down\nSelect multiple cells\n\n\nCtrl+F\nFind and replace\n\n\nCtrl+S\nSave notebook\n\n\n\n\n\nTable 1.1: Windows/Linux keyboard shortcuts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut\nAction\n\n\n\n\n⌘+M H\nView keyboard shortcuts\n\n\n⌘+Enter\nRun current cell\n\n\nShift+Enter\nRun cell and move to next\n\n\nOption+Enter\nRun cell and insert below\n\n\n⌘+M A\nInsert code cell above\n\n\n⌘+M B\nInsert code cell below\n\n\n⌘+M M\nConvert to text cell\n\n\n⌘+M Y\nConvert to code cell\n\n\n⌘+M D\nDelete current cell\n\n\n⌘+M L\nToggle line numbers\n\n\n⌘+M O\nToggle output\n\n\n⌘+M X\nCut cell\n\n\n⌘+M C\nCopy cell\n\n\n⌘+M V\nPaste cell below\n\n\nShift+Up/Down\nSelect multiple cells\n\n\n⌘+F\nFind and replace\n\n\n⌘+S\nSave notebook\n\n\n\n\n\nTable 1.2: Mac keyboard shortcuts\n\n\n\n\n\n\n\n\n1.3.7 Common issues and solutions\n\nRuntime Disconnections:\n\nClick “Reconnect” when prompted\nYour variables will be reset, but saved code remains\n\nPackage Installation Issues:\n\nRestart the runtime after installing new packages\nUse Runtime &gt; Restart runtime\n\nMemory Errors:\n\nClear unnecessary variables as you go\nConsider using smaller data samples during development\n\n\n\n\n\n\n\n\nMemory errors are common when working with large datasets or complex models on the free tier of Colab. If you encounter these issues, consider using a paid version of Colab or connecting a Google Cloud Platform virtual machine (VM).\n\n\n\n\n\n1.3.8 Getting help\n\nAccess Colab’s documentation: Help &gt; Frequently Asked Questions\nTry using Google Gemini for AI assistance.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#ai-assistance-in-colab",
    "href": "part-00-intro/01-intro-compute.html#ai-assistance-in-colab",
    "title": "1  Compute setup",
    "section": "1.4 AI assistance in Colab",
    "text": "1.4 AI assistance in Colab\nGoogle Gemini is a powerful AI assistant seamlessly integrated with Google Colab. You can use it to generate code, comments, or markdown text to improve your notebooks. Gemini can be accessed in several ways in Colab, all starting by selecting the Gemini icon in different parts of the notebook editor.\n\n\n\n\n\n\nGemini icon\n\n\n\n\nLook for this icon to indicate where you can click to access Gemini in Colab.\n\n\nHere are a few ways you can use Google Gemini effectively in Colab:\n\n1.4.1 Chat support\nClick the Gemini button in the top-right corner to open a chat interface where you can ask questions about your code, debug issues, or get explanations of concepts. This option is especially useful for beginners or for tackling complex problems.\n\n\n1.4.2 Code generation\nUse the “Generate code” option (the sparkle icon) above any empty code cell to generate new code based on your description. You can ask it to do many different things including:\n\nLoading a dataset called my_data.csv\nPlotting a histogram of the data\nBuilding a model to predict y from X\n\n\n\n1.4.3 Code explanation\nUse the “Explain code” option (the sparkle icon) above any complete code cell to open a chat interface that will automatically explain the code in the cell. This is useful for understanding code written by someone else, learning new concepts, or getting a second opinion on your work.\n\n\n1.4.4 Code completion\nColab provides intelligent autocomplete as you type:\n\nPress Tab to accept suggestions\nUse Ctrl+Space (Cmd+Space on Mac) to manually trigger suggestions\nGet real-time documentation and parameter hints\n\n\n\n\n\n\n\nWhile these AI tools are helpful, always review and understand the code they suggest before using it in your work.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#accessing-course-notebooks",
    "href": "part-00-intro/01-intro-compute.html#accessing-course-notebooks",
    "title": "1  Compute setup",
    "section": "1.5 Accessing course notebooks",
    "text": "1.5 Accessing course notebooks\nAll course notebooks are hosted on GitHub and can be accessed directly in Google Colab. There are two ways to open them:\n\n1.5.1 Method 1: Direct links\nEach section of this book includes direct “Open in Colab” links for relevant notebooks. Simply click the badge to open the notebook:\nExample \nThis method will open a fresh copy of the notebook as it is saved on GitHub. If you have already clicked the badge once, made changes, and saved your notebook, then you will need to navigate to your drive folder where it is saved to access those changes.\n\n\n\n\n\n\nClicking the badge in this book will always open a fresh copy.\n\n\n\n\n\n1.5.2 Method 2: Clone the notebook\nTo select a notebook from the repository Notebook repository:\n\nOpen Google Colab (colab.research.google.com)\nClick File &gt; Open Notebook\nSelect the GitHub tab\nEnter the repository URL: https://github.com/[username]/[repo] (UPDATE WITH REPO)\nSelect the notebook you want to open\n\n\n\n1.5.3 Saving your work\nWhen you open a notebook from GitHub in Colab, it creates a temporary copy. To save your work:\n\nClick File &gt; Save a copy in Drive\nThis creates your own editable copy in your Google Drive\nAll future changes will be saved to your copy\n\n\n\n1.5.4 Notebook organization\nThe course notebooks are organized into:\n\ndemos/: Complete demonstration notebooks\nexercises/: Interactive notebooks with exercises to complete\nsolutions/: Complete versions of exercise notebooks\n\nEach notebook includes:\n\nClear instructions and explanations in markdown cells\nCode cells with examples or exercises\nTO DO sections for exercises\nValidation cells to check your work",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#data-access-and-management",
    "href": "part-00-intro/01-intro-compute.html#data-access-and-management",
    "title": "1  Compute setup",
    "section": "1.6 Data access and management",
    "text": "1.6 Data access and management\nThere are several ways to access data in Colab notebooks. Here are the main approaches:\n\n1.6.1 Direct downloads\nFor data hosted on repositories like Zenodo, you can download directly using wget:\n# Download the data\n!wget https://zenodo.org/records/14040658/files/Data.zip\n\n# Unzip the data\n!unzip Data.zip\n\n\n1.6.2 Google Drive integration\n\n1.6.2.1 Mount Google Drive\nFor data stored in Google Drive:\n\nFirst, mount your Google Drive:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nAccess your data using the mounted path:\ndrive_path = \"/content/drive/MyDrive/&lt;project_folder&gt;\"\n\n\n\n1.6.2.2 Copy data to the VM (optional)\nFor better performance, make local copies of the data on the virtual machine (VM):\nimport os\nimport shutil\n\n# Create local directory\nlocal_dir = \"/content/data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Copy data from Drive to VM\ndrive_data = os.path.join(drive_path, \"my_data\") \nshutil.copytree(drive_data, local_dir, dirs_exist_ok=True)\n\n\n\n\n\n\nRemember that the VM’s storage is temporary - files will be deleted when the runtime disconnects. Always keep a backup of your data in Drive or another permanent storage location.\n\n\n\n\n1.6.2.2.1 Why copy data to the VM?\nWhen working with data in Colab, copying files from Google Drive to the virtual machine (VM) can significantly improve performance:\n\nFaster Access: Reading directly from Google Drive requires data to be transferred over the network for each operation. Local VM storage provides much faster read/write speeds.\nReduced Latency: Network latency between Colab and Google Drive can slow down operations that require multiple data accesses. Local data eliminates this latency.\nMore Reliable: Network connectivity issues or Drive access problems won’t interrupt your analysis once data is copied locally.\nBetter for Iterative Processing: If your code needs to read the same data multiple times (like in machine learning training loops), local access is much more efficient.\n\nFor example, reading a 1 GB dataset from Drive might take 30 seconds, while reading from local VM storage could take just a few seconds. The time spent copying data once at the start of your session can save significant time during analysis. This is especially true in a notebook environment where a user may develop code that repeatedly accesses the same data files, but cannot store it all in memory (e.g., many image files).\n\n\n\n1.6.2.3 Save outputs to Google Drive\nTo save outputs or models to Google Drive:\n# Set the output directory\noutput_dir = \"/content/drive/MyDrive/project_folder/output\"\n\n# Save outputs\nshutil.copytree(local_output, output_dir, dirs_exist_ok=True)\nThis ensures that any work done in the notebook is saved to your Google Drive for future reference. If output files are not copied and remain in the VM, they will be lost when the runtime disconnects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#local-environment-setup",
    "href": "part-00-intro/01-intro-compute.html#local-environment-setup",
    "title": "1  Compute setup",
    "section": "1.7 Local environment setup",
    "text": "1.7 Local environment setup\nWhile this book’s primary approach is to use Google Colab, some learners may prefer or need to run code locally. The book is largely setup to do this, though the user will need to manage their own computing environment. For that purpose, we provide an environment.yml file (located in the environment directory of this book). Below are the steps to get you set up with Miniconda and create a local environment.\n\n\n\n\n\n\nThough local environments can offer more control, we strongly recommend Google Colab for consistency and free cloud-based resources. This local setup is purely optional and might be more suitable for those with particular dependencies or advanced setups.\n\n\n\n\n1.7.1 Downloading and installing Miniconda\nMiniconda is a minimal installer for conda. Choose the installer for your operating system from the links below and follow the prompts.\n\nWindowsmacOSLinux\n\n\n\nGo to the Miniconda Windows Installer.\nDownload the .exe installer for your Windows system (64-bit recommended).\nDouble-click the installer and follow the on-screen instructions.\nWhen prompted, check the option to Add Miniconda to PATH or select “Install for All Users” which typically adds conda to PATH automatically.\n\n\n\n\nGo to the Miniconda macOS Installer.\nDownload the .pkg (or .sh if you prefer) installer for macOS (64-bit).\nDouble-click the installer and follow the on-screen instructions.\nWhen prompted, check the option to Add Miniconda to PATH or add the appropriate path lines to your ~/.zshrc or ~/.bash_profile file manually.\n\n\n\n\nGo to the Miniconda Linux Installer.\nDownload the .sh installer for your Linux distribution (64-bit recommended).\nOpen a terminal and run bash Miniconda3-latest-Linux-x86_64.sh.\nFollow the prompts; consider allowing the installer to initialize Miniconda for your shell (adding conda to your PATH).\n\n\n\n\n\n\n1.7.2 Adding conda to your PATH\nIf you did not add conda to your PATH during installation, you can manually do so by adding a line to your shell configuration file (~/.bashrc, ~/.zshrc, or similar):\n# Example for Linux/macOS users\nexport PATH=\"$HOME/miniconda3/bin:$PATH\"\nFor Windows, ensure that you selected the option to add conda to PATH during installation, or run the Anaconda Prompt (which automatically has conda available) to manage your environment.\n\n\n1.7.3 Creating a local environment from environment.yml\nIn the environment directory of the course repository, you will find a file named environment.yml. This file lists all the packages needed for the local setup.\n\nClone or download the book repository to your local machine.\nOpen a terminal (or Anaconda Prompt on Windows).\nNavigate to the folder containing environment.yml.\ncd path/to/MOSAIKS-Training-Manual/environment\nCreate the environment:\nconda env create -f environment.yml\nActivate the environment:\nconda activate &lt;environment_name&gt;\nWhere &lt;environment_name&gt; is the name specified in environment.yml (check the name: field in the file). In this case the name is mosaiks.\n\n\n\n1.7.4 Using the new environment in VS Code\nVisual Studio Code (VS Code) can detect and use your new conda environment for Python development.\n\nOpen VS Code.\nInstall the Python extension (if not already installed).\nPress Ctrl+Shift+P (or Cmd+Shift+P on macOS) and type “Python: Select Interpreter”.\nSelect the interpreter associated with your newly created environment (it should be listed by name or path).\nOpen or create a new Python file or notebook, and verify that VS Code is using the correct environment (you can see the chosen environment in the bottom-right corner of VS Code).\n\n\n\n1.7.5 Other environment managers\nWhile conda is a common tool for managing Python environments, there are other popular options such as:\n\nPoetry\n\npipenv\n\nvirtualenv\n\nEach has its own configuration files and setup instructions. If you prefer these tools or already use them, you can typically replicate the packages listed in environment.yml. Check the respective tool’s documentation for specific instructions on how to translate the dependencies.\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will take a closer look at the MOSAIKS framework, its core concepts, and how it can be applied to solve real-world problems.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Compute setup</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html",
    "href": "part-00-intro/02-intro-mosaiks.html",
    "title": "2  What is MOSAIKS?",
    "section": "",
    "text": "2.1 The challenge\nRight now, numerous public satellite systems collect huge amounts of data about the world every day. But there is so much imagery (terabytes per day) that it’s overwhelming to sort through by hand; and it’s too complex and unstructured to be usable in its raw form for most applications.\nThat is why linking satellite imaging to machine learning (sometimes referred to as SIML or SatML) is incredibly powerful. It enables vast amounts of unstructured image data to be transformed into structured information that can be used for planning, research, and decision-making.\nOur hope is that people all over the world can access and use SIML technologies, but we recognize that many who would benefit from these tools don’t have the time or resources to manage enormous satellite imagery data sets and learn how to apply machine learning to them.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#the-challenge",
    "href": "part-00-intro/02-intro-mosaiks.html#the-challenge",
    "title": "2  What is MOSAIKS?",
    "section": "",
    "text": "Figure 2.1: Visual representation of satellites orbiting earth.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#the-solution",
    "href": "part-00-intro/02-intro-mosaiks.html#the-solution",
    "title": "2  What is MOSAIKS?",
    "section": "\n2.2 The solution",
    "text": "2.2 The solution\nThat’s why we developed MOSAIKS. MOSAIKS aims to lower the barriers to entry into SIML, diversifying the users of this powerful technology and the problems we solve with it.\n\n\n\n\n\n\nMOSAIKS\n\n\n\nMulti-task Observation using SAtellite Imagery & Kitchen Sinks\n\n\nMOSAIKS is designed to work “out of the box” for a wide array of SIML applications, for people with no SIML expertise who work on normal desktop or laptop computers. For many applications, MOSAIKS users never have to touch satellite imagery themselves and only need to have basic statistical training.\n\nIf you can run a regression, you can use MOSAIKS!\n\nMOSAIKS empowers users to create their own new datasets from satellite imagery. We don’t control what variables users look at, and we never need to know. MOSAIKS is a system that allows users to quickly transform vast amounts of imagery into maps of new variables, using their own training data.\nIf you’ve ever been curious about trying machine learning with satellite imagery, but don’t know anything about machine learning or satellite imagery, MOSAIKS is for you.\nAnd if you know a lot about machine learning and satellite imagery, MOSAIKS might still be for you, since it performs competitively with deep learning methods but is much simpler and cheaper to use.\n\n\n\n\n\n\n\nFigure 2.2: Traditional framework of deep learning models (i.e., machine learning with artificial neural networks) applied to imagery. In this example, the model is attempting to classify what vehicle the image contains.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#how-mosaiks-works",
    "href": "part-00-intro/02-intro-mosaiks.html#how-mosaiks-works",
    "title": "2  What is MOSAIKS?",
    "section": "\n2.3 How MOSAIKS works",
    "text": "2.3 How MOSAIKS works\n\n\n\n\n\n\nRecommended reading\n\n\n\nA generalizable and accessible approach to machine learning with global satellite imagery (Rolf et al. 2021)\n\n\n\n2.3.1 Separating users from imagery\nThe basic idea of MOSAIKS is to separate users from the costly and difficult process of transforming imagery into inputs (called “features”) to a downstream machine learning algorithm (images → X). The MOSAIKS team has computed these features globally, so in many use cases users never have to download or manage imagery themselves. Instead, users download a table of MOSAIKS features (X), link them to their own geocoded data on the outcome (Y) they are interested in predicting from satellite imagery (we call these data “labels”), and then run a linear regression (or something fancier if desired!) to predict their labels using MOSAIKS features (Y = Xβ). Importantly, this prediction can be performed in locations, time periods, and at spatial resolutions for which labels are not available.\n\n\n \n\n\nFigure 2.3: This false-color image shows snow-capped peaks and ridges of the eastern Himalayas between major rivers in southwest China. The Himalayas are made up of three parallel mountain ranges that together stretch for more than 1800 miles (2,900 kilometers). This particular image was taken by NASA’s Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), flying aboard the Terra satellite, on February 27, 2002. The picture is a composite made by combining near-infrared, red and green wavelengths. (Source: NASA)\n\n\n\n2.3.2 Generalizability of MOSAIKS\nBecause MOSAIKS features synthesize information contained in raw imagery that is not tailored for any specific outcome (e.g., biodiversity, household income, land use), many users can use the same MOSAIKS features and simply match them to their own labels based on location. Users can run their analysis on any statistical software they are comfortable with. For most applications, the computing demands will not require users to work with specialized machines, since desktops and laptops work.\n\n\n\n\n\nFigure 2.4: MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context. A Satellite imagery is shared across multiple potential tasks. B Schematic of the MOSAIKS process. N images are transformed using random convolutional features into a compressed and highly descriptive K-dimensional feature vector before labels are known. Once features are computed, they can be stored in tabular form (matrix X) and used for unlimited tasks without recomputation. Users interested in a new task (s) merge their own labels (ys) to features for training. Here, user 1 has forest cover labels for locations p + 1 to N and user 2 has population density labels for locations 1 to q. Each user then solves a single linear regression for βs. Linear prediction using βs and the full sample of MOSAIKS features X then generates SIML estimates for label values at all locations. Generalizability allows different users to solve different tasks using an identical procedure and the same table of features—differing only in the user-supplied label data for training. Each task can be solved by a user on a desktop computer in minutes without users ever manipulating the imagery. C Illustration of the one-time unsupervised computation of random convolutional features. K patches are randomly sampled from across the N images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image. (Source: Rolf et al. 2021 Figure 1)\n\n\n\n2.3.3 Why it works\nMOSAIKS works because MOSAIKS features capture a huge amount of information about the colors, patterns and textures that show up in satellite imagery. We don’t know what patterns/colors/textures will be important for the application that users have (since we don’t know what applications users will try), so we just try to capture all of them. The purpose of the regression step is to teach the model which patterns/colors/textures predict the labels, and then to use that understanding to make predictions in locations where users don’t have labels. In addition, MOSAIKS encodes image information in a way that allows for nonlinear relationships between labels and imagery data, even though the regression that users generally implement is a linear regression.\n\n\n\n\n\nFigure 2.5: Example of 4 (of 4,000) MOSAIKS feature maps (right) computed from satellite imagery (left). These features were chosen at random from what is available on the MOSAIKS API.\n\n\nFor learn more about these features, see Chapter 12 where we attempt to provide intuition for what a feature is and how it is made.\n\n2.3.4 Five steps to using MOSAIKS\n\n\n\n\n\n\nThis section is a very broad overview of the steps to use MOSAIKS. Later chapters will provide more detailed guidance on each step.\n\n\n\nIn many cases, users aiming to predict an outcome from satellite imagery can do so using pre-computed imagery features (X) in a simple linear regression framework. Later in this training course, we will detail more customized workflows that remain tractable but allow for more flexibility. In the standard case, however, the procedure for using MOSAIKS has five steps (corresponding figure from Rolf et al. is below):\n\nDownload pre-computed MOSAIKS features (X) corresponding to the locations where you have labels (Chapter 3).\nMerge the features with your labels (Y) based on location (so features at position P are linked to labels at position P) (Chapter 7).\nRun a cross-validated ridge regression of your labels on the MOSAIKS features (Y = Xβ + e; or any other model you choose! See ?sec-sec-model-choice).\nEvaluate performance.\nUse the results of the regression model (β) to make predictions (Xβ) in a new region of interest where you do not have labels, using only the MOSAIKS features (X) that correspond with those new locations.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#what-can-mosaiks-predict",
    "href": "part-00-intro/02-intro-mosaiks.html#what-can-mosaiks-predict",
    "title": "2  What is MOSAIKS?",
    "section": "\n2.4 What can MOSAIKS predict?",
    "text": "2.4 What can MOSAIKS predict?\n\n\n\n\n\n\nThis question is answered in greater detail in Chapter 5\n\n\n\nMOSAIKS has been successfully used to predict a wide range of outcomes including:\n\nEnvironmental conditions (forest cover, elevation)\nPopulation patterns (density, nighttime lights)\nEconomic indicators (income, house prices)\nInfrastructure (road networks)\n\nThe figure below is from the original MOSAIKS publication (Rolf et al. 2021). The left maps show the input labels. The right map shows the modeled predictions. The scatter plot shows the modeled predictions against the true labels and reports the coefficient of determination (R²) as a measure of performance.\n\n\n\n\n\nFigure 2.6: (100,000 daytime images were each converted to 8,192 features and stored. Seven tasks were then selected based on coverage and diversity. Predictions were generated for each task using the same procedure. Left maps: 80,000 observations used for training and validation, aggregated up to 20 km × 20 km cells for display. Right maps: concatenated validation set estimates from 5-fold cross-validation for the same 80,000 grid cells (observations are never used to generate their own prediction), identically aggregated for display. Scatters: Validation set estimates (vertical axis) vs. ground truth (horizontal axis); each point is a ~1 km × 1 km grid cell. Black line is at 45∘. Test-set and validation set performance are essentially identical; validation set values are shown for display purposes only, as there are more observations. The tasks in the top three rows are uniformly sampled across space, the tasks in the bottom four rows are sampled using population weights; grey areas were not sampled in the experiment. Source: Rolf et al. 2021 Figure 2)\n\n\nImportantly, all these predictions use the same set of satellite features - there’s no need to reprocess the imagery for different tasks. MOSAIKS achieves accuracy comparable to more complex deep learning methods, but at a fraction of the computational cost. This is the power of MOSAIKS, it removes the need for reprocessing the imagery after the initial encoding.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#is-mosaiks-always-the-best-choice",
    "href": "part-00-intro/02-intro-mosaiks.html#is-mosaiks-always-the-best-choice",
    "title": "2  What is MOSAIKS?",
    "section": "\n2.5 Is MOSAIKS always the best choice?",
    "text": "2.5 Is MOSAIKS always the best choice?\nNo! MOSAIKS is a powerful tool, but it is not always the best choice for every application. We recommend that users first start by searching for existing methods that have been developed for their specific application. An excellent place to begin this search is at satellite-image-deep-learning where you can find a list of deep learning methods that have been developed for satellite imagery, as well as existing datasets, tools and tutorials.\nThe world of SIML is vast and rapidly evolving. This means there is a good choice you do not have to make global scale predictions yourself. Instead, you might be able to use or build off the hard work of many others in the field.\nIf you have a specific context where you want tailored information or a variable/outcome no one else has predicted before, then you want MOSAIKS. Not only will MOSAIKS allow you to make predictions in a new context, but it will also allow you to do so quickly and with minimal computational resources.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#summary",
    "href": "part-00-intro/02-intro-mosaiks.html#summary",
    "title": "2  What is MOSAIKS?",
    "section": "\n2.6 Summary",
    "text": "2.6 Summary\nMOSAIKS is a powerful tool that allows users to predict a wide range of outcomes from satellite imagery using pre-computed features. The system is designed to be accessible to users with no prior experience in machine learning or satellite imagery. The MOSAIKS framework involves five simple steps\n\nDownload features\nMerge with labels\nrun a regression\nEvaluate performance\nMake predictions\n\nIn this book we will explore all the ways in which this is an oversimplification. You will learn to adapt this framework to your own needs, and to understand the limitations and assumptions of the MOSAIKS system. Many skills presented in this training manual will be applicable to other satellite imagery and machine learning workflows.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, you will be introduced to the MOSAIKS API which is a free and open resource for accessing pre-computed MOSAIKS features.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is MOSAIKS?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html",
    "href": "part-00-intro/03-intro-api.html",
    "title": "3  Access MOSAIKS",
    "section": "",
    "text": "3.1 Introduction\nAt its core, MOSAIKS requires two main inputs: satellite features and ground truth data. Our aim is to make these features as accessible as possible so that the majority of users do not have to worry about the technical details of satellite imagery processing.\nTo this end, we have worked to develop multiple ways to access MOSAIKS features:\nThe MOSAIKS API should be considered the primary way to access features. It is a user-friendly interface that allows you to download features for any location on Earth. The API is designed to be accessible to users with a range of technical backgrounds, from beginners to experts. For more details on what features are available on the API, see Chapter 13.\nHowever, there are many settings where users will want or need to compute their own customized features. Chapter 14 guides readers through this process.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#introduction",
    "href": "part-00-intro/03-intro-api.html#introduction",
    "title": "3  Access MOSAIKS",
    "section": "",
    "text": "Option\nImagery Source\nSpatial Coverage\nSpatial Resolution\nTemporal Resolution\nWeighting\n\n\n\nMOSAIKS API\nPlanet Labs Visual Basemap\nGlobal land areas\n0.01°\n2019 Q3\nUnweighted\n\n\nMOSAIKS API\nPlanet Labs Visual Basemap\nGlobal land areas\n0.1°, 1°\n2019 Q3\nArea & population\n\n\nMOSAIKS API\nPlanet Labs Visual Basemap\nGlobal land areas\nADM0, ADM1, ADM2\n2019 Q3\nArea & population\n\n\nRolf et al 2021\nGoogle Static Maps\nContinental United States (~100k locations)\n0.01°\n2019\nunweighted\n\n\nChapter 14\nAny - see Chapter 9\n\nUser-defined\nUser-defined\nUser-defined\nUser-defined\n\n\n\n\n\nTable 3.1: Summary of MOSAIKS feature sources. The weighting column indicates that the features were generated at 0.01 degree resolution and are weighted when they are aggregated up to lower resolutions. The available weighting schemes are based on area of the aggregation polygon, or by the population of the the aggregation polygon.\n\n\n\n\n\n\n\n\n\nChapters 1 through 8 focus on publicly available features\nChapters 9 through 15 cover computing custom features from imagery",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#mosaiks-api",
    "href": "part-00-intro/03-intro-api.html#mosaiks-api",
    "title": "3  Access MOSAIKS",
    "section": "\n3.2 MOSAIKS API",
    "text": "3.2 MOSAIKS API\n\n\n\n\n\n\nMOSAIKS API Link\n\n\n\napi.mosaiks.org\n\n\nThe MOSAIKS API is a user-friendly interface that allows you to download features for any land location on Earth. The API is designed to be accessible to users with a range of technical backgrounds, from beginners to experts. To take advantage of the API, you will need to register for an account.\n\n3.2.1 Register for an account\nVisit api.mosaiks.org.\n\n\n\n\n\nFigure 3.1: Login page for the MOSAIKS API.\n\n\nSelect Register to create an account. You will need to provide a username, an email, and a password.\n\n\n\n\n\nFigure 3.2: Registration page for the MOSAIKS API.\n\n\nOnce registered, you can log in to begin downloading MOSAIKS features.\n\n\n\n\n\nFigure 3.3: MOSAIKS API landing page.\n\n\n\n3.2.2 API resources\nFrom the landing page, you can read additional information about MOSAIKS and access resources to help you get started.\n\n\n\n\n\n\nThis book is developed to provide you with all the information you need to use MOSAIKS.\n\n\n\nThe API contains the following pages:\n\n\n\n\n\n\n\n\nPage\nDescription\n\n\n\nHome\nLanding page for the API. Contains general information about using MOSAIKS and the API\n\n\nPrecomputed Files\nPrecomputed features at administrative boundary scales\n\n\nHDI\nGlobal Human Development Index (HDI) estimates at the municipality and grid levels\n\n\nGlobal Grids\nPrecomputed and area or population features at 0.1° and 1° resolution\n\n\nMap Query\nPrecomputed features at 0.01 degree resolution, user defines bounding box over area of interest\n\n\nFile Query\nPrecomputed features at 0.01 degree resolution, user uploads file with latitude and longitude coordinates\n\n\nMy Files\nFiles you have queried from the API, available to download\n\n\nResources\nExample Python and R notebooks for using the MOSAIKS framework\n\n\n\n\n\nTable 3.2: MOSAIKS API pages and descriptions.\n\n\n\n3.2.3 API features\nCurrently the MOSAIKS API has a single set of global features (with several aggregation levels). The features are freely available to the public for download; this is the fastest and easiest way to begin using MOSAIKS.\nThe API features use input imagery from Planet Labs, Inc. Visual Basemap Global Quarterly 2019 (quarter 3) product. Image quality, and therefore feature quality, may be affected by local conditions. For example, an area undergoing a rainy season in the third quarter (July to September), is likely to contain image artifacts from cloud cover. This will in turn effect the feature calculations. For more details on the input imagery Chapter 9.\nGiven the static nature of the API, the easiest way to get started with MOSAIKS is to have label data for a recent time period (ideally from 2019 for fast changing labels, or a close year for more steady labels).\n\n\n\n\n\nFigure 3.4: Planet Labs basemap imagery (left) and 4 of 4,000 MOSAIKS features downloaded from the API (right).\n\n\n\n\n\n\n\n\nUsing MOSAIKS for time series data is possible and can work well, however this currently requires computing your own custom features. See Chapter 14 and Chapter 18 for more information.\n\n\n\nThe MOSAIKS features are created using a 0.01 x 0.01 degree latitude-longitude global land grid. These are the native features available for download from the API, but it also offers pre-aggregated features at 0.1 and 1 degree resolution, as well as administrative boundaries (ADM0, ADM1, and ADM2).\n\n\n3.2.4 High resolution features\nThe file query and map query are the two methods to obtain the high resolution (0.01 degree) features through the API. For simplicity, we store these features in a tabular format with latitude and longitude coordinates. These coordinates are the center of each grid cell.\nWhen you download the high resolution features (0.01 degree), you will receive them in a tabular .csv format where:\n\nEach row (N) represents a unique grid cell\nThe first two columns contain latitude and longitude coordinates (grid centroids)\nThe remaining columns represent K features (currently K = 4000 features)\n\n\nNote: There is a limit of N = 100,000 records per query\n\n\n3.2.4.1 Map query\n\nCreate rectangular boxes by specifying latitude and longitude coordinates\nMultiple boxes can be created\nThe system displays an estimated number of records for each box\nNote that estimates are based on box area and may not reflect actual record numbers, especially for areas containing seas and oceans\n\n\n\n\n\n\nFigure 3.5: MOSAIKS API Map Query page.\n\n\n\n\n\n\n\n\nUse geojson.io to find the bounding box coordinates for your area of interest.\n\n\n\n\n3.2.4.2 File query\n\nSubmit a file with custom latitude and longitude coordinates\nThe API returns features for grid cells closest to your input coordinates\nPoints are allocated to the nearest grid point if they don’t exactly match\nThe output file may have a different number of rows than your input\nPoint ordering may change in the output\n\n\n\n\n\n\nFigure 3.6: API File Query\n\n\n\n3.2.5 Aggregated features\nMany users may find it easier to work with features aggregated to some level. The MOSAIKS API offers pre-aggregated features to accommodate these needs. The API offers several levels, including larger grid cells (0.1 and 1 degree) or summarized to administrative boundaries (ADM0, ADM1, and ADM2). These files are available for download as either single or chunked files depending on the resolution.\n\n\n\n\n\nFigure 3.7: Example showing of 3 representative random convolutional features (rows). Features are downloaded from the MOSAIKS API at 0.01° resolution (the native resolution) and aggregated to 3 levels, including (A) larger grid cells (0.1°), (B) counties, and (C) states.\n\n\n\n3.2.5.1 Weighting schemes\nAt each level of aggregation, we offer area weighted features and population weighted features. Population weights are from the Gridded Population of the World (GPWv4) population density dataset. The area weighting scheme is based on the area of the high resolution grid cells.\n\n3.2.5.2 When to use aggregated features\nThe aggregated features are particularly useful in a few scenarios:\n\nYou have data at a scale larger than the 0.01 degree grid cells. Many datasets come at the country, state, or county level.\nYour data has a lot of noise that can be smoothed out by aggregating to a larger scale. A common example of this might be household survey data that is noisy at the individual level but smooths out when aggregated to the village or district level.\nYou want to do global analysis and don’t have the computational resources to work with the full 0.01 degree grid cells.\n\nIn all cases using the pre-aggregated features can save you time and computational resources.\n\nScenario: You are working with a Living Standards Measurement Study - Integrated Surveys on Agriculture (LSMS-ISA). This dataset has survey data with geographic coordinates at the household level. To protect, the privacy of the respondents, the data is jittered within a 5 km radius but it always remains within local administrative boundaries. You can therefore summarize your labels to the administrative units and build a model with the aggregated features.\n\n\n\n\n\n\n\nIf you want to make high resolution predictions, with low resolution label data, you can build your model with aggregated features and use the high resolution features to make predictions. This will be covered in Chapter 17.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#using-mosaiks-features-for-prediction",
    "href": "part-00-intro/03-intro-api.html#using-mosaiks-features-for-prediction",
    "title": "3  Access MOSAIKS",
    "section": "\n3.3 Using MOSAIKS features for prediction",
    "text": "3.3 Using MOSAIKS features for prediction\n\n\n\n\n\n\nThis is a brief overview. Detailed instructions appear later in the manual (Chapter 16).\n\n\n\nBasic workflow:\n\nObtain ground truth measurements (“labels”; see Chapter 5)\nDownload matching features (see Chapter 13 for more details).\nSpatially merge labels and features (see Chapter 7)\nUse regression to model relationship between imagery and outcome (see Chapter 16)\nUse regression results to predict outcome in new locations (see Chapter 16)\n\nYou can experiment with various machine learning approaches in the regression step. For beginners, we recommend starting with our example Jupyter notebook (Chapter 4) that demonstrates a simple ridge regression approach (suitable for both R and Python users).\n\n\n\n\n\nFigure 3.8: Using MOSAIKS, a simplified workflow design.\n\n\nThis topic will be covered in greater depth in later chapters (see Chapter 16). In the next chapter, you will see a simple MOSAIKS workflow which replicates the results of Rolf et al. 2021.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#citation-requirements",
    "href": "part-00-intro/03-intro-api.html#citation-requirements",
    "title": "3  Access MOSAIKS",
    "section": "\n3.4 Citation requirements",
    "text": "3.4 Citation requirements\nWhen referring to the MOSAIKS methodology or when generating MOSAIKS features, please reference: Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\nYou can use the following Bibtex:\n@article{article,\n    author = {Rolf, Esther and Proctor, Jonathan and Carleton, Tamma and Bolliger, Ian and Shankar, Vaishaal and Ishihara, Miyabi and Recht, Benjamin and Hsiang, Solomon},\n    year = {2021},\n    month = {07},\n    pages = {},\n    title = {A generalizable and accessible approach to machine learning with global satellite imagery},\n    volume = {12},\n    journal = {Nature Communications},\n    doi = {10.1038/s41467-021-24638-z}\n}\nIf using features downloaded from the API, please reference, in addition to the publication above, the MOSAIKS API.\nYou can cite the API using the following Bibtex:\n @misc{MOSAIKS API,\n    author = {{Carleton, Tamma and Chong, Trinetta and Druckenmiller, Hannah and Noda, Eugenio and Proctor, Jonathan and Rolf, Esther and Hsiang, Solomon}},\n    title = {{Multi-Task Observation Using Satellite Imagery and Kitchen Sinks (MOSAIKS) API}},\n    howpublished = \"\\url{ https://api.mosaiks.org }\",\n    version = {1.0},\n    year = {2022},\n}\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter you will have a chance to try MOSAIKS on Google Colab with the data from Rolf et al 2021.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Access MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html",
    "href": "part-00-intro/04-intro-demo.html",
    "title": "4  Try MOSAIKS",
    "section": "",
    "text": "4.1 Overview\nThis demo replicates key results from the original MOSAIKS publication (Rolf et al. 2021). While MOSAIKS has great potential to improve access to satellite-based prediction in data-sparse environments, the original paper focused on demonstrating performance in the United States where high-quality training data was readily available.\nThe US served as an ideal testing ground for several reasons:\nThis validation in a data-rich environment was crucial for establishing MOSAIKS as a reliable tool before deploying it in contexts where ground truth data is scarce or unreliable.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#overview",
    "href": "part-00-intro/04-intro-demo.html#overview",
    "title": "4  Try MOSAIKS",
    "section": "",
    "text": "Extensive ground truth data available across multiple variables\nReliable spatial referencing of data\nDiverse landscapes and built environments\nAbility to benchmark against existing methods\nSystematic validation of predictions",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#demonstration-code",
    "href": "part-00-intro/04-intro-demo.html#demonstration-code",
    "title": "4  Try MOSAIKS",
    "section": "\n4.2 Demonstration code",
    "text": "4.2 Demonstration code\n\n4.2.1 Workflow\nBelow is a link to a Jupyter notebook intended to demonstrate practical use of MOSAIKS with real data. In fact, this notebook uses the original input data and features from Rolf et al. 2021. The code demonstrates:\n\nLoading pre-computed MOSAIKS features and labels\nMerging the features and labels\nTraining a ridge regression model\nEvaluating predictions\nVisualizing results\n\n4.2.2 Label data\nThe demo showcases MOSAIKS predicting several variables, and with a subset of the data used in, the original paper. The variables include:\n\n\nForest Cover\nElevation\nPopulation Density\nNighttime Lights\nIncome\nRoad Length\n\n\n\n\n\n\n\n\nFigure 4.1: Forest cover input data (left) from Global Land Analysis & Discover (GLAD) Global 2010 Tree Cover (30 m)\n\n\n\n\n\n\n\n\n\nFigure 4.2: Elevation input data (left) provided by Mapzen, and accessed via the Amazon Web Services (AWS) Terrain Tile service. Download code can be found here.\n\n\n\n\n\n\n\n\n\nFigure 4.3: Population density input data (left) from the Gridded Population of the World (GPW) dataset. These data can be accessed here.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Nighttime lights luminosity input data (left) generated from nighttime satellite imagery, which is provided by the Earth Observations Group at the National Oceanic and Atmospheric Administration (NOAA) and the National Geophysical Data Center (NGDC). These data can be accessed here.\n\n\n\n\n\n\n\n\n\nFigure 4.5: Income input data (left) from the American Community Survey (ACS) 5-year estimates of median annual household income in 2015. These data are accessible using the acs package in R (48), table number B19013\n\n\n\n\n\n\n\n\n\nFigure 4.6: Road length input data (left) from the United States Geological Survey (USGS) National Transportation Dataset, which is based on TIGER/Line data provided by US Census Bureau in 2016. These data can be accessed here.\n\n\n\n\n\nA user simply needs to select which variable they would like to predict, and no other changes need to be made to the code. All data has been preprocessed and the code will download the necessary files from Zenodo.\n\n4.2.3 Constraints\nTo stay within the Colab free tier limits of memory usage, we subset the data. We take a 50% random sample of both features (K=4,000 instead of 8,192) and observations (N=50,000 instead of 100,000) compared to the original paper. Despite using this reduced dataset, the demo still achieves strong predictive performance, highlighting MOSAIKS’s efficiency.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#run-the-code",
    "href": "part-00-intro/04-intro-demo.html#run-the-code",
    "title": "4  Try MOSAIKS",
    "section": "\n4.3 Run the code!",
    "text": "4.3 Run the code!\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#dont-want-to-run-code",
    "href": "part-00-intro/04-intro-demo.html#dont-want-to-run-code",
    "title": "4  Try MOSAIKS",
    "section": "\n4.4 Don’t want to run code?",
    "text": "4.4 Don’t want to run code?\nConsider watching this demonstration instead!\n\n\n\n\n\nFigure 4.7: An overview of MOSAIKS and a live demonstration of generating novel predictions using the system. Video recorded by CIGAR Generalized Planetary Remote Sensing - 2020 Convention session. Presented by Esther Rolf and Tamma Carleton.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#whats-next",
    "href": "part-00-intro/04-intro-demo.html#whats-next",
    "title": "4  Try MOSAIKS",
    "section": "\n4.5 What’s next?",
    "text": "4.5 What’s next?\nAfter establishing MOSAIKS’s capabilities in the US context, the MOSAIKS development team have successfully demonstrated the system in many additional settings. This includes on the global scale, or in settings with few or low quality data. In the coming chapters, we will explore some of these applications, showing how MOSAIKS can help address data gaps in regions where traditional data collection is challenging or costly.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section we will take a closer look at the label data that can be used with MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Try MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html",
    "href": "part-01-labels/00-labels.html",
    "title": "Label data",
    "section": "",
    "text": "Overview\nThis section explores the ground truth data (labels) that can be used to train a predictive model with MOSAIKS. While the system is designed to be flexible with respect to the types of outcomes it can predict, understanding what makes good label data and how to prepare it properly is crucial for success.\nLabel data represents the “truth” that MOSAIKS attempts to predict - whether that’s crop yields, population density, economic indicators, or any other variable that might be visible (directly or indirectly) in satellite imagery. The quality and characteristics of this label data significantly influence model performance.",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#what-makes-good-label-data",
    "href": "part-01-labels/00-labels.html#what-makes-good-label-data",
    "title": "Label data",
    "section": "What makes good label data?",
    "text": "What makes good label data?\nFor optimal performance with MOSAIKS, label data should have several key characteristics:\n\nAccurate geographic location information\nAppropriate spatial resolution (typically ≥1km²)\nReasonable temporal alignment with imagery features\nSufficient sample size (generally ≥300 observations)\nObservable connection to surface features",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#section-outline",
    "href": "part-01-labels/00-labels.html#section-outline",
    "title": "Label data",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key considerations for working with label data in MOSAIKS:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n5  What labels work?\nExample applications, performance analysis, validation\n\n\n6  Survey data\nSurvey integration, sampling design, geographic referencing\n\n\n7  Preparing labels\nData cleaning, spatial joining, quality control\n\n\n8  Label data demo\nHands-on example, practical workflow, troubleshooting\n\n\n\n\n\nTable 1: Outline of the label data section\n\n\nThese chapters provide both practical guidance for preparing your own label data and deeper understanding of what types of outcomes MOSAIKS can effectively predict.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore over 100 different outcomes that have been tested with MOSAIKS, examining what works well and what doesn’t.",
    "crumbs": [
      "Label data"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html",
    "href": "part-01-labels/01-labels-100-maps.html",
    "title": "5  What labels work?",
    "section": "",
    "text": "5.1 Overview\nMOSAIKS is a designed to be useful for predicting anything that may be visible in satellite imagery. Some things are easier to predict than others, but the system is designed to be flexible. For instance, some variables can be seen directly in the imagery itself, such as forest cover, which clearly emits a visible green signal. Other outcomes can only be predicted through indirect relationships between imagery data and labels. For example, income and housing price are not themselves directly visible in raw imagery, but their values can be reliably estimated from objects (e.g., cars), textures (e.g., roofing material), and colors (e.g., grey road infrastructure) contained within the imagery.\nIn this chapter we will discuss a new working paper (Proctor et al., in prep.) that explores the question of what can and cannot be reliably predicted from satellite imagery using MOSAIKS. This paper investigates over 100 different outcomes, reporting predictive performance for each using MOSAIKS. We will discuss the results of a selection of these outcomes in detail, and provide a brief overview of the rest. Of course, this list of outcomes is not exhaustive and reported performance can differ substantially across contexts and in particular with varying quality of ground truth data. We show these results as an initial investigation of what might be possible with SIML using MOSAIKS, but encourage users to conduct their own experiments as resulst are likely to differ in new settings. The pre-print of Proctor et al. will be posted here when publicly available.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#overview",
    "href": "part-01-labels/01-labels-100-maps.html#overview",
    "title": "5  What labels work?",
    "section": "",
    "text": "Figure 5.1: MOSAIKS versatility makes it the perfect tool for a wide range of applications.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "href": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "title": "5  What labels work?",
    "section": "\n5.2 One hundred maps",
    "text": "5.2 One hundred maps\n\n5.2.1 Original publication\nIn Rolf et al. 2021, the authors tested MOSAIKS on 7 outcomes: forest cover, income, housing price, population density, nighttime luminosity, and elevation. The study area is focused in the continental United States. This is an excellent starting point for understanding the capabilities of MOSAIKS as the data quality is high for a diverse set of outcomes. While the results showed significant promise and demonstrated the potential of MOSAIKS, the true test is in the application to new outcomes and new geographies.\n\n5.2.2 Going global\nTo test the global applicability of MOSAIKS, across a diverse set of outcomes, there were 2 primary things that needed to happen:\n1. The creation of a global set of features\n\n\n\n\n\nFigure 5.2: Planet Labs visual basemap imagery from quarter 3 of 2019 (left) and 4 of 4,000 MOSAIKS features downloaded from the API (right).\n\n\n2. Collecting and curating a large set of outcomes with diversity in spatial structures and categories\n\n\n\n\n\n\n\n\n\nCategory\nNumber of Labels\nExample Label\n\n\n\nAgricultural Assets\n5\nAgricultural land ownership\n\n\nAgriculture\n16\nMaize yield\n\n\nBuilt Infrastructure\n9\nBuildings\n\n\nDemographics\n5\nMedian age\n\n\nEducation\n10\nExpected years of schooling\n\n\nHealth\n15\nMalaria in children\n\n\nHousehold Assets\n21\nMobile phones\n\n\nIncome\n9\nHuman development index\n\n\nNatural Systems\n8\nTree cover\n\n\nOccupation\n17\nUnemployment\n\n\n\n\n\nTable 5.1: The authors selected 115 variables across 10 categories and set to work testing each in the MOSAIKS system.\n\n\nWith this data in hand, they were able to devise a few simple questions to test:\n\nWhich variables can be effectively measured?\nWhat are the most compelling applications?\nWhat are the modes of failure?",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#results",
    "href": "part-01-labels/01-labels-100-maps.html#results",
    "title": "5  What labels work?",
    "section": "\n5.3 Results",
    "text": "5.3 Results\n\n5.3.1 Overall performance\nThe results of the 100 maps experiment are shown in the scatter plot below (Figure 5.3). Each point in each scatter sub-plot represents a location in the study for a given label. The x-axis is the observed value of the label, and the y-axis is the MOSAIKS-predicted value. The diagonal 45° line in each sub-plot represents perfect prediction. The coefficient of determination (R²) is used here as the primary measure of accuracy.\nA few broad insights stand out:\n\nSubstantial variation: Even within the same category, we see varying degrees of predictive power. For instance, in the “Agriculture” category, some labels (such as high-level yield averages) are predicted quite accurately, while others (like certain niche crops or management practices) remain more elusive.\nCategory differences: Some categories have consistently higher R² scores. For instance, “Natural Systems” (e.g., tree cover) often score better because the patterns are more directly visible from above—think of large, contiguous forest areas contrasted with open fields or urban centers. On the other hand, “Occupation” or “Demographics” include variables (like unemployment rates) that are largely socio-economic in nature, requiring more indirect and subtle cues.\nFailure cases: A few outcomes show near-random performance, suggesting that the satellite imagery features alone are insufficient to capture their spatial patterns, or that the signals are drowned out by noise (see Failures below).\n\n\n\n\n\n\nFigure 5.3: The results of the 100 maps experiment with the x-axis shows the observed values of the outcome, while the y-axis shows the predicted values. Each point in each scatter is a location from the study. The diagonal line (45°) represents perfect prediction. Performance is measured with the coefficient of determination (R²).\n\n\nIn the box plot (Figure 5.4) we see the distribution of R² values for each category across all 115 labels. This confirms the wide range of performance. Categories such as “Agriculture,” “Income,” and “Natural Systems” tend to have higher median R² values; categories such as “Health” and “Occupation” show more varied or lower overall performance.\n\n\n\n\n\nFigure 5.4: The results of the 100 maps experiment.\n\n\nThis heterogeneity underscores an important takeaway: MOSAIKS is not a one-size-fits-all solution. Some phenomena lend themselves to easier detection via satellite data than others. Still, the ability to simultaneously handle over 100 different outcomes from a single feature set is itself a testament to MOSAIKS’ flexibility and global applicability.\n\n5.3.2 Successes\n\n5.3.2.1 Maize yield\n\n5.3.2.1.1 Maize data\nThe maize yield data comes from crop yield data collected at the second administrative level (e.g., counties in the US) from the United States, China, Brazil and the European Union. Yields are calculated as harvested grain divided by the planted area, though in some cases harvested area is used instead of planted area. The data covers years from 1983-2009, with the most recent available year used for each location.\n\n5.3.2.1.2 Maize yield results\nA standout example of a high-performing label is Maize yield (Figure 5.5). This outcome is naturally suited to detection by satellite imagery:\n\n\nDirect visual signal: Agricultural fields have characteristic features, including crop texture, canopy cover, and phenological (growth stage) patterns, all of which can be captured in the spectral and spatial signals from satellite images.\n\n\n\nSpatial contiguity: Large, contiguous fields of maize reduce noise and enable easier extraction of relevant features.\n\nIn the left-hand scatter plot of Figure 5.5, the predicted yield values match well with the observed values, often clustering along the 45° line. On the right, we see that visually identifiable patterns in maize-growing regions are clearly reflected in the predicted maps. This strong alignment highlights how MOSAIKS can quickly yield robust predictions for outcomes that are clearly manifested in the satellite imagery.\n\n\n\n\n\nFigure 5.5: Performance of MOSAIKS on Maize yield, showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\n5.3.2.1.3 Why it works\nCrop yields are a classic use case for remote sensing because farmland is often large, geographically dispersed, and subject to rapid changes from weather and management practices—conditions that satellite imagery can routinely monitor at scale. By measuring vegetation indices (e.g., NDVI, EVI), researchers gain insight into plant health, canopy density, and photosynthetic activity, all of which correlate strongly with agricultural productivity. This non-invasive, timely, and spatially comprehensive approach makes it invaluable for crop forecasting, detecting stress, and guiding resource allocation. Consequently, remote sensing has become a cornerstone in modern yield estimation methods for staple crops around the world. MOSAIKS is a natural extension of this trend, leveraging the latest in machine learning to extract actionable insights from satellite imagery.\n\n\n\n\n\nFigure 5.6: Agricultural fields in the United States Midwest region. This examples shows the clear delineation of fields with varying color intensities, making for easily detectable features in the satellite imagery. Source: NASA\n\n\n\n5.3.2.2 International wealth index (IWI)\n\n5.3.2.2.1 IWI data\nThe International Wealth Index data comes from the Demographic and Health Surveys (DHS) program. The index is expressed as a value between 0 and 100, with higher values indicating greater wealth. It is computed from household data on ownership of consumer durables, housing characteristics, and access to basic services like water and electricity. The data is processed and provided by the Global Data Lab with permission from DHS, with values averaged across households within each survey cluster. Survey clusters are displaced by up to 2km for urban areas and up to 5km for rural areas to protect privacy, while remaining within the same administrative boundaries.\n\n5.3.2.2.2 IWI results\nAnother notable success is the International Wealth Index (IWI; Figure 5.7). This composite measure of household wealth is derived from a variety of indicators, such as housing quality, access to services, and ownership of durable goods. While wealth itself is not directly visible from space, the underlying factors that contribute to it often are. For example, wealthier areas tend to have more developed infrastructure, larger homes, and more vehicles—all of which leave distinct signatures in satellite imagery.\n\n\n\n\n\nFigure 5.7: Performance of MOSAIKS on the International Wealth Index (IWI), showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\n5.3.2.2.3 Why it works\nDespite being a composite measure of socioeconomic status, the IWI’s underlying indicators—housing conditions, access to utilities, and asset ownership—often manifest in the built environment as features that satellites capture well. For instance, wealthier neighborhoods typically exhibit a higher density of substantial buildings, paved roads, formal layouts, and visible amenities (e.g., swimming pools, parked vehicles). These cues translate into distinctive patterns in the spectral and spatial data extracted by MOSAIKS’ features. Furthermore, infrastructure development and housing materials (like metal roofs versus thatch) can produce detectable differences in reflectance, making it easier for the algorithm to discern socioeconomic gradients.\n\n\n\n\n\n\nThis highlights one of MOSAIKS’ core advantages: even when the target variable isn’t directly “visible,” the system can tease out its proxies from wide-ranging visual cues, leading to robust predictions of wealth indices around the globe.\n\n\n\n\n5.3.3 Failures\n\n5.3.3.1 Pipeline infrastructure\n\n5.3.3.1.1 Pipeline data\nThe pipeline data comes from the Energy Information Association (EIA) and includes interstate trunk lines and selected intrastate lines across three types: crude oil pipelines, hydrocarbon gas liquids (HGL) pipelines, and petroleum product pipelines. The data represents pipeline infrastructure as of January 2020 and covers the lower 48 United States. The variable measures the length in kilometers of each pipeline type within each grid cell.\n\n5.3.3.1.2 Pipeline results\nCertain labels show extremely low R² values, effectively indicating no predictive power under this approach. One notable example is the presence of underground pipelines Figure 5.8.\n\n\n\n\n\nFigure 5.8: Where it fails\n\n\n\n5.3.3.1.3 Why it fails\nUnlike forests or agricultural fields, pipeline infrastructure is typically hidden from direct visual inspection—often located entirely underground or obscured in ways that do not provide surface indicators distinguishable in imagery (Figure 5.9).\n\nLack of visible features: There is no spectral or structural cue (e.g., coloration, texture, shape) that reliably indicates the presence of a pipeline.\nIndirect clues Are unreliable: One might speculate that pipelines could follow roads or distinct corridors, but these correlations vary widely by region and do not consistently appear in the imagery.\nSignal-to-noise ratio: In many areas, the pipeline corridor may appear visually indistinguishable from farmland or other vegetation, leaving little to no unique satellite signature.\n\nAs a result, MOSAIKS has little chance to identify and learn features predictive of such hidden infrastructure. This stands in stark contrast to more visually prominent targets like maize fields or tree cover.\n\n\n\n\n\nFigure 5.9: Why it fails - buried\n\n\n\n5.3.3.2 Bee diversity\n\n5.3.3.2.1 Bee data\nThe bee diversity data comes from the US Geological Survey Eastern Ecological Science Center Native Bee Laboratory, which provides species occurrence records for native and non-native bees, wasps, and other insects collected through various trapping methods. Each record includes taxonomic identification and geographic coordinates. Using point data from 2009-2019, bee diversity is computed as a count of unique species documented within each 0.01° × 0.01° grid cell in North America (including U.S. territories and Minor Outlying islands). For cells with multiple sampling events, species are counted only once. Importantly, the database only includes records of species presence - absences are not recorded - which can lead to sampling bias in the diversity measures.\n\n5.3.3.2.2 Bee results\nAnother notable failure case is bee diversity. While bees play a crucial role in ecosystems and agriculture, their presence and diversity cannot be directly observed from satellite imagery. Several factors contribute to this limitation:\n\n\nScale mismatch: Bees operate at a much finer spatial scale than the resolution of typical satellite imagery\n\nIndirect relationships: While bees rely on vegetation, the link between plant cover visible from space and bee populations is complex and varies by context\n\nTemporal dynamics: Bee populations fluctuate seasonally and can change rapidly, while imagery captures only static snapshots\n\nHidden factors: Critical habitat features like nesting sites are often concealed under canopy or in small spaces invisible from above\n\nThis case illustrates an important principle: MOSAIKS works best when predicting outcomes that have consistent, visible relationships with surface features captured in satellite imagery. When those relationships become too indirect or complex, performance typically suffers.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#types-of-variables",
    "href": "part-01-labels/01-labels-100-maps.html#types-of-variables",
    "title": "5  What labels work?",
    "section": "\n5.4 Types of variables",
    "text": "5.4 Types of variables\nMOSAIKS can work with both continuous and categorical outcome variables, though the approach and evaluation metrics differ between these types.\n\n\n\n\n\n\nThis section provides a brief overview of the two types of variables and some of the metrics used to evaluate them. This will be covered in greater detail in Chapter 16.\n\n\n\n\n5.4.1 Continuous variables\nContinuous variables take numeric values along a spectrum, such as:\n\nCrop yields (e.g., tons per hectare)\nPopulation density (people per square kilometer)\nForest cover percentage (0-100%)\nIncome levels (dollars)\nBuilding height (meters)\n\nFor continuous variables, MOSAIKS typically uses regression approaches and evaluates performance using metrics like R² (coefficient of determination) or RMSE (root mean square error). The R² values reported throughout this chapter indicate the proportion of variance in the outcome that is explained by the MOSAIKS predictions.\n\n\n\n\n\nFigure 5.10: Continuous variable example showing the breeding bird species diversity over the continental United States\n\n\n\n5.4.2 Categorical variables\nCategorical variables group observations into distinct classes or categories, such as:\n\nLand use types (urban/agriculture/forest)\nBuilding types (residential/commercial/industrial)\nCrop types (maize/wheat/rice)\nPresence/absence of features (roads, buildings)\nDevelopment categories (low/medium/high)\n\nFor categorical variables, MOSAIKS can be used in two ways:\n\nBinary classification: For variables with two categories (e.g., presence/absence), MOSAIKS can output probability predictions between 0 and 1. Performance is typically evaluated using metrics like accuracy, precision, recall, or area under the receiver operating characteristic curve (AUC-ROC).\n\nMulti-class classification: For variables with multiple categories, MOSAIKS can either:\n\nUse a one-vs-all approach, treating each category as a separate binary classification problem\nOutput probabilities for each possible category that sum to 1\nConvert categories to numeric values if they have a natural ordering\n\n\n\n\n\n\n\n\nFigure 5.11: Classifier example showing Ecoregions of the world.\n\n\n\n5.4.3 Choosing appropriate metrics\nThe choice of evaluation metric should match the type of variable:\n\n\nVariable Type\nCommon Metrics\nInterpretation\n\n\n\nContinuous\nR², RMSE, MAE\nR² ranges 0-1, higher is better\n\n\nBinary\nAccuracy, AUC-ROC\nRange 0-1, higher is better\n\n\nMulti-class\nAccuracy, F1-score\nRange 0-1, higher is better",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#summary",
    "href": "part-01-labels/01-labels-100-maps.html#summary",
    "title": "5  What labels work?",
    "section": "\n5.5 Summary",
    "text": "5.5 Summary\nThis exploration of over 100 different outcomes reveals several key insights about MOSAIKS:\n\nPerformance varies significantly: Predictive power ranges from very strong (R² &gt; 0.8) to essentially random guessing, depending on the outcome\nDirect visibility matters: Outcomes that are directly observable in imagery (like forest cover) or have strong visible proxies (like wealth indices) tend to perform better\nCategory patterns: Some categories like Natural Systems and Agriculture show consistently stronger performance than others like Health or Demographics\n\nPractical implications: Understanding these patterns helps users:\n\nSet realistic expectations for new applications\nIdentify which types of outcomes are most suitable\nRecognize when alternative approaches might be needed\n\n\n\nThese results demonstrate both the power and limitations of MOSAIKS. While it excels at predicting many important outcomes globally, it is not a universal solution. Success depends largely on whether the outcome of interest has a meaningful relationship with features visible in satellite imagery.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore survey label data in more detail.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html",
    "href": "part-01-labels/02-labels-survey.html",
    "title": "6  Survey data",
    "section": "",
    "text": "6.1 Why does survey data needs its own chapter?\nNotes:\nSurvey data presents unique challenges and opportunities when working with MOSAIKS. Unlike many other data sources that may be consistently gathered through automated systems or administrative records, survey data:",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "href": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "title": "6  Survey data",
    "section": "",
    "text": "Captures detailed household and individual-level information that’s otherwise unobservable\nOften follows complex sampling designs that need special handling\nMay have inconsistent geographic referencing across different surveys\nRequires careful consideration of privacy and ethical concerns\nCan be expensive and time-consuming to collect, making validation of MOSAIKS predictions particularly valuable",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "href": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "title": "6  Survey data",
    "section": "\n6.2 Types of survey data",
    "text": "6.2 Types of survey data\nSeveral major categories of surveys are commonly used with MOSAIKS:\n\n6.2.1 Household surveys\n\nLiving Standards Measurement Study (LSMS)\nDemographic and Health Surveys (DHS)\nMultiple Indicator Cluster Surveys (MICS)\nLabor force surveys\nNational census data\n\n6.2.2 Agricultural surveys\n\nAgricultural censuses\nCrop cutting surveys\nFarm management surveys\nAgricultural household surveys\n\n6.2.3 Economic surveys\n\nEnterprise surveys\nMarket price surveys\nConsumer expenditure surveys",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "href": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "title": "6  Survey data",
    "section": "\n6.3 Accessing survey data",
    "text": "6.3 Accessing survey data\nSurvey data access varies by source and type:\n\n6.3.1 Public repositories\n\nWorld Bank Microdata Library\nIPUMS International\nDHS Program website\nFAO statistical databases\n\n6.3.2 National statistical offices\n\nCensus bureaus\nAgricultural ministries\nEconomic agencies\n\n6.3.3 Research institutions\n\nUniversities\nThink tanks\nResearch organizations",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "href": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "title": "6  Survey data",
    "section": "\n6.4 Working with survey data",
    "text": "6.4 Working with survey data\n\n6.4.1 LSMS data\nThe Living Standards Measurement Study (LSMS) requires specific considerations:\n\nComplex multi-topic household surveys\nDetailed geographic information\nPanel structure in some countries\nIntegration with agricultural data\nVarying spatial referencing methods\n\n6.4.2 DHS data\nThe Demographic and Health Surveys (DHS) present unique characteristics:\n\nStandardized across countries\nCluster-based sampling\nDisplaced GPS coordinates\nRich health and demographic indicators\nRegular update cycle",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "href": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "title": "6  Survey data",
    "section": "\n6.5 Remote sensing informed survey design",
    "text": "6.5 Remote sensing informed survey design\nMOSAIKS can enhance survey design in several ways:\n\n6.5.1 Pre-survey planning\n\nOptimize sampling frame using satellite-derived information\nIdentify areas of interest based on physical characteristics\nStratify sampling based on predicted characteristics\n\n6.5.2 During survey implementation\n\nValidate location information\nGuide field teams with up-to-date imagery\nMonitor survey progress\n\n6.5.3 Post-survey analysis\n\nValidate survey responses against satellite indicators\nFill data gaps in hard-to-reach areas\nCreate high-resolution predictions from survey samples\n\nExample of using MOSAIKS features for survey planning:\nThis integration of MOSAIKS with survey data represents a powerful approach for both enhancing traditional survey methods and extending their reach through satellite-based prediction.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll look at practical guidance for preparing label data for use with MOSAIKS, including data cleaning, aggregation, and joining to satellite features.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html",
    "href": "part-01-labels/03-labels-data-prep.html",
    "title": "7  Preparing labels",
    "section": "",
    "text": "7.1 Overview\nIn this chapter, we discuss the process of preparing labels for use with MOSAIKS features. Labels are the observed values we aim to predict with our model—such as crop yields, forest cover, or any variable of interest. They can come in many spatial formats (e.g., points, polygons, or gridded rasters), but they must include a spatial component. We use this spatial information to join the labels with MOSAIKS features, which are also spatially explicit.\nAlthough MOSAIKS offers many optional steps, two components are essential:\nBoth datasets must align in spatial resolution and contain appropriate geographic data for merging.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#overview",
    "href": "part-01-labels/03-labels-data-prep.html#overview",
    "title": "7  Preparing labels",
    "section": "",
    "text": "Ground observations (labels)\n\nSatellite features",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#mosaiks-grid",
    "href": "part-01-labels/03-labels-data-prep.html#mosaiks-grid",
    "title": "7  Preparing labels",
    "section": "\n7.2 MOSAIKS Grid",
    "text": "7.2 MOSAIKS Grid\nBefore we talk about labels, we must first understand the resolution that the MOSAIKS features are offered in. The resolution you choose will serve as the target resolution for your summarizing your labels.\n\n7.2.1 Resolution\nThe standard resolution of MOSAIKS is a global grid at 0.01° resolution. Each grid cell is approximately 1 km² at the equator. The grid is often represented as a point grid, where each point is the center of a grid cell. This means that standard MOSAIKS features come with a latitude and longitude coordinate, which is the center of the grid cell.\n\n\n\n\n\n\nMOSAIKS grid cell centroids are rounded to 0.005 degrees and are spaced by 0.01 degree (e.g., 10.005, 10.015, 10.025,…).\n\n\n\n\n\n\n\n\nFigure 7.1: Visual representation of a standardized grids at varying resolutions (δ) with the highest resolution on the left, and lower resolutions moving right. Source: Rolf et al. 2021 Figure 3 c.\n\n\n\n7.2.2 Advantages of the MOSAIKS grid\nThe MOSAIKS grid has several advantages. The primary advantage is that it helps avoid overlapping labels. Often label data come with coordinates which are unevenly spaced. If you are forced to align your labels to a grid and summarize within cells, you can avoid bleed over from one label to another. This is especially important when you are working with data that has a high degree of spatial autocorrelation.\n\n\n\n\n\n\nThe MOSAIKS API is designed to predict outcomes at scales of 1 km² or larger. Custom solutions are possible for higher-resolution applications (see Chapter 14).\n\n\n\n\n7.2.3 Disadvantages of the MOSAIKS grid\nThe MOSAIKS grid is defined in degrees, therefore the area of a given cell varies with latitude. Near the equator, each cell is approximately 1 km², while at higher latitudes the area decreases as the distance between meridians (longitude lines) converges.\n\n\n\n\n\nFigure 7.2: The area of a grid cell at different latitudes.\n\n\n\n7.2.4 Choosing your resolution\nWe know from Chapter 3 that the MOSAIKS API offers features at 0.01°, 0.1°, 1°, ADM2, ADM1, and ADM0 resolutions. In all cases, features are computed at 0.01° resolution and then aggregated to the desired resolution. If you plan on using the API to download features, you need to make sure your labels are at the same resolution as the features you plan to download.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "href": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "title": "7  Preparing labels",
    "section": "\n7.3 Ground Observations",
    "text": "7.3 Ground Observations\n\n7.3.1 Resolution\nPreparing label data for MOSAIKS depends on location, extent, time, and resolution. When observations have a resolution finer than 1 km², you must select or aggregate them to match the features you plan to use. MOSAIKS can incorporate labels from raster, point, polygon, or vector data. In Chapter 8, we will demonstrate how to prepare labels for each input type.\n\n\n\n\n\nFigure 7.3: Examples of label data formats that can be easily integrated into the MOSAIKS pipeline. Label data of any spatial format that can be aggregated to at least the scale of 1km² (or larger) can be used directly in combination with MOSAIKS imagery features for downstream prediction tasks. Examples shown here are from Rolf et al. (2021) and include: forest cover, elevation, population, and nighttime lights datasets (all raster format); income data (polygon format); road length (vector format); and housing price data (point format). Source: Rolf et al. 2021 Supplementary Figure 4.\n\n\n\n7.3.2 Administrative level data\nIn some cases, you will find labels for administrative regions (states, districts, etc.). In this case, it might not come with any geographic information other than a place name. You can usually find the relevant geographic information online. A good resource for this is the Global Administrative Areas (GADM) database, which provides the boundaries for administrative division at various levels for completely free.\n\n\n\n\n\nFigure 7.4: Global Human Development Index (HDI) data at the first sub-national level of administrative division (ADM1). Source: Smits & Permanyer 2019.\n\n\n\n7.3.3 Challenges of administrative data\nData can be messy. There are two main challenges with administrative data.\n\nName matching: The names in your dataset might not match perfectly with the names in the GADM database or in the MOSAIKS API. Finding a way to comprehensively match these can be time consuming and difficult.\nBoundary changes: Administrative boundaries are not always static. Some regions undergo frequent changes and you need to ensure your data matches the boundaries of the features you use.\n\n7.3.4 Sample Size\nIncreasing sample size (N) often yields higher model performance. MOSAIKS has demonstrated effectiveness across a wide range of sample N. The sample size depends on the spatial (and potentially temporal) resolution of your label data. For instance, if each record is aggregated at the county level, then N equals the number of counties. Incorporating multiple time periods can increase N but also requires more imagery to be featurized (see Chapter 14).\n\n\n\n\n\n\nAs a general rule, a minimum of 300 observations for model training is recommended, though every application is unique and may require more or fewer observations.\n\n\n\nThe original MOSAIKS publication (Rolf et al., 2021) evaluated models with sample sizes from 60,000 to 100,000. In most cases there were only modest performance declines with a few hundred observations (Figure 7.5). In recent crop yield experiments, high performance (R² = 0.80) was achieved with around 400 observations, provided the data were cleaned and aggregated to a district level. It is important to note that the original crop yield dataset included interview records from thousands of farmers across the study country. While this data has a large sample size, it is messy. In this case, a clean dataset with a low number of observations was preferred to a large but noisy dataset.\n\n\n\n\n\nFigure 7.5: Model performance for all seven tasks while varying the number of random convolutional features K and holding N = 64,000 (left) and while varying N and holding K = 8,192 (right). Shaded bands indicate the range of predictive skill across five folds. Lines indicate average accuracy across validation folds. Source: Rolf et al. 2021 Figure 3 b.\n\n\n\n7.3.5 Data Types\nMOSAIKS can accommodate both continuous labels (e.g., fraction of area forested) and discrete labels (e.g., presence/absence of mine). Data type informs model development, performance evaluation, and choice of metric (see Chapter 16). Continuous variables often use coefficient of determination (R²), while discrete variables often use Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n\n\n\n\n\nFigure 7.6: A The coefficient of determination (R²) is a measure of how well the model fits the data. It ranges from 0 to 1, where 1 indicates a perfect fit. The closer each point is to the 45 degree line, the higher the better the model fit and the higher the score will be. B The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity). The area under the curve (AUC) ranges from 0 to 1, where 1 indicates a perfect model. Diagonal line is equivalent to random guessing.\n\n\nThe data type may also effect how the data is cleaned and prepared. For example you may have a dataset of mining locations across a country. If you are interested in predicting the presence of mining, you may want to convert this to a binary variable where a 0 indicates no mining and a 1 indicates mining. Alternatively, you may be interested in predicting the area of mining in each location, in which case you might need to calculate the area of the mining polygons to make the variable continuous.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#joining-data",
    "href": "part-01-labels/03-labels-data-prep.html#joining-data",
    "title": "7  Preparing labels",
    "section": "\n7.4 Joining Data",
    "text": "7.4 Joining Data\n\n7.4.1 Merging Labels with MOSAIKS Features\nTo merge labels with features, you must align the geographic location of both datasets. The native resolution MOSAIKS feature files have rows for each location and columns with longitude, latitude, and features. Your label dataset must also have columns for longitude, latitude, and label values (at minimum). Alternatively, aggregated features will come with the name of the administrative region (e.g., district) and the features. You can join this with your label data by matching the district names.\n\n7.4.2 Example Data Structure\nA simple example: district-level crop yield labels might look like:\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\n\n\n\n1\nChibombo\n2019\n1.520\n\n\n2\nKabwe\n2019\n1.878\n\n\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n\n\n\n\n\nTable 7.1: Fictional crop yield data for districts in Zambia.\n\n\nMOSAIKS features at this same resolution might look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n10.6\n1.1\n…\n2.2\n\n\n\n\n\nTable 7.2: Fictional MOSAIKS feature data for the same districts.\n\n\nAfter a spatial join, you end up with a merged dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n1.520\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n1.878\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n10.6\n1.1\n…\n2.2\n\n\n\n\n\nTable 7.3: Example of joined data with both labels and features.\n\n\nIn the above example, our geographic location is the district and our label is the crop yield (mt/ha). We then have K columns containing the features and N observations.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "href": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "title": "7  Preparing labels",
    "section": "\n7.5 Data Cleaning Considerations",
    "text": "7.5 Data Cleaning Considerations\n\n7.5.1 Coordinate Reference Systems (CRS)\nThe default coordinate reference system used by MOSAIKS is World Geodetic System 84 (WGS 84). The standardized code that defines WGS 84 is “EPSG:4326”. EPSG stands for the European Petroleum Survey Group, which maintains a database of coordinate systems and projections. The WGS 84 coordinate system is the most commonly used coordinate system for GPS data.\n\n\n\n\n\nFigure 7.7: Nine small-scale map projections.\n\n\nIf your data is not in WGS 84, you will need to reproject it to this coordinate system before joining it with MOSAIKS features.\n\n7.5.2 Label Quality\nConfirm that label values are within expected ranges, deal with any outliers or missing data, and ensure units are consistent and numeric fields are properly formatted. This book does not go into a great deal of detail on cleaning messy data. This topic is covered in exhaustive detail in the book R for Data Science.\n\n7.5.3 Temporal Alignment\nIf you have time series labels, you will need to compute custom features for each time period. See Chapter 14 for more information on how to do this and ?sec-model-temporal for guidance on modeling time series data with MOSAIKS features.\n\n7.5.4 Data Formats\nMOSAIKS can work with several common spatial data formats:\n\n\n\n\n\n\n\n\n\n\n\nData Type\nCommon File Formats\nDescription\nExample\nGeographic information\n\n\n\nPoint Data\nCSV, GeoJSON, Shapefile\nCoordinate pairs\nPlaces of interest\ngeometry\n\n\nLine Data\nGeoJSON, Shapefile\nGeographic lines\nRoads, rivers\ngeometry\n\n\nPolygon\nGeoJSON, Shapefile\nGeographic areas\nBuildings, fields\ngeometry\n\n\nRaster\nGeoTIFF, NetCDF\nGridded data\nForest cover, elevation\ngrid\n\n\nAdministrative\nCSV\nAdministrative boundaries\nDistricts, states\nplace names\n\n\n\n\n\nTable 7.4: Common spatial data formats with file types, descriptions, examples, and indication of how the spatial information is stored.\n\n\n\n\n\n\n\n\nWhen working with large datasets, converting to efficient data formats such as Parquet, Feather, GeoTIFF, or Zarr can reduce memory usage and improve processing speed.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#summary",
    "href": "part-01-labels/03-labels-data-prep.html#summary",
    "title": "7  Preparing labels",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\nThe most important considerations when preparing labels for MOSAIKS are spatial resolution, sample size, data type, and data quality. Once you have prepared your labels, you can join them with MOSAIKS features to create a dataset ready for modeling.\nA demonstration of how to process data for the data types in Table 7.4 is provided in the next chapter. This notebook will cover the process of creating a MOSAIKS grid, downloading data, and creating a label dataset at the grid level (0.01 degree) and at the second level of administrative division (ADM2).\n\n7.6.1 Labels data checklist\nFor optimal use with MOSAIKS, label data should be:\n\nConsistently geolocated as point, polygon, vector, or raster data\nAggregable to ≥ 1km² resolution\nObservable in daytime satellite imagery (directly or indirectly)\nRecent or slow-changing if using current API features\nSample size N≥300 (larger samples generally perform better)\nCleaned and formatted for modeling\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll look at practical guidance for preparing label data for use with MOSAIKS including data cleaning and aggregation.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html",
    "href": "part-01-labels/04-labels-demo.html",
    "title": "8  Label data demo",
    "section": "",
    "text": "8.1 Overview\nThis demonstration will show you a few key concepts about label data in MOSAIKS.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html#demonstration-code",
    "href": "part-01-labels/04-labels-demo.html#demonstration-code",
    "title": "8  Label data demo",
    "section": "\n8.2 Demonstration code",
    "text": "8.2 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical preparation of label data for use in MOSAIKS. The notebook will guide you through the process of preparing a dataset for use in MOSAIKS, including:\n\nBuilding and use a MOSAIKS standardized grid\nPreparing geographic point labels (latitude and longitude coordinates)\nPreparing geographic line labels (e.g. shapefiles)\nPreparing geographic polygon labels (e.g. shapefiles)\nPreparing raster labels (e.g. GeoTIFFs)\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html#whats-next",
    "href": "part-01-labels/04-labels-demo.html#whats-next",
    "title": "8  Label data demo",
    "section": "\n8.3 What’s next?",
    "text": "8.3 What’s next?\nNow that we have covered the basics of label data, we will move on to the next section, where we will discuss considerations for choosing and processing satellite imagery. This section is not necessary for users who are either a) okay with using the MOSAIKS API to access features, or b) already have experience working with satellite imagery.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next part, we will take a look at considerations for choosing and processing satellite imagery.",
    "crumbs": [
      "Label data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html",
    "href": "part-02-satellite/00-satellite.html",
    "title": "Satellite imagery",
    "section": "",
    "text": "Overview\nThis section of the book aims to help navigate the complex world of satellite imagery, with a focus on applying these data within the MOSAIKS framework. While MOSAIKS API features are sufficient for many applications, some use cases require working directly with satellite imagery. Understanding how to select and process appropriate imagery becomes crucial in these situations.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#overview",
    "href": "part-02-satellite/00-satellite.html#overview",
    "title": "Satellite imagery",
    "section": "",
    "text": "The skills covered in these chapters are primarily relevant for users who need to generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can skip this section.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "href": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "title": "Satellite imagery",
    "section": "When to look beyond the MOSAIKS API",
    "text": "When to look beyond the MOSAIKS API\nThe MOSAIKS API provides pre-computed features from 2019 Planet Labs imagery. While these features enable a wide range of applications, you may need to work directly with satellite imagery when:\n\nYour analysis requires data from a different time period\nYou need higher spatial or temporal resolution\nYour application requires specific spectral bands\nYou want to validate or compare MOSAIKS features\nYou’re developing new methodologies",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "href": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "title": "Satellite imagery",
    "section": "Earth observation satellites: Past, present and future",
    "text": "Earth observation satellites: Past, present and future\nSatellite-based Earth observation has revolutionized our understanding of the planet since the launch of the first Landsat satellite in 1972. Today, hundreds of Earth observation satellites collect terabytes of imagery daily, supporting applications from weather forecasting to precision agriculture.\nThe variety of available satellite data has grown dramatically, with options including:\n\nFree public satellites (Landsat, Sentinel)\nCommercial providers (Planet Labs, Maxar)\nSpecialized sensors (radar, hyperspectral)\nSatellite constellations providing frequent coverage\nVery high resolution imagery (&lt;1m pixels)\n\nThis wealth of options creates both opportunities and challenges in selecting appropriate imagery for your specific needs.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#section-outline",
    "href": "part-02-satellite/00-satellite.html#section-outline",
    "title": "Satellite imagery",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key considerations when working with satellite imagery:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n9  Choosing imagery\nResolution types, data sources, selection criteria\n\n\n10  Processing imagery\nPre-processing steps, quality control, computing needs\n\n\n\n\n\nTable 1: Outline of the satellite imagery section\n\n\nThese chapters provide practical guidance for incorporating satellite imagery into your MOSAIKS workflow while highlighting important technical and logistical considerations.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will take a look at how to choose the right satellite imagery for your application.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html",
    "href": "part-02-satellite/01-satellite-imagery.html",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "9.1 Background\nThe number of Earth observation satellites has grown exponentially in recent years, driven by advancements in technology, reduced launch costs, and an increasing demand for environmental, economic, and societal insights.\nAlongside this growth, the quality of satellite imagery has improved significantly, with higher spatial, temporal, and spectral resolutions becoming the norm. The proliferation of satellites, coupled with enhanced imaging capabilities, has resulted in an unprecedented volume of data, offering researchers and practitioners a wealth of opportunities—but also presenting challenges in navigating the diversity of data products and selecting those most suited to specific applications.\nMOSAIKS leverages satellite imagery to generate high-quality training data for machine learning models. In this chapter, we explore the key factors to consider when choosing satellite imagery for use with MOSAIKS, including spatial and spectral resolution, temporal frequency, cloud cover, and processing levels.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#background",
    "href": "part-02-satellite/01-satellite-imagery.html#background",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "Figure 9.1: Growth in number of satellites over recent years. Source: Union of Concerned Scientists\n\n\n\n\n\n\n\n\nFigure 9.2: Growth in satellite iamge resolution over time. Source: FlyPix AI\n\n\n\n\n\n\n\n\n\nAccess the UCS Satellite Database for a comprehensive list of Earth observation satellites, including key specifications and operational details.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#overview",
    "href": "part-02-satellite/01-satellite-imagery.html#overview",
    "title": "9  Choosing imagery",
    "section": "\n9.2 Overview",
    "text": "9.2 Overview\nWhen selecting satellite imagery for use with MOSAIKS, several key factors need to be considered. The choice of imagery should be guided by your specific research needs, label characteristics, and practical constraints like cost and processing requirements. Different use cases, such as monitoring vegetation, mapping urban areas, or tracking climatic variables, may require specific imaging capabilities, making it essential to align satellite features with your intended application. Additionally, trade-offs between resolution, temporal frequency, and accessibility can significantly influence the feasibility of your analysis.\nFor example, high-resolution data from commercial satellites might offer detailed insights but could be prohibitively expensive for large-scale applications. On the other hand, publicly available datasets like Sentinel-2 provide cost-effective options, albeit with potentially coarser resolutions or limited spectral coverage. Understanding these trade-offs is essential to ensure the optimal use of resources.\n\n\n\n\n\n\n\n\nConsideration\nDescription\n\n\n\nAvailability\nThe availability and cost of imagery from public and private sources.\n\n\nTime range\nThe time period for which satellite images are available.\n\n\nTemporal resolution\nThe frequency at which satellite images are captured (e.g., daily, weekly).\n\n\nSpatial resolution\nThe spatial resolution of the satellite sensor (e.g., 10m, 30m, 250m).\n\n\nSpectral resolution\nThe specific wavelengths captured by the sensor (e.g., RGB, NIR, SWIR).\n\n\nSensor type\nThe type of sensor used (e.g., optical, radar, thermal).\n\n\nCloud cover\nCloud cover or other atmospheric interference.\n\n\nProcessing level\nThe preprocessing applied (e.g., orthorectification, normalization).\n\n\n\n\n\nTable 9.1: Based on your labels, a user needs to decide on these key factors. Each consideration will be covered in detail in the following sections.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#availability",
    "href": "part-02-satellite/01-satellite-imagery.html#availability",
    "title": "9  Choosing imagery",
    "section": "\n9.3 Availability",
    "text": "9.3 Availability\n\n9.3.1 Public Satellites\nPublic satellite programs offer free or low-cost imagery with global coverage. These programs, funded by government agencies, provide extensive data for scientific research and environmental monitoring. They are particularly valuable for long-term studies, thanks to consistent data archives spanning decades, and for projects with limited budgets. Publicly funded satellites often balance moderate resolutions with frequent revisit times, ensuring data accessibility and temporal consistency.\n\n\n\n\n\n\n\n\n\n\n\nSatellite\nSensor Type\nResolution\nRevisit Time\nSpecial Features\n\n\n\nSentinel-2\nOptical\n10-60m\n5 days\n13 spectral bands\n\n\nLandsat 8/9\nOptical\n15-100m\n16 days\n11 spectral bands\n\n\nMODIS\nOptical\n250m-1km\nDaily\n36 spectral bands\n\n\nSentinel-1\nRadar (C-band)\n5-40m\n6-12 days\nSynthetic Aperture Radar (SAR)\n\n\nNISAR\nRadar (L- and S-band)\nTBD\nLaunching 2024\nDual-band SAR\n\n\nVIIRS\nOther\n375m-750m\nDaily\nDaily global coverage\n\n\nASTER\nOther\n15-90m\n16 days\n14 spectral bands\n\n\nNICFI\nOptical\n4.77 m\nmonthly composite\nTropical forest monitoring\n\n\n\n\n\nTable 9.2: A selection of public satellite programs and their key specifications.\n\n\n\n9.3.2 Private Satellites\nPrivate satellite programs often provide higher resolution imagery, greater specialization, and more frequent revisits than public programs, but typically at a higher cost. These satellites are particularly useful for applications requiring fine detail, such as urban mapping, infrastructure monitoring, or precision agriculture. Private providers often offer custom data products, tailored delivery timelines, and advanced analytics, which can be invaluable for time-sensitive or commercially driven projects. However, the higher costs associated with private imagery may limit their use for large-scale or long-term studies without significant funding.\n\n\n\n\n\n\n\n\n\n\n\nSatellite\nSensor Type\nResolution\nRevisit Time\nSpecial Features\n\n\n\nMaxar WorldView\nOptical\n31cm (panchromatic), 1.24m (multispectral)\nVaries\nVery high resolution\n\n\nPlanet SkySat\nOptical\n50cm (panchromatic), 1m (multispectral)\nVaries\nCompact, agile satellites\n\n\nAirbus Pleiades\nOptical\n50cm (panchromatic), 2m (multispectral)\nVaries\nVery high resolution\n\n\nPlanet PlanetScope\nOptical\n3-5m\nDaily\nLarge constellation\n\n\nPlanet RapidEye\nOptical\n5m\n5.5 days\nDesigned for agriculture\n\n\nSPOT\nOptical\n1.5-6m\n26 days\nLong-standing program\n\n\nICEYE\nRadar (X-band)\n&lt;1m\nFrequent\nHigh revisit SAR\n\n\nCapella Space\nRadar (X-band)\n50cm\nFrequent\nHigh resolution SAR\n\n\nGHGSat\nSpecialized\nTBD\nVaries\nGreenhouse gas monitoring\n\n\n\n\n\nTable 9.3: A selection of private satellite programs and their key specifications.\n\n\n\n9.3.3 Cost considerations\n\n\n\n\n\n\nRemember that the most expensive or highest resolution imagery isn’t always the best choice. Balancing your needs with practical constraints—like budget, storage capacity, and processing expertise—can often yield better long-term outcomes than simply opting for the highest-specification option.\n\n\n\nSelecting satellite imagery involves not only the direct expense of acquiring data but also a range of indirect or “hidden” costs. When factoring these into your overall budget, consider how frequently you need updates, the extent of pre- or post-processing required, and whether your team has the technical skills to work with complex datasets. Some satellites and data portals also bundle analytics and cloud storage into subscription services, which can alleviate infrastructure demands at an additional cost.\nBelow is an example of how imagery costs might break down, offering a quick reference for typical price points and considerations across different data sources:\n\n\n\n\n\n\n\n\n\n\nCost Tier\nApproximate Range\nExample Missions/Programs\nKey Considerations\n\n\n\nFree Public Data\n$0\nLandsat (30m), Sentinel (10m), MODIS (250m–1km), VIIRS (375m–750m)\nGovernment-funded; widely used for large-scale or long-term projects; moderate resolution.\n\n\nMedium-Resolution\n$1–5 per km²\nSPOT (1.5–6m)\nRelatively affordable for moderate detail; suitable for regional analyses.\n\n\nHigh-Resolution\n$5–15 per km²\nPlanet (3–5m), RapidEye (5m)\nUseful for more detailed studies; frequent revisit can drive up cost if coverage is large.\n\n\nVery High Resolution\n$15–25 per km²\nWorldView (sub-meter), SkySat (0.5–1m)\nOffers fine-grained detail but at a premium; cost can escalate quickly for large areas.\n\n\nSubscription Services\nVariable\nSome commercial data providers bundle storage & analytics\nOften convenient “one-stop shop,” but pricing may be unpredictable or scale poorly.\n\n\n\n\n\nTable 9.4: Cost tiers for satellite imagery and key considerations.\n\n\nIn addition to these basic acquisition costs, you should also consider:\n\n\nData Storage and Computing: Large volumes of high-resolution imagery require greater storage space and more computational power for processing.\n\n\nExpertise and Training: Interpreting complex datasets (such as radar or hyperspectral imagery) often demands specialized skills.\n\n\nSoftware Licenses: Proprietary GIS or image processing software can add to your operational budget.\n\n\nCloud Infrastructure: Commercial cloud solutions can handle large-scale processing but may entail ongoing subscription fees.\n\nCarefully weighing these direct and indirect expenses will help you choose imagery that optimizes both scientific value and cost-effectiveness.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#time-range",
    "href": "part-02-satellite/01-satellite-imagery.html#time-range",
    "title": "9  Choosing imagery",
    "section": "\n9.4 Time range",
    "text": "9.4 Time range\nThe operational lifespan and launch history of a satellite mission are essential considerations when selecting imagery for your project. Older programs such as Landsat offer data archives spanning several decades, making them valuable for long-term change detection or historical analyses. In contrast, newer satellites like Sentinel, launched in 2014, provide data with more advanced sensor technologies but cover a shorter temporal window.\nWhen deciding on a time range, it is important to assess the historical depth your study demands, as well as the consistency of sensor technology over that period. Projects that require comparisons across many years benefit from missions with well-documented calibration procedures and stable sensor performance over time. You may also need to factor in whether future data continuity is guaranteed by planned satellite launches or extended operational budgets. For instance, Landsat’s multi-generational program ensures a relatively consistent data record, while newer constellations might offer superior capabilities but face funding uncertainties that could impact long-term availability.\n\n\n\n\n\nFigure 9.3: The Landsat satellite constellation service timeline. Source: NASA",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#temporal-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#temporal-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.5 Temporal resolution",
    "text": "9.5 Temporal resolution\nTemporal resolution, or revisit frequency, dictates how often a satellite images the same location. Higher revisit rates allow you to monitor rapidly changing phenomena, such as vegetation growth cycles or flood dynamics, and respond more quickly to evolving events. However, satellites that pass over more frequently often trade off spatial or spectral detail.\nWhen selecting temporal resolution, consider the pace at which your target variables change. Monitoring daily fluctuations in crop conditions requires denser time series than an annual assessment of deforestation. Cloud conditions in your study area can also influence the effective revisit rate: if clouds frequently obscure the land surface, you may need a satellite constellation with multiple imaging opportunities or a sensor that can “see” through clouds, like radar (SAR). Budget and data processing capacities further shape your decision, since frequent imaging can lead to increased data storage needs and processing overhead.\n\n\n\n\n\nFigure 9.4: Comparison of temporal resolution from different satellite systems. Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#spatial-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#spatial-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.6 Spatial resolution",
    "text": "9.6 Spatial resolution\nSpatial resolution of the satellite sensor determines the level of detail captured in satellite imagery. At higher resolutions, features such as individual trees, vehicles, or small buildings are discernible. Lower resolutions provide broader overviews of landscapes—suitable for regional-scale studies like climate analysis or large-area land cover mapping—but may miss fine-grained changes in smaller features.\nAlthough very high resolution imagery (below one meter) can yield rich information, it often carries higher costs and storage requirements. Meanwhile, moderate resolutions (10–30 m) balance meaningful detail with affordability and manageable data volumes. Your choice should account for the physical size of the features relevant to your labels, the scale of your study area, and the computational resources at your disposal. In many cases, public missions like Sentinel-2 or Landsat offer practical resolutions (10–30 m) that suffice for broad-scale analyses without incurring the expense associated with commercial high-resolution imagery.\n\n\n\n\n\nFigure 9.5: The spatial resolution of satellite imagery from 3 public satellites (top row) and 3 commercial satellites (bottom row). Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#spectral-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#spectral-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.7 Spectral resolution",
    "text": "9.7 Spectral resolution\nSpectral resolution refers to the sensor’s ability to measure discrete wavelengths across the electromagnetic spectrum. Different materials—such as vegetation, water, and urban infrastructure—exhibit distinct spectral signatures, so capturing multiple bands can enhance classification accuracy and thematic mapping. For instance, near-infrared (NIR) bands are extremely valuable for quantifying vegetation health, while shortwave infrared (SWIR) can help distinguish between minerals and moisture content.\nChoosing an appropriate spectral resolution involves balancing the number and width of bands with the specific thematic information you need. Sensors that capture only visible light (RGB) may suffice for broad land cover discrimination, whereas applications such as agricultural health monitoring often benefit from additional NIR or SWIR coverage. Thermal or microwave bands can also be critical in specialized domains—like thermal anomaly detection or soil moisture estimation—and thus influence the utility of certain satellite platforms for specific scientific or operational goals.\n\n\nSpectral resolution and its applications. Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "href": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "title": "9  Choosing imagery",
    "section": "\n9.8 Sensor type",
    "text": "9.8 Sensor type\nSatellite sensors can generally be characterized as passive or active instruments. Both categories have distinct advantages and limitations that influence their suitability for particular applications.\nPassive sensors measure natural radiation, typically reflected sunlight (optical) or emitted heat (thermal). Because they rely on external energy sources, data collection can be constrained by sunlight availability and atmospheric conditions. However, optical imagery from passive sensors often offers intuitive color representations, making it easier to interpret. Thermal bands can reveal heat signatures useful in land surface temperature studies.\nActive sensors emit their own energy and record the backscatter signal. Radar systems (e.g., SAR) and LiDAR instruments fall under this category. They can penetrate clouds, operate at night, and measure surface characteristics such as roughness or elevation. On the downside, the data are often more complex to process and interpret, requiring specialized expertise and software.\n\n\n\n\n\nFigure 9.6: Graphic depicting the difference between active and passive sensors. Source: Radiant Earth\n\n\nBelow is a simple comparison of these sensor types:\n\n\n\n\n\n\n\n\n\n\n\nSensor Type\nExamples\nOperating Principle\nPros\nCons\n\n\n\nPassive\nOptical, Thermal\nMeasures naturally available reflected energy (sunlight or emitted heat)\nEasier to interpret; often intuitive (e.g., RGB images)\nLimited by cloud cover and sensitive to atmospheric conditions\n\n\nActive\nRadar (SAR), LiDAR\nEmits energy and measures return signals\nCan penetrate clouds, capture data day or night\nData may require more complex processing\n\n\n\n\n\nTable 9.5: Sensor types and their key characteristics.\n\n\nUnderstanding which sensor type best aligns with your research needs is crucial. For instance, if your study area is frequently cloudy, a radar-based approach can offer more reliable coverage. Alternatively, optical sensors might be sufficient for regions with seasonal cloud-free windows or for general land-cover classification when consistent sunlight is available.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "href": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "title": "9  Choosing imagery",
    "section": "\n9.9 Cloud cover",
    "text": "9.9 Cloud cover\nCloud cover is one of the most persistent challenges in working with optical satellite imagery, as clouds and shadows can degrade the quality of the collected data. In regions with frequent cloudiness, images may require additional processing or longer acquisition windows to achieve acceptable coverage. Strategies to address this issue include applying cloud masking algorithms, merging multiple images to create a cloud-free composite, or using radar-based satellites that penetrate clouds.\nOptical sensors, such as those on Sentinel-2 or Landsat, often include quality assurance layers that flag pixels contaminated by clouds. Filtering out these pixels improves data reliability but reduces the amount of usable imagery, potentially limiting temporal coverage in cloudy regions. On the other hand, radar missions like Sentinel-1 or ICEYE remain largely unaffected by cloud conditions, offering consistent coverage at the cost of different data characteristics and additional expertise required for radar data interpretation. Weighing these trade-offs helps in choosing imagery that meets both your accuracy requirements and your practical constraints.\n\n\n\n\n\nFigure 9.7: Cloud cover obscuring the view over over North America. Source: NASA Earth Observatory",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "href": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "title": "9  Choosing imagery",
    "section": "\n9.10 Processing levels",
    "text": "9.10 Processing levels\nSatellite data are distributed at various processing levels, ranging from unprocessed instrument measurements to derived products that are nearly ready for analysis. Selecting a processing level that aligns with your project’s technical requirements can streamline workflows and ensure consistent results.\nBelow is a general outline of commonly encountered processing levels:\n\n\n\n\n\n\n\n\n\nLevel\nDescription\nTypical Use Case\n\n\n\nLevel 0\nRaw instrument data with minimal or no preprocessing. Often not orthorectified or radiometrically calibrated.\nRarely used for general analysis due to complexity.\n\n\nLevel 1\nRadiometric calibration and basic geometric corrections. Sometimes subdivided (e.g., 1A, 1B, 1C).\nSuitable if you need control over final atmospheric corrections or custom workflows.\n\n\nLevel 2\nSurface reflectance products with atmospheric corrections applied, often including cloud/shadow masks.\nMost common for analyses requiring accurate reflectance values and multi-temporal comparisons.\n\n\nLevel 3\nDerived or composite products (e.g., mosaics, NDVI layers, gap-filled data).\nIdeal for thematic applications that benefit from aggregated or pre-processed data.\n\n\n\n\n\nTable 9.6: Common processing levels for satellite imagery and their typical use cases.\n\n\nAdditional processes often applied during or after these levels include:\n\n\nOrthorectification: Removal of geometric distortions caused by sensor tilt, terrain relief, and Earth curvature. Ensures accurate spatial representation for mapping and analysis.\n\nAtmospheric correction: Adjustment for atmospheric effects (aerosols, water vapor) to standardize reflectance values across different times or sensors.\n\nRadiometric calibration: Conversion of raw sensor counts to physical units like reflectance or brightness temperature, enabling consistent comparisons among images from different sensors and acquisition dates.\n\n\n\n\n\n\nFigure 9.8: NICFI satellite imagery raw pixel values showing each band (R, G, B, NIR) for each statistic (min, max, mean, median). These data come with atmospheric correction, cloud correction, and normalized pixel values. After Products created after April 2022 also apply a sharpening filter to the images.\n\n\nKnowing which processing steps your data provider already performs helps you avoid redundant work and ensures you acquire data that meet your accuracy requirements. For instance, if you need consistent multi-temporal comparisons, opt for Level 2 or higher to minimize atmospheric variations. Conversely, if you prefer to perform your own corrections and calibrations, Level 1 might offer greater flexibility at the cost of added complexity.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "href": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "title": "9  Choosing imagery",
    "section": "\n9.11 Choosing the right combination",
    "text": "9.11 Choosing the right combination\nSelecting the right combination of spatial, spectral, temporal, and sensor characteristics can feel overwhelming, given the numerous satellites and data products available. This choice often comes down to the trade-offs you are willing to make between cost, resolution, revisit frequency, spectral coverage, and data availability.\n\n\n\n\n\n\n\n\n\n\n\nApplication\nSpatial resolution\nTemporal frequency\nKey bands\nExample sensors\n\n\n\nAgricultural\n5–30m\nWeekly–monthly\nNIR, Red, SWIR\nPlanet, Sentinel-2, Landsat 8/9\n\n\nUrban\n0.5–10m\nMonthly–annual\nRGB, NIR\nWorldView, Planet, Sentinel-2\n\n\nForest Monitoring\n10–30m\nMonthly–annual\nNIR, SWIR, Red\nLandsat, Sentinel-2\n\n\nWater Resources\n10–30m\nWeekly–monthly\nNIR, SWIR, Blue\nSentinel-2, Landsat 8/9\n\n\n\n\n\nTable 9.7: Sample satellite characteristics for different applications.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "href": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "title": "9  Choosing imagery",
    "section": "\n9.12 Making the final decision",
    "text": "9.12 Making the final decision\nConsider these questions when making your final imagery selection:\n\nWhat is the minimum spatial resolution needed?\nHow frequent do observations need to be?\nWhat spectral bands are required?\nWhat is the time period of interest?\nWhat is your budget?\nWhat processing level is needed?\nHow will you handle clouds and data gaps?\nWhat are your storage and computing resources?\n\nThe answers to these questions will guide you toward the most appropriate imagery source for your specific application.\n\n\n\n\n\nFigure 9.9: DALL-E generated image of finding the perfect satellite.\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will explore how to access and process satellite imagery for use with MOSAIKS.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html",
    "href": "part-02-satellite/02-satellite-processing.html",
    "title": "10  Processing imagery",
    "section": "",
    "text": "10.1 Overview\nAfter selecting appropriate satellite imagery (as discussed in Chapter 9), the next step is accessing and processing that imagery for use with MOSAIKS. This chapter covers key considerations and practical approaches for working with satellite data.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#overview",
    "href": "part-02-satellite/02-satellite-processing.html#overview",
    "title": "10  Processing imagery",
    "section": "",
    "text": "Source: Microsoft Planetary Computer",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#cloud-vs-local-processing",
    "href": "part-02-satellite/02-satellite-processing.html#cloud-vs-local-processing",
    "title": "10  Processing imagery",
    "section": "\n10.2 Cloud vs local processing",
    "text": "10.2 Cloud vs local processing\nThere are two main approaches to accessing and processing satellite imagery:\n\n10.2.1 Cloud-based platforms\nModern cloud platforms offer several advantages for satellite image processing:\n\nNo need to download raw imagery\nScalable computing resources\nPre-configured environments\nOften include common datasets\nPay only for what you use\n\nPopular platforms include:\n\nGoogle Earth Engine\nMicrosoft Planetary Computer\nAmazon Web Services\nPlanet Platform\nEuro Data Cube\n\n10.2.2 Local processing\nTraditional local processing may be preferred when:\n\nInternet connectivity is limited\nData privacy is paramount\nWorking with proprietary algorithms\nComputing needs are modest\nFrequent reuse of same imagery\n\nConsider these factors when choosing:\n\nData volume\nProcessing complexity\nBudget constraints\nTime requirements\nSecurity needs",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#accessing-imagery",
    "href": "part-02-satellite/02-satellite-processing.html#accessing-imagery",
    "title": "10  Processing imagery",
    "section": "\n10.3 Accessing imagery",
    "text": "10.3 Accessing imagery\n\n10.3.1 Cloud platforms\nNASA Earthdata Cloud Cookbook\n\n10.3.1.1 Google Earth Engine\nGoogle Earth Engine provides a vast catalog of satellite imagery and geospatial datasets that can be accessed through a Python API. The GEE API is based on the Earth Engine Python API, which allows you to interact with the Earth Engine servers and run geospatial analyses in the cloud.\n\n10.3.1.2 Microsoft Planetary Computer\nMicrosoft Planetary Computer provides a multi-petabyte catalog of global environmental data that can easily be accessed through APIs. The MPC API is based on the STAC (SpatioTemporal Asset Catalog) specification, which is an emerging open standard for geospatial data that aims to increase the interoperability of geospatial data, particularly satellite imagery.\nFor more information on using the Microsoft Planetary Computer API, see Reading Data from the STAC API.\n\n\n\n\n\n\nFor practical guidance on accessing the Microsoft Planetary Computer Data Catalog through their API,see the Master of Environmental Data Science (MEDS) course EDS 220 section on the STAC specification\n\n\n\n\n10.3.2 Local downloads\n\n10.3.2.1 Sentinelsat python API\n\n10.3.2.2 Landsat\nimport landsatxplore.api\n\n# Initialize API\napi = landsatxplore.api.API('user', 'password')\n\n# Search scenes\nscenes = api.search(\n    dataset='landsat_ot_c2_l2',\n    bbox=bbox,\n    start_date='2019-01-01',\n    end_date='2019-12-31'\n)",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "href": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "title": "10  Processing imagery",
    "section": "\n10.4 Processing steps",
    "text": "10.4 Processing steps\n\n10.4.1 1. Quality assessment\n\nCloud detection\nShadow masking\nBad pixel identification\nSensor artifacts removal\n\n10.4.2 2. Atmospheric correction\n\nConvert to surface reflectance\nAccount for atmospheric effects\nStandardize across images\n\n10.4.3 3. Geometric correction\n\nOrthorectification\nCo-registration\nProjection alignment\n\n10.4.4 4. Mosaicking\n\nImage stitching\nFeathering/blending\nGap filling\nColor balancing\n\n10.4.5 5. Temporal compositing\n\nBest-pixel selection\nWeighted averaging\nGap filling\nSmoothing",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-workflows",
    "href": "part-02-satellite/02-satellite-processing.html#processing-workflows",
    "title": "10  Processing imagery",
    "section": "\n10.5 Processing workflows",
    "text": "10.5 Processing workflows\n\n10.5.1 Example cloud-based workflow\ndef process_sentinel2(aoi, start_date, end_date):\n    \"\"\"\n    Process Sentinel-2 imagery on Google Earth Engine\n    \n    Parameters:\n    -----------\n    aoi : ee.Geometry\n        Area of interest\n    start_date : str\n        Start date (YYYY-MM-DD)\n    end_date : str\n        End date (YYYY-MM-DD)\n        \n    Returns:\n    --------\n    ee.Image\n        Processed composite image\n    \"\"\"\n    s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date)\n    \n    # Cloud masking\n    def mask_clouds(img):\n        clouds = img.select('QA60').bitwiseAnd(1 &lt;&lt; 10)\n        return img.updateMask(clouds.Not())\n    \n    # Apply cloud mask\n    s2_masked = s2.map(mask_clouds)\n    \n    # Create median composite\n    composite = s2_masked.median()\n    \n    return composite\n\n10.5.2 Example local workflow\ndef process_local_images(image_dir):\n    \"\"\"\n    Process downloaded satellite imagery\n    \n    Parameters:\n    -----------\n    image_dir : str\n        Directory containing images\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Processed image array\n    \"\"\"\n    import rasterio\n    from rasterio.merge import merge\n    \n    # List images\n    images = []\n    for file in os.listdir(image_dir):\n        if file.endswith('.tif'):\n            with rasterio.open(os.path.join(image_dir, file)) as src:\n                images.append(src)\n    \n    # Merge images\n    mosaic, transform = merge(images)\n    \n    return mosaic",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#best-practices",
    "href": "part-02-satellite/02-satellite-processing.html#best-practices",
    "title": "10  Processing imagery",
    "section": "\n10.6 Best practices",
    "text": "10.6 Best practices\n\n\nDocument everything\n\nProcessing steps\nParameter choices\nQuality control decisions\nSoftware versions\n\n\n\nValidate outputs\n\nVisual inspection\nStatistical checks\nGround truth comparison\nCross-platform verification\n\n\n\nOptimize resources\n\nBatch processing\nParallel computing\nMemory management\nStorage efficiency\n\n\n\nVersion control\n\nTrack code changes\nArchive key datasets\nDocument dependencies\nMaintain reproducibility",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "href": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "title": "10  Processing imagery",
    "section": "\n10.7 Common challenges",
    "text": "10.7 Common challenges\n\n10.7.1 Storage requirements\n\nRaw imagery can be massive\nMultiple processing steps multiply storage needs\nIntermediate products management\nBackup considerations\n\n10.7.2 Computing resources\n\nProcessing can be CPU/GPU intensive\nMemory limitations\nI/O bottlenecks\nNetwork bandwidth\n\n10.7.3 Quality issues\n\nCloud contamination\nAtmospheric effects\nSensor artifacts\nGeometric distortions\n\n10.7.4 Time management\n\nProcessing can be slow\nDownload times\nQuality checking\nIteration cycles",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "href": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "title": "10  Processing imagery",
    "section": "\n10.8 Future considerations",
    "text": "10.8 Future considerations\nAs you develop your processing pipeline, consider:\n\nScalability needs\nAutomation opportunities\nQuality control requirements\nResource constraints\nTime limitations\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will explore how to extract MOSAIKS features from processed imagery.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html",
    "href": "part-02-satellite/03-satellite-demo.html",
    "title": "11  Imagery demo",
    "section": "",
    "text": "11.1 Overview\nThis demonstration will show you a few key concepts about satellite data and how it can be prepared for featurization (featurization covered in Chapter 14).",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#demonstration-data",
    "href": "part-02-satellite/03-satellite-demo.html#demonstration-data",
    "title": "11  Imagery demo",
    "section": "\n11.2 Demonstration data",
    "text": "11.2 Demonstration data\nFor this demonstration, we will use satellite imagery from the NICFI Basemaps. The NICFI Basemaps are a collection of high-resolution satellite images that are freely available for research purposes. These data come with atmospheric correction, cloud correction, and normalized pixel values. After Products created after April 2022 also apply a sharpening filter to the images.\nMore details about NICFI Basemap processing can be found here.\nTo learn how to access NICFI Basemaps, follow the sign up instructions here.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#demonstration-code",
    "href": "part-02-satellite/03-satellite-demo.html#demonstration-code",
    "title": "11  Imagery demo",
    "section": "\n11.3 Demonstration code",
    "text": "11.3 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical preparation of satellite data for use in MOSAIKS. The notebook will guide you through the process of preparing imagery, including:\n\nLoading satellite imagery\nInspecting image properties\nImage normalization\nImage visualization\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#whats-next",
    "href": "part-02-satellite/03-satellite-demo.html#whats-next",
    "title": "11  Imagery demo",
    "section": "\n11.4 What’s next?",
    "text": "11.4 What’s next?\nNow that we have covered the basics of working with satellite data, we will move on to the next section, where we will discuss image featurization.\n\n\n\n\n\n\nLooking forward\n\n\n\nThis is the end of the satellite data section. Next, we will move on to the featurization of satellite data.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html",
    "href": "part-03-features/00-features.html",
    "title": "Satellite features",
    "section": "",
    "text": "Overview\nThis section explores MOSAIKS features - the compressed representations of satellite imagery that enable efficient prediction across diverse tasks. While many users can rely on pre-computed API features, understanding how these features work and how to generate them provides valuable context and enables customization when needed.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#overview",
    "href": "part-03-features/00-features.html#overview",
    "title": "Satellite features",
    "section": "",
    "text": "The technical details covered in these chapters are primarily relevant for users who need to understand or generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can focus on 13  API features.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "href": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "title": "Satellite features",
    "section": "When to look beyond the API features",
    "text": "When to look beyond the API features\nThe MOSAIKS API provides pre-computed features derived from 2019 Planet Labs imagery. While these features enable many applications, you may need to work directly with feature generation when:\n\nYour analysis requires data from a different time period\nYou need features at a different spatial resolution\nYou want to experiment with different feature parameters\nYou’re developing new methodological approaches\nYou need to validate or compare feature types",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#feature-types-and-computation",
    "href": "part-03-features/00-features.html#feature-types-and-computation",
    "title": "Satellite features",
    "section": "Feature types and computation",
    "text": "Feature types and computation\nMOSAIKS features transform raw satellite imagery into a concise tabular format that captures essential patterns while dramatically reducing data volume. The system currently supports:\n\nRandom convolutional features (RCFs)\nGaussian random features\nEmpirical patch features\nHybrid approaches\n\nThese different feature types offer various tradeoffs in terms of computation time, storage requirements, and predictive performance across tasks.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#section-outline",
    "href": "part-03-features/00-features.html#section-outline",
    "title": "Satellite features",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key aspects of MOSAIKS features:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n12  Understanding features\nRandom convolutional features, implementation details\n\n\n13  API features\nPre-computed features, specifications, usage\n\n\n14  Computing features\nProcessing requirements, workflows, storage\n\n\n\n\n\nTable 1: Outline of the features section\n\n\nThese chapters provide both practical guidance for working with MOSAIKS features and deeper technical understanding of how the feature extraction process works.\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will attempt to provide some context and intuition for the random convolutional features (RCFs) that are at the core of MOSAIKS.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html",
    "href": "part-03-features/01-features-rcf.html",
    "title": "12  Understanding features",
    "section": "",
    "text": "12.1 Kitchen sinks?\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. Whenever we present MOSAIKS, we get the question, “Where does ‘kitchen sinks’ come from?” The answer stems from the phrase “everything but the kitchen sink,” which means “almost everything imaginable.” In the context of MOSAIKS, everything but the kitchen sink emphasizes that we take a huge amount of information out of the raw imagery—though, of course, we’re not capturing every last pixel or every possible relationship. It’s as if we’re taking the most useful “ingredients” (i.e., features) from the imagery and leaving the rest behind.\nThis idea of leaving behind the raw imagery is key to MOSAIKS’s power. It means that most users never have to handle the massive amounts of satellite imagery directly. Instead, the MOSAIKS team takes on that burden—extracting a large set of random convolutional features—and then discards the imagery. End users do not need to see the raw imagery or interpret what each individual feature means; they can just use the numerical representation. As a result, users can simply download these features and apply them to their own predictive tasks.\nIn this section, however, we lift the hood to see what’s going on. We focus on how we extract the features from satellite imagery and attempt to provide some intuition for what these features represent.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "href": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "title": "12  Understanding features",
    "section": "",
    "text": "Figure 12.1: Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by Grace Lewin.\n\n\n\n\n\n\n\n\n\n\nIn this book, the terms random convolutional features, RCFs, features, satellite features, and MOSAIKS features are used interchangeably.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#turning-images-into-features",
    "href": "part-03-features/01-features-rcf.html#turning-images-into-features",
    "title": "12  Understanding features",
    "section": "\n12.2 Turning images into features",
    "text": "12.2 Turning images into features\n\n\n\n\n\n\nFigure 12.2: A collection of satellite images. Source: Microsoft Planetary Computer\n\n\n\n\n12.2.1 Overview\nThe MOSAIKS featurization process produces a fixed-length feature representation for each patch of satellite imagery. Practically, this means we end up with a numerical vector for each image. Because MOSAIKS uses satellite image, you can substitute the word image for location which means we end up with a numerical representation for each location.\n\n\n\n\n\nFigure 12.3: A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. K patches are randomly sampled from across the N images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image. Source: Rolf et al. 2021 Figure 1 C\n\n\nThe featurization process has three main steps: convolution, activation, and average pooling.\nIn the coming sub-sections, we will illustrate these steps. In our example, our input imagery will have the dimensions \\(3 x 256 x 256\\), where \\(3\\) is the number of color channels (red, green, and blue) and \\(256 x 256\\) is the image size (width x height) in pixels.\n\n12.2.2 Understanding convolutions\nIn simple terms, applying a convolution to an image can highlight certain patterns, like edges, textures, or colors. Convolutional filters “scan” across the image, computing element-wise products with small patches of the image. Many computer vision models stack multiple convolutional and other layers into a convolutional neural network (CNN). Deep neural networks can have many layers, which is why the approach is often called deep learning.\n\n\n\n\n\nFigure 12.4: A gif showing a convolutional layer with no padding and no strides. Source: A guide to convolution arithmetic for deep learning.\n\n\nMOSAIKS, on the other hand, uses a single convolutional layer. These layer weights are randomly initialized and remain fixed; they aren’t updated during training. This is why the features we get are essentially “random samples” of the spatial and spectral information in the images.\n\n\n\n\n\n\nThere are many great resources for visualizing convolutions, including Convolutional Neural Networks (CNNs) explained.\n\n\n\n\n12.2.2.1 Initializing filters\nFor the convolution step, we need to initialize a set of filters. Each filter is a 3-dimensional tensor with the same number of color channels as the images and a width and height size specified by a kernel size parameter. We can initialize these filters in two ways:\n\nGaussian initialization: We draw the filter weights from a normal distribution.\nEmpirical patches: We randomly sample patches from the image dataset and use these as filters.\n\nEither way, each filter has the same number of color channels as the original images and a specified kernel size (e.g., 3×3). So if our kernel size is 3, each filter might have shape (3, 3, 3). The number of filters is a hyperparameter that you can set when you define your model.\nTo perform the convolution, we compute the dot product of each filter with every portion of the image as the filter slides across it. The result is a new image (a convolution output map) that highlights regions similar to that filter’s pattern.\n\n\n\n\n\n\nWe use pytorch’s torch.nn.functional.conv2d function to perform this operation.\n\n\n\n\n12.2.2.2 Whitening filters\nIf you decide to use empirical patches in the convolution step, it is best practice to whiten the patches. We use a process called Zero-phase Component Analysis (ZCA) whitening, which preserves the spatial structure of the data while removing correlations among pixels.\nThe key steps are:\n\nSubtract the mean of each feature from the data.\nCompute the covariance matrix \\(\\Sigma\\) and perform its eigendecomposition.\nUse the eigenvectors \\(U\\) and eigenvalues \\(D\\), along with a small constant \\(\\varepsilon\\), to form the ZCA transform \\(W\\).\nMultiply the original data by \\(W\\) to obtain the whitened data \\(\\widetilde{X}\\).\n\nMathematically, assuming \\(X\\) is your zero-mean data matrix of shape \\((N \\times d)\\):\n\\[\n\\Sigma = \\frac{1}{N} X^\\top X\n\\quad\\quad\\text{and}\\quad\\quad\n\\Sigma = U \\, D \\, U^\\top,\n\\]\n\\[\nW = U \\Bigl(D + \\varepsilon I\\Bigr)^{-\\tfrac{1}{2}} U^\\top,\n\\]\n\\[\n\\widetilde{X} = X \\, W.\n\\]\n\n\n\n\n\n\nSee the ZCA whitening implementation in torchgeo for more details.\n\n\n\n\n12.2.3 Activation\nNext, we apply a non-linear activation function—ReLU (Rectified Linear Unit)—to the convolution output map. ReLU outputs max(0, x), meaning it zeroes out negative values. This step helps capture non-linearities in the data.\nThe 2-for-1 activation special\nWhen we define our model, one of the parameters that needs to be specified is the number of features. This number needs to be a positive value because we generate 2 features for each filter.\nWe do this by applying the activation function twice:\n\n\nFeature A – ReLU activation on the convolution output.\n\n\nFeature B – ReLU activation on the inverted convolution output (i.e., multiplication by –1, or “negative” of the same filter output).\n\nSo if you specify 200 features, you are actually drawing 100 weights and getting 200 features back. This improves computational efficiency and ensures that each filter has the potential to capture both positive and negative cues.\n\n12.2.4 Average pooling\nWe then apply an adaptive average pooling layer to each activation map, collapsing the 2D spatial grid into a single number per filter. Essentially, this “global average” is a single summary value for how strongly that filter responded to the image.\n\n\n\n\n\n\nWe use pytorch’s torch.nn.functional.adaptive_avg_pool2d function to perform this operation.\n\n\n\n\n12.2.5 Putting it all together\nWe repeat the 3 steps (convolution, activation, and pooling) for all filters. If we have K filters, we end up with a K-dimensional feature vector.\n\n\n\n\n\nFigure 12.5: Input image, selected patches, whitened patches, convolution output, and activation maps",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#intuition-for-rcfs",
    "href": "part-03-features/01-features-rcf.html#intuition-for-rcfs",
    "title": "12  Understanding features",
    "section": "\n12.3 Intuition for RCFs",
    "text": "12.3 Intuition for RCFs\nBelow is a cartoon that illustrates what a random convolutional feature vector might be looking for. Suppose we randomly draw three 3×3 patches from different parts of an image: a forest patch, a road patch, and a river patch.\n\n\n\n\n\nFigure 12.6: A single image with 3 random convolutional patches, highlighting 3 different feature activations of the imagery.\n\n\nWhen we convolve the forest patch across the entire image, the resulting map “lights up” places that look like forest. Likewise, the road patch lights up roads, and the river patch lights up the river. After the ReLU and average-pooling steps, we get a set of summary values—one for each filter.\n\n\n\n\n\nFigure 12.7: Two cartoon images with the same patches used in the convolution step. We see that we have several labels for each image and that we can model each outcome with the same set of features.\n\n\nIf Image 1 has more trees than Image 2, the “forest” feature’s value is higher in Image 1. If Image 2 has more roads, its “road” feature will be higher. Because these features each capture a different (random) spatial pattern, we can then combine them for many downstream prediction tasks—whether it’s predicting tree cover, paved roads, or even something else that correlates with these visual patterns.\n\n\n\n\n\nFigure 12.8: A cartoon image projected into random feature space. If we look at just 2 dimensions, how grey on the x-axis and how green on the y-axis, we can find the vector through this feature space which represents our outcome.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "href": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "title": "12  Understanding features",
    "section": "\n12.4 Why use RCFs?",
    "text": "12.4 Why use RCFs?\n\n12.4.1 Traditional convolutional neural networks (CNNs)\nTo appreciate RCFs, let’s contrast them with traditional CNNs. In a standard CNN, filters are learned via backpropagation. The network sees many examples, calculates errors, and updates the filter weights so that over time they become good at extracting task-specific features. This makes CNNs powerful, but only for what they have been trained to do.\n\n\n\n\n\nFigure 12.9: A simplified diagram showing a typical convolutional neural network model architecture.\n\n\n\n12.4.2 Replacing minimization with randomization in learning\nMOSAIKS takes a radically simpler approach: random filters are used to sample the imagery’s information. Because they are random, they are not “tailored” to any one task. This sounds counterintuitive at first—surely you’d want to learn your filters! But the virtue of randomization is speed and broad applicability. Since no training is required to set these filters, we can produce features at planet-scale very quickly. These features can then be shared with many users, each of whom can apply them to their own predictive tasks (e.g., estimating forest cover, housing density, crop yields, etc.).\n\n\n\n\n\nFigure 12.10: Rolf et al. 2021 Figure 1: MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#summary",
    "href": "part-03-features/01-features-rcf.html#summary",
    "title": "12  Understanding features",
    "section": "\n12.5 Summary",
    "text": "12.5 Summary\nRandom Convolutional Features (RCFs) form the backbone of MOSAIKS’s ability to handle massive amounts of satellite imagery at scale. Instead of learning tailored filters via backpropagation (like a traditional convolutional neural network), MOSAIKS uses randomly initialized filters that remain fixed. This single-layer approach may seem counterintuitive, but it has several advantages:\n\n\nLightweight and Task-Agnostic\n\nBecause the filters are not tied to any particular outcome, the same feature set can be applied to countless downstream tasks. This drastically reduces the need to reprocess raw imagery for every new prediction goal.\n\n\n\nPlanet-Scale Featurization\n\nMassive amounts of imagery can be processed quickly because no iterative training is needed to refine filters. After a one-time generation of RCFs, they can be stored, shared, and reused—removing the bottleneck of handling petabytes of raw data.\n\n\n\nBroad Feature Capture\n\nRandom filters effectively “sample” a wide variety of spatial and spectral patterns, capturing edges, textures, colors, and more. The “2-for-1” feature approach ensures that each filter captures both positive and negative cues, doubling the dimensionality (and potential information) without doubling compute time.\n\n\n\nEasily Distributed\n\nThe resulting feature vectors (rather than raw images) are small enough to download and manipulate on standard hardware. Researchers and practitioners can thus apply these features to varied applications, from ecological monitoring to socioeconomic modeling.\n\n\n\nBy converting raw imagery into a rich but compact numerical representation, RCFs offer a robust, flexible, and scalable gateway to satellite-based insights—without the headache of storing or manually interpreting vast image archives.\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we’ll look at what is publicly available in the MOSAIKS API and how you can access these features without featurizing data yourself.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html",
    "href": "part-03-features/02-features-api.html",
    "title": "13  API features",
    "section": "",
    "text": "13.1 Overview\nIn this chapter, we focus on the publicly available MOSAIKS features that can be accessed via the MOSAIKS API. These features offer a quick and straightforward way to incorporate satellite-based predictors into your analyses without having to manually process satellite imagery.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#overview",
    "href": "part-03-features/02-features-api.html#overview",
    "title": "13  API features",
    "section": "",
    "text": "Accessing features on the API is covered in Chapter 3. This chapter provides additional details on the features and how they were generated. To generate your own features, see Chapter 14.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#input-imagery",
    "href": "part-03-features/02-features-api.html#input-imagery",
    "title": "13  API features",
    "section": "\n13.2 Input imagery",
    "text": "13.2 Input imagery\n\n\nSourcePlanet Labs Visual Basemap (Global Quarterly 2019, Q3).\n\nTemporal Coverage\nPredominantly images captured between July and September 2019 (Q3), though exact capture dates vary by region.\n\nSpatial Coverage\nGlobal land areas, excluding most large water bodies.\n\nPotential Artifacts\nCloud cover, haze, or shadows may affect the quality of imagery in regions with significant cloud coverage during Q3 2019.\n\n\n\n\n\n\nFigure 13.1: Planet Labs Basemap imagery thumbnail image.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#deeper-look-how-the-api-features-were-generated",
    "href": "part-03-features/02-features-api.html#deeper-look-how-the-api-features-were-generated",
    "title": "13  API features",
    "section": "\n13.3 Deeper Look: How the API Features Were Generated",
    "text": "13.3 Deeper Look: How the API Features Were Generated\nThe MOSAIKS team performed a single featurization pass over the Planet Labs 2019 Q3 Visual Basemap images. The process is similar to the Random Convolutional Features (RCFs) method described earlier, but here are the specific parameters:\n\n\nPatch Collection\n\nRandom patches (filters) were drawn from real satellite images (Planet Labs 2019 Q3).\n\nEach patch was whitened: the raw pixel values were mean-centered and decorrelated so that each filter highlights distinct visual patterns.\n\n\n\nKernel Sizes\n\n\n2,000 features from a 4×4 patch shape\n\n\n2,000 features from a 6×6 patch shape\n\nAll patches maintain 3 color channels (R, G, B).\n\n\n\nBias & Activation\n\nA bias of –1 is added to each filter’s convolution output, allowing more nuanced activation levels.\n\nA ReLU activation (max(0, x)) is then applied to keep the model non-linear and remove negative values.\n\n\n\nPooling\n\nAfter convolution + ReLU, the response is average-pooled over each 0.01° patch (i.e., the local 256×256 pixel area, if we approximate each degree of latitude or longitude as ~100 km—though the exact pixel count can vary by latitude).\n\nThis results in a single numeric value per filter.\n\n\n\nFinal Feature Vector\n\nCombining all filters yields a 4,000-dimensional vector at each 0.01° grid cell.\n\nThis entire process was run once, creating a global 0.01° “feature layer.”\n\n\n\n\n\n\n\n\nFigure 13.2: Three randomly generated feature activation maps plotted from the Planet Labs Basemap thumbnail image.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#derived-aggregations",
    "href": "part-03-features/02-features-api.html#derived-aggregations",
    "title": "13  API features",
    "section": "\n13.4 Derived aggregations",
    "text": "13.4 Derived aggregations\nAlthough the API stores and provides these features at 0.01° resolution, it also offers pre-aggregated versions:\n\n\n\n\nResolution\nDescription\nWeighting\nDownload\nAccess\n\n\n\n0.01°\nNative resolution (~1km²)\nUnweighted\nMap or File Query\nAPI Portal\n\n\n0.1°\nAggregated grid (~10km²)\nArea or population\nChunked files\nGlobal Grids\n\n\n1°\nAggregated grid (~100km²)\nArea or population\nSingle file\nGlobal Grids\n\n\nADM2\nCounty or district level\nArea or population\nSingle file\nPrecomputed Files\n\n\nADM1\nState or province level\nArea or population\nSingle file\nPrecomputed Files\n\n\nADM0\nCountry level\nArea or population\nSingle file\nPrecomputed Files\n\n\n\n\n\nTable 13.1: The MOSAIKS API offers features derived from Planet Labs imagery (2019 Q3) at various resolutions for the entire globe. All features share the same 4,000 feature columns, with the only difference being the resolution/aggregation level. At each aggregation level above the native resolution, we offer both area-weighted and population-weighted versions. Population weights come from the Gridded Population of the World (GPWv4) population density dataset.\n\n\nIt is nice to have options, but sometimes it is hard to visualize what these aggregations mean. Figure 13.3 shows how these features might look when aggregated to different levels.\n\n\n\n\n\nFigure 13.3: Example showing of 3 representative random convolutional features (rows). Features are downloaded from the MOSAIKS API at 0.01° resolution (the native resolution) and aggregated to 3 levels, including (A) larger grid cells (0.1°), (B) counties, and (C) states.\n\n\n\n13.4.1 Area vs. Population Weighting\nEach aggregated level comes in two weighting flavors: - Area Weighting: Larger grid cells or polygons with more land area receive more weight in the aggregation.\n- Population Weighting: Uses population density (GPWv4) to weight cells more heavily where more people live.\nnote maybe a population map here",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#when-to-use-these-features",
    "href": "part-03-features/02-features-api.html#when-to-use-these-features",
    "title": "13  API features",
    "section": "\n13.5 When to use these features",
    "text": "13.5 When to use these features\n\nReadily Available: If your labels are from ~2019 or a period that hasn’t changed drastically since 2019, start with these API features—they’re the fastest and easiest to incorporate into your analyses.\nCoarse-Resolution or Aggregated Labels: If your data is aggregated (e.g., a country-level statistic), consider downloading the aggregated features to match your label resolution.\nResource Constraints: If you’re storage- or compute-limited, using the API’s pre-aggregated files can save time and processing overhead.\nHigh-Resolution or Household Data: For fine-grained tasks (e.g., household surveys), you may still benefit from the 0.01° resolution features—especially if you want to capture local variation. Or you can aggregate to match your label geometry and then use high-resolution features for prediction later (see Chapter 17).\n\n\n\n\n\n\nFigure 13.4: Global Human Development Index (HDI) data at the first sub-national level of administrative division (ADM1). Source: Smits & Permanyer 2019.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#key-takeaways",
    "href": "part-03-features/02-features-api.html#key-takeaways",
    "title": "13  API features",
    "section": "\n13.6 Key takeaways",
    "text": "13.6 Key takeaways\n\n\nSingle Source Imagery\n\n\nPlanet Labs Visual Basemap (2019 Q3)\n\nGlobal coverage of land areas at approximately 0.01° (~1 km) resolution\n\nThree color channels (Red, Green, Blue)\n\n\n\nSingle Featurization Pass\n\nAll features are computed once at the native 0.01° resolution.\n\nAny aggregated features (e.g., 0.1°, 1°, administrative boundaries) are derivative and use exactly the same underlying 0.01° features.\n\n\n\nRandom Convolutional Features (RCFs)\n\n\n4,000 total features\n\n\nKernel sizes: 2,000 features from a 4×4 kernel, and 2,000 features from a 6×6 kernel\n\n\nEmpirical patch whitening: random patches are drawn from the image set and then whitened (mean-centered, decorrelated)\n\n\nBias: –1 (applied to each filter output before activation)\n\n\nActivation: ReLU (Rectified Linear Unit)\n\n\n3 color channels from the original RGB imagery\n\n\n\nFlexible Resolutions\n\n\nHigh resolution (0.01°): Download via File Query or Map Query\n\n\nAggregations: 0.1°, 1°, and ADM0/ADM1/ADM2 boundaries, available as precomputed downloads\n\n\n\nUse Cases\n\nIdeal for tasks with label data from the same (or close) time period (2019 or neighboring years)\n\nQuick, straightforward integration into machine learning models\n\nGlobal scale analysis or smaller local/regional analysis",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#future-directions",
    "href": "part-03-features/02-features-api.html#future-directions",
    "title": "13  API features",
    "section": "\n13.7 Future directions",
    "text": "13.7 Future directions\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html",
    "href": "part-03-features/03-features-computing.html",
    "title": "14  Computing features",
    "section": "",
    "text": "14.1 Overview\nWhile the MOSAIKS API provides pre-computed features for many applications, some use cases require computing custom features. This chapter covers the technical details of generating MOSAIKS features from satellite imagery.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#requirements",
    "href": "part-03-features/03-features-computing.html#requirements",
    "title": "14  Computing features",
    "section": "\n14.2 Requirements",
    "text": "14.2 Requirements\nTo compute MOSAIKS features, you’ll need:\n\nSatellite imagery (see Chapter 10)\nGPU-enabled computing environment (recommended)\nPython with deep learning libraries (pytorch recommended)\nSufficient storage for features",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#implementation",
    "href": "part-03-features/03-features-computing.html#implementation",
    "title": "14  Computing features",
    "section": "\n14.3 Implementation",
    "text": "14.3 Implementation\nThere are several ways to implement MOSAIKS feature extraction:\n\n14.3.1 torchgeo implementation\nThe torchgeo library provides a PyTorch implementation of random convolutional features:\nimport torch\nfrom torchgeo.models import RCF\n\n# Define model parameters\npatch_size = 3  # Size of random patches\nin_channels = 4  # Number of input image channels\nnum_filters = 4000  # Number of features to generate\n\n# When empirical, supply a custom pytorch dataset class that returns \n# a dictionary with 'image' key. This samples the dataset for model \n# weights. If gaussian do not supply a dataset class.\n\n# Initialize RCF model\nmodel = RCF(\n    in_channels=in_channels, \n    features=num_filters, \n    kernel_size=3, \n    bias=-1.0, \n    seed=42, \n    mode='empirical',\n    dataset=CustomDataset,\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#feature-parameters",
    "href": "part-03-features/03-features-computing.html#feature-parameters",
    "title": "14  Computing features",
    "section": "\n14.4 Feature parameters",
    "text": "14.4 Feature parameters\nSeveral key parameters influence feature extraction:\n\n14.4.1 Number of features (K)\n\nControls feature vector dimensionality\nMore features capture more information\nIncreases computation and storage needs\nTypical range: 1,000-8,192\nDiminishing returns above ~4,000\n\n14.4.2 Patch size\n\nDetermines spatial context captured\nLarger patches see more context\nBut increase computation\nTypical size: 3x3 or 5x5 pixels\nMatch to imagery resolution\n\n14.4.3 Input channels\n\nDepends on available spectral bands\nRGB = 3 channels\nCan use additional bands\nMore bands = richer spectral info\nBut increases computation",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#practical-considerations",
    "href": "part-03-features/03-features-computing.html#practical-considerations",
    "title": "14  Computing features",
    "section": "\n14.5 Practical considerations",
    "text": "14.5 Practical considerations\n\n14.5.1 Memory management\nWhen processing large imagery datasets:\n\n14.5.2 Storage formats\nEfficient formats for large feature matrices:\n\n14.5.3 Parallel processing\nFor large-scale feature extraction:",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#quality-control",
    "href": "part-03-features/03-features-computing.html#quality-control",
    "title": "14  Computing features",
    "section": "\n14.6 Quality control",
    "text": "14.6 Quality control\nImportant checks during feature extraction:\n\n\nInput validation\n\nImage dimensions\nValue ranges\nMissing data\nBand ordering\n\n\n\nFeature statistics\n\nDistribution checks\nZero/missing values\nCorrelation analysis\nFeature importance\n\n\n\nPerformance monitoring\n\nMemory usage\nProcessing speed\nGPU utilization\nStorage efficiency",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#best-practices",
    "href": "part-03-features/03-features-computing.html#best-practices",
    "title": "14  Computing features",
    "section": "\n14.7 Best practices",
    "text": "14.7 Best practices\n\n\nDocumentation\n\nRecord all parameters\nTrack data sources\nDocument processing steps\nNote any issues\n\n\n\nTesting\n\nUnit tests for functions\nIntegration tests\nPerformance benchmarks\nValidation checks\n\n\n\nVersion control\n\nCode versioning\nFeature versioning\nParameter tracking\nResult logging\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll work through a complete example of computing custom MOSAIKS features.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html",
    "href": "part-03-features/04-features-demo.html",
    "title": "15  Featurization demo",
    "section": "",
    "text": "15.1 Overview\nThis demonstration will show you a few key concepts",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html#demonstration-code",
    "href": "part-03-features/04-features-demo.html#demonstration-code",
    "title": "15  Featurization demo",
    "section": "\n15.2 Demonstration code",
    "text": "15.2 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical featurization of satellite data for use in MOSAIKS. The notebook will guide you through the process of preparing imagery, including:\n\nBuilding a pytorch model from scratch\nUsing an out of the box solution with torchgeo\nBuilding a torch dataset and dataloader\nFeaturizing satellite imagery\nSaving features to disk\nVisualizing features\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html#whats-next",
    "href": "part-03-features/04-features-demo.html#whats-next",
    "title": "15  Featurization demo",
    "section": "\n15.3 What’s next?",
    "text": "15.3 What’s next?\nNow that we have covered the basics of featurizing satellite data, we will move on to the next section, where we will discuss modeling.\n\n\n\n\n\n\nLooking forward\n\n\n\nThis is the end of the feature section. Next, we will move on to the modeling labels with satellite features.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-04-models/00-model.html",
    "href": "part-04-models/00-model.html",
    "title": "Task modeling",
    "section": "",
    "text": "Section outline\nThe following chapters will guide you through key aspects of modeling tasks within the MOSAIKS framework:\nThese chapters provide",
    "crumbs": [
      "Task modeling"
    ]
  },
  {
    "objectID": "part-04-models/00-model.html#section-outline",
    "href": "part-04-models/00-model.html#section-outline",
    "title": "Task modeling",
    "section": "",
    "text": "Outline of the modeling section\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n16  Build, evaluate, deploy\nAlgorithm selection, hyperparameter tuning\n\n\n17  Going over space\nSpatial cross-validation, geographic generalization\n\n\n18  Going over time\nTime series analysis, temporal alignment\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Task modeling"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html",
    "href": "part-04-models/01-model-choice.html",
    "title": "16  Build, evaluate, deploy",
    "section": "",
    "text": "16.1 Overview\nWhen using MOSAIKS (Multi-task Observation using SAtellite Imagery & Kitchen Sinks), model selection largely depends on your label data type. While more complex methods are certainly possible, experience shows that linear models often perform remarkably well. This is because the random convolutional features in MOSAIKS already capture and encode non-linear information from the underlying satellite imagery. This chapter will outline modeling approaches for different types of label data, and offer guidance on best practices for model evaluation.\nBefore jumping into the details, it is important to emphasize that all modeling tasks benefit from a systematic approach to training, validation, and testing:\nThis ensures that you have separate, unbiased datasets for both model tuning and final performance evaluation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#overview",
    "href": "part-04-models/01-model-choice.html#overview",
    "title": "16  Build, evaluate, deploy",
    "section": "",
    "text": "Train/validation split (80% of the data)\n\n\nTest split (20% of the data)",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#continuous-labels",
    "href": "part-04-models/01-model-choice.html#continuous-labels",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.2 Continuous labels",
    "text": "16.2 Continuous labels\nMany MOSAIKS applications involve predicting continuous outcomes such as forest cover percentage, population density, average income, crop yields, or building height. In these cases, penalized linear regression approaches (especially ridge regression) work well. These methods offer simplicity, computational efficiency, and interpretability, while mitigating overfitting through the use of regularization.\n\n16.2.1 Ridge regression\nRidge regression (L2 regularization) is often the default choice in MOSAIKS applications because it effectively handles the large number of potentially correlated features produced by the random convolution process. It achieves this via an L2 penalty on the regression coefficients, which shrinks coefficients towards zero and reduces variance, thereby improving generalization.\nThe ridge regression objective function:\n\\[\n\\min_{\\beta} \\|y - X\\beta\\|^2_2 + \\lambda\\|\\beta\\|^2_2\n\\]\nWhere:\n\n\n\\(\\lambda\\) controls the strength of the regularization,\n\n\\(\\beta\\) are the regression coefficients,\n\n\\(y\\) is the vector of observed labels,\n\n\\(X\\) is the feature matrix produced by the MOSAIKS pipeline.\n\n16.2.2 Lasso regression\nLasso regression (L1 regularization) is an alternative that shrinks some coefficients to zero, effectively performing feature selection. The L1 penalty helps enforce sparsity, which can be valuable when interpretability or identification of key features is a priority.\nThe lasso objective function:\n\\[\n\\min_{\\beta} \\|y - X\\beta\\|^2_2 + \\lambda\\|\\beta\\|_1\n\\]\nWhere:\n\n\n\\(\\lambda\\) again controls the strength of the penalty,\n\nThe \\(\\|\\beta\\|_1\\) term encourages some coefficients to be exactly zero.\n\n16.2.3 Why linear models work well\nThough the models themselves are linear, the features are not. MOSAIKS uses random convolutions, non-linear activation functions (ReLU), and average pooling to transform raw satellite imagery into highly expressive features. Because these transformations capture a broad range of non-linear spatial patterns, traditional linear methods can then perform well with minimal additional complexity.\n\n16.2.4 Evaluation metrics\nFor continuous outcomes, key evaluation metrics include:\n\n\nR² (coefficient of determination): Measures the proportion of variance in the label data explained by the model.\n\n\nRMSE (root mean squared error): Quantifies the average magnitude of the prediction errors.\n\n\nMAE (mean absolute error): Focuses on the absolute value of prediction errors, less sensitive to large outliers than RMSE.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#binary-classification",
    "href": "part-04-models/01-model-choice.html#binary-classification",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.3 Binary classification",
    "text": "16.3 Binary classification\nIn some MOSAIKS applications, your labels may be binary (e.g., building presence vs. absence, land use change vs. no change). Here, logistic regression often serves as a straightforward choice.\n\n16.3.1 Logistic regression\nLogistic regression models the probability that a given example belongs to the “positive” class:\n\\[\n\\text{logit}(p) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\n\\]\nwhere \\(p\\) is the probability of belonging to the positive class. Despite its simplicity, logistic regression provides robust and interpretable performance for many binary classification tasks with MOSAIKS features.\n\n16.3.2 Evaluation metrics\nFor binary classification, common metrics include:\n\n\nAUC-ROC: The area under the ROC curve, assessing the trade-off between true positive rate and false positive rate across different threshold settings.\n\n\nAccuracy: The proportion of correct predictions.\n\n\nPrecision: Of the positive predictions made, how many were correct?\n\n\nRecall: Of the actual positives in the dataset, how many did we correctly identify?\n\n\nF1 score: The harmonic mean of precision and recall, often used in imbalanced classification settings.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#multi-class-classification",
    "href": "part-04-models/01-model-choice.html#multi-class-classification",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.4 Multi-class classification",
    "text": "16.4 Multi-class classification\nSome applications require predicting multiple categories (e.g., land cover types, crop variety, building categories). These are multi-class classification problems. Several approaches are possible:\n\n\nOne-vs-Rest: Train a binary classifier for each class; each classifier distinguishes one class from “all others.”\n\n\nMultinomial (softmax) regression: A single model to predict probabilities across all classes simultaneously.\n\n\nOrdinal regression: For predicting ordered categories (e.g., mild, moderate, severe damage).\n\n\n16.4.1 Evaluation metrics\nFor multi-class problems, consider:\n\n\nOverall accuracy: Percentage of examples correctly classified.\n\n\nPer-class accuracy: Accuracy within each class, useful if class sizes are imbalanced.\n\n\nConfusion matrix: Provides a detailed breakdown of predictions vs. actual classes.\n\n\nWeighted F1 score: Averages F1 across classes, weighting by class frequency.\n\n\nCohen’s kappa: Measures agreement between predicted and actual labels, adjusting for chance agreement.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#model-selection-workflow",
    "href": "part-04-models/01-model-choice.html#model-selection-workflow",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.5 Model selection workflow",
    "text": "16.5 Model selection workflow\n\n\nIdentify label type\n\nContinuous → Ridge or Lasso regression\n\nBinary → Logistic regression\n\nMulti-class → One-vs-Rest or Multinomial\n\n\n\nCross-validation\n\nSplit data into train, validation, and test sets, respecting spatial structure where possible (e.g., leave-cluster-out).\n\nSelect appropriate evaluation metrics (e.g., R² for continuous, ROC-AUC for binary).\n\nTune hyperparameters (\\(\\lambda\\) in ridge/lasso, regularization in logistic) using the validation set.\n\n\n\nModel deployment\n\nRetrain on the entire training set using the chosen hyperparameters.\n\nGenerate predictions on the test set (or new data).\n\nQuantify uncertainty (e.g., confidence intervals, out-of-sample error estimates).",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#summary",
    "href": "part-04-models/01-model-choice.html#summary",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.6 Summary",
    "text": "16.6 Summary\n\n\nStart with simple linear models: Let MOSAIKS features do the heavy lifting of extracting non-linear patterns.\n\n\nMatch the metric to the task: R² or RMSE for continuous labels, AUC-ROC or F1 score for classification, etc.\n\n\nUse cross-validation: Always separate training and testing to maintain unbiased estimates of model performance.\n\n\nConsider spatial structure: When dealing with spatial data, standard random splits may lead to overly optimistic estimates of performance.\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll discuss strategies for accounting for spatial dependencies in your modeling workflow, including methods for spatially stratified cross-validation, spatial interpolation, and spatial extrapolation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html",
    "href": "part-04-models/02-model-spatial.html",
    "title": "17  Going over space",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter,",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Going over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html",
    "href": "part-04-models/03-model-temporal.html",
    "title": "18  Going over time",
    "section": "",
    "text": "18.1 Understanding temporal resolution\nTime series analysis in satellite-based ML applications requires careful attention to the temporal resolution of both your labels and imagery. In the context of MOSAIKS, you may opt for different strategies depending on the frequency of satellite acquisitions and how quickly your outcome of interest changes.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "href": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "title": "18  Going over time",
    "section": "",
    "text": "Figure 18.1: ESRI visualization of pixel values for a given location changing over time.\n\n\n\n18.1.1 Temporal alignment\nWhen aligning data in time, consider:\n\nLabel frequency: How often are ground measurements collected? For instance, annual crop yields, monthly economic indicators, or even daily weather observations. The granularity of these data points will guide how to match them to your satellite-derived features.\nImagery availability: How frequently can you obtain clear satellite images of your area of interest? High revisit rates enable more frequent observations, but factors like cloud cover or sensor anomalies may reduce the actual number of usable images.\nChange detection: How quickly does your variable of interest change? Urban development may unfold over months or years, whereas flood events can occur within days. Matching the temporal granularity of your features to the dynamics of your phenomenon is crucial.\n\n18.1.2 Seasonal patterns\nBoth natural and human-driven processes often exhibit strong seasonal signals:\n\nNatural cycles: Vegetation growth, snow cover, and water extent are all influenced by seasons. For example, NDVI metrics might be more informative during peak growing seasons (Figure 18.2).\nHuman activities: Cropping cycles, holiday travel, and heating or cooling demand are just a few examples of how human behavior can vary throughout the year. These temporal rhythms can introduce systematic patterns in your data.\nFeature extraction: Because satellite observations reflect surface conditions, different times of the year may require distinct feature sets. For example, reflectance values change when vegetation is senescent vs. when it is fully grown. Similarly, atmospheric conditions (like haze) may be more prevalent in certain seasons.\n\n\n\n\n\n\nFigure 18.2: EOS Data Analytics time series visualization of the Normalized Difference Vegetation Index (NDVI) showing seasonal trends.\n\n\n\n18.1.3 Challenges in time series applications\nAlthough the potential benefits of time series analysis are significant, there are a few common pitfalls:\n\n18.1.3.1 Data gaps\n\nCloud cover: In regions with high cloud coverage, usable imagery can be sparse, leading to irregular time intervals between valid observations.\nSatellite maintenance or sensor dropout: Even short interruptions in satellite operations can reduce data availability.\nOrbital patterns: Some satellites have specific revisit schedules, meaning certain areas might not be imaged as frequently as others, leading to patchy time series data.\n\n\n\n\n\n\nFigure 18.3: Visualization of cloud cover over the Amazon rainforest.\n\n\n\n\n\n\n\n\nRecommended reading\n\n\n\nFor more information on cloud cover and its impact on satellite data, see Flores-Anderson et al. 2023.\n\n\n\n18.1.4 Pre-processed satellite products\nSeveral satellite providers offer pre-processed data products specifically designed for time series analysis. These products handle common challenges like cloud cover and normalization:\n\n18.1.4.1 MODIS vegetation indices\n\n16-day or monthly composites of vegetation indices (e.g., NDVI, EVI)\nAutomated cloud masking and quality control\nSurface reflectance values normalized for atmospheric effects\nGlobal coverage at 250m-1km resolution since 2000\nIdeal for monitoring seasonal vegetation dynamics\n\n18.1.4.2 Planet Basemap\n\nQuarterly visual composites from multiple PlanetScope satellites\nCloud-free mosaics using best available pixels\nColor-balanced and radiometrically calibrated\nGlobal coverage at ~4.7m resolution\nSuitable for tracking gradual land use changes\n\n18.1.4.3 Harmonized Landsat-Sentinel (HLS)\n\nCombined product using Landsat 8-9 and Sentinel-2 imagery\n2-3 day revisit frequency at 30m resolution\nAtmospherically corrected and co-registered\nConsistent surface reflectance values between sensors\nEnables dense time series from 2013-present\n\nThese products reduce the preprocessing burden for users but may not capture rapid changes that occur between compositing periods. The choice between using raw imagery or pre-processed products depends on the temporal resolution needed for your specific application. These data products and how to use them will be covered in greater detail in Chapter 9 and Chapter 10.\n\n18.1.4.4 Temporal consistency\n\nSensor drift: Over time, satellite sensors can degrade or drift, influencing the consistency of your data. Proper calibration can mitigate these issues.\nMulti-sensor calibration: If you are combining data from multiple satellites in a constellation, ensure that differences in sensor sensitivity or bands do not introduce spurious signals in your time series.\n\n\nVideo\nTime lapse imagery of North Platte, Nebraska. Imagery and visualization from planet Labs Inc.\n\n\n18.1.4.5 Storage and computation\n\nData volume: Each additional time step in your analysis increases the storage and processing requirements.\nTemporal correlation: Many time series phenomena exhibit autocorrelation, which can influence how you design and train models. Standard ML algorithms assume independence between samples, so specialized methods or features (such as lagged features) may be required to handle temporal dependencies.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "href": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "title": "18  Going over time",
    "section": "\n18.2 Approaches to time series modeling",
    "text": "18.2 Approaches to time series modeling\nBelow are three common strategies for incorporating time series data within a MOSAIKS-like framework. Each approach has trade-offs in terms of complexity, computational cost, and interpretability.\n\n18.2.1 Feature stacking\nIn a feature-stacking approach, you generate MOSAIKS features for multiple time steps and then concatenate them into a single feature vector. This is straightforward but can lead to very large feature spaces if you have many time points.\n\nCompute features for each time period: Run your MOSAIKS feature extraction for each quarter, month, or year—whatever time granularity is relevant.\nConcatenate features: Combine them into a single vector, ensuring that naming conventions keep time steps distinguishable.\nModel training: Input the stacked features into your preferred machine learning algorithm (e.g., linear regression, random forest, or neural network).\n\nExample (quarterly stacking):\n# Create feature names for each quarter\nfeatures_Q1 = [f'X_{i}_Q1' for i in range(1000)]  # X_0_Q1, X_1_Q1, ..., X_999_Q1\nfeatures_Q2 = [f'X_{i}_Q2' for i in range(1000)]  # X_0_Q2, X_1_Q2, ..., X_999_Q2\nfeatures_Q3 = [f'X_{i}_Q3' for i in range(1000)]  # X_0_Q3, X_1_Q3, ..., X_999_Q3\nfeatures_Q4 = [f'X_{i}_Q4' for i in range(1000)]  # X_0_Q4, X_1_Q4, ..., X_999_Q4\n\n# Combine features names for all quarters\nfeatures_annual = features_Q1 + features_Q2 + features_Q3 + features_Q4\n\n# Total length = 4,000 features for four quarters\nfeatures_df = pd.DataFrame(data=features, columns=features_annual)\nThis approach is typically suitable for annual or seasonal data where the number of time steps remains manageable. However, it may be less practical if you have daily or weekly observations over several years.\n\n18.2.2 Temporal aggregation\nIn temporal aggregation, you compute features at a higher frequency but then summarize them over a time window:\n\nExtract features at a high frequency: This provides a rich temporal view.\nAggregate: Compute statistical summaries such as the mean, max, or variance of each feature across the chosen time window. Common windows include daily, weekly, monthly, and quarterly aggregations.\nModel with aggregated features: The aggregated features can represent dynamic processes while controlling the dimensionality of your dataset.\n\nThis method captures general trends and reduces noise from cloud cover or other transient factors. However, important temporal nuances (like specific short-lived events) might be lost in the aggregation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#demeaning",
    "href": "part-04-models/03-model-temporal.html#demeaning",
    "title": "18  Going over time",
    "section": "\n18.3 Demeaning",
    "text": "18.3 Demeaning\nEvaluation over time\n\n18.3.1 Sequential modeling\nFor phenomena where temporal order is crucial and frequent observations exist, sequential modeling can be more powerful:\n\nFeature extraction: Maintain the time dimension in your feature matrix (e.g., one matrix per location, with time as rows and features as columns).\nApply time series modeling: Techniques like LSTM (Long Short-Term Memory) networks, temporal convolutional networks, or classical state-space models (e.g., ARIMA) can handle temporal dependencies explicitly.\nLagged relationships: Incorporate features from previous time steps to capture delayed effects (e.g., precipitation from last month affecting vegetation today).\n\nAlthough these methods provide a more nuanced view of temporal processes, they require additional modeling expertise and computational resources.\n\n\n\n\n\n\nStart with a simpler approach like feature stacking or temporal aggregation. If you find that your phenomenon has rapid or complex temporal dynamics that aren’t well-captured by these methods, then explore more advanced sequential models.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "href": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "title": "18  Going over time",
    "section": "\n18.4 Hands on with time series data",
    "text": "18.4 Hands on with time series data\nIn lieu of time series MOSAIKS features, we will instead demonstrate similar examples using MODIS NDVI data. This will allow us to explore the challenges and opportunities of time series analysis without the need for custom feature extraction.\nNOTE: UPDATE ME with new notebook link.\n\n\n\n\n\n\nClick the badge to run the demo in Google Colab!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\nRemember to click File -&gt; Save a copy in Drive to save any changes you make.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "href": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "title": "18  Going over time",
    "section": "\n18.5 Filling temporal gaps",
    "text": "18.5 Filling temporal gaps\nWhen working with time series data, you may encounter missing values due to cloud cover, sensor issues, or other factors. Here are some common strategies for filling these gaps:",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#summary",
    "href": "part-04-models/03-model-temporal.html#summary",
    "title": "18  Going over time",
    "section": "\n18.6 Summary",
    "text": "18.6 Summary\nHandling time series in satellite-based ML workflows requires balancing data volume, temporal alignment, and modeling complexity. While feature stacking can be effective for low-frequency or seasonal processes, more sophisticated techniques may be needed to capture high-frequency changes or long-range temporal dependencies. Ultimately, the “best” approach depends on the nature of your target variable, data availability, and the resources at your disposal.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will look at",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/04-model-demo.html",
    "href": "part-04-models/04-model-demo.html",
    "title": "19  Build a model",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will look at",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Build a model</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/00-responsible.html",
    "href": "part-05-responsible/00-responsible.html",
    "title": "Responsible SIML",
    "section": "",
    "text": "Section outline\nThe following chapters will discuss important aspects of model uncertainty and bias, as well as the ethical considerations in the context of MOSAIKS:\nThese chapters provide",
    "crumbs": [
      "Responsible SIML"
    ]
  },
  {
    "objectID": "part-05-responsible/00-responsible.html#section-outline",
    "href": "part-05-responsible/00-responsible.html#section-outline",
    "title": "Responsible SIML",
    "section": "",
    "text": "Chapter\nKey Topics\n\n\n\n20  Bias, equity, & ethics\nResponsible use, communication, limitations\n\n\n21  Quantifying uncertainty\nError sources, confidence intervals, validation\n\n\n22  Transparency (demo)\nDemonstrating how MOSAIKS can be used to characterize uncertainty\n\n\n\n\n\nTable 1: Outline of the uncertainty section\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Responsible SIML"
    ]
  },
  {
    "objectID": "part-05-responsible/01-responsible-ethics.html",
    "href": "part-05-responsible/01-responsible-ethics.html",
    "title": "20  Bias, equity, & ethics",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\nhttps://www.earthcube.org/fair-training-materials\nhttps://www.fatml.org/\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will\n\n\nCould include concepts from this paper: The politics of pixels: A review and agenda for critical remote sensing\nSociopolitical factors impact who collects remotely sensed data, how it is processed, and who benefits from the results. This paper discuses the politics of remote sensing and how it can be used to promote social justice and equity.",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bias, equity, & ethics</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/02-responsible-uncertainty.html",
    "href": "part-05-responsible/02-responsible-uncertainty.html",
    "title": "21  Quantifying uncertainty",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantifying uncertainty</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/03-responsible-demo.html",
    "href": "part-05-responsible/03-responsible-demo.html",
    "title": "22  Transparency (demo)",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe end! The only thing left is references.",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Transparency (demo)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This chapter is in early draft form and may be incomplete.",
    "crumbs": [
      "References"
    ]
  }
]