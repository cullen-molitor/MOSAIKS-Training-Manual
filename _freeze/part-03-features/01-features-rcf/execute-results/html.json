{
  "hash": "656601c1d3c29d67fa99bcfb9bca0c94",
  "result": {
    "engine": "knitr",
    "markdown": "# Understanding features {#sec-features-rcf} \n\n\n\n\n\n\n:::: status\n::: callout-warning \nThis chapter is in early draft form and may be incomplete.\n:::\n::::\n\n\n\n\n\n::: {.callout-tip}\nThis section of the book is not for everyone. If you just want to use MOSAIKS to predict outcomes, you can skip this section. \n:::\n\n## Kitchen sinks?\n\nMOSAIKS stands for **<ins>M</ins>ulti-task <ins>O</ins>bservation using <ins>SA</ins>tellite <ins>I</ins>magery & <ins>K</ins>itchen <ins>S</ins>inks**. Whenever we present this system, people always ask us, \"where does kitchen sinks come from?\" The answer stems from the phrase \"everything but the kitchen sink,\" which means \"almost everything imaginable.\" If you were to hear someone describe making a sandwhich with \"everything but the kitchen sink,\" you would know that they used every ingredient at their disposal. You might also hear the phrase \"everything and the kitchen sink,\" which means \"everything imaginable.\" For MOSAIKS, I think \"everything but the kitchen sink\" is more appropriate as we are not extracting every last drop of information from the imagery. However we are getting a lot out of the imagery. Additionally, if it helps, you can think of the imagery as something we are not taking along with us, rather just the useful bits we extracted. \n\n![Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by [Grace Lewin](https://young-lab.eemb.ucsb.edu/people/grace-lewin).](../images/Everything%20but%20the%20kitchen%20sink.gif){#fig-everything-but-the-kitchen-sink}\n\nThis idea of leaving behind the imagery is one of the reasons MOSAIKS is so powerful. It means that for most users, they never have to sufer through the issues that arise from dealing with large amounts of satellite imagery. We (the MOSAIKS team) take the imagery, extract a bunch of random convolutional features, and then we leave the imagery behind. We don't need to know what the imagery looks like, we don't need to interpret what any feature is, we just need to know that we have a bunch of features that we can use to predict different outcomes. Then users can download these features and use them to predict *their* outcomes.\n\nIn this section of the book, we leave this simplistic aspect of MOSAIKS behind. We go back to the root of the problem and focus on how we extract features from imagery. We also attempt to provide *some* intuition for what these features are.\n\n::: {.callout-tip}\nIn this book, we refer to the random convolutional features as RCFs, features, satellite features, MOSAIKS features, and possibly other terms. You should consider these terms interchangeable in this context.\n:::\n\n## Why use RCFs?\n\n### Traditional convolutional neural networks (CNNs)\n\nTo learn why we use random convolutional features (RCFs), we need to understand how traditional convolutional neural networks (CNNs) work. CNNs are a type of neural network that are designed to work with images. They are made up of layers of neurons that are connected in a way that allows them to learn features from the images. In the case of CNNs, these features are learned through a process called backpropagation. Backpropagation is a method for training neural networks that uses the chain rule of calculus to calculate the gradient of the loss function with respect to the weights of the network. This gradient is then used to update the weights of the network in a way that minimizes the loss function. This means that the network is learning to recognize features in the images that are important for the task it is trying to solve. It is therfore using minimization to learn the features that are important for the task.\n\n![A simplified diagram showing a typical convolutional nueral network model architecture used for ](../images/CNN.jpg){#fig-CNN}\n\n### Replacing minimization with randomization in learning\n\nMOSAIKS takes a different approach to learning features from images. Instead of using backpropagation to learn the features, we use random convolutional filters to extract features from the images. The resultant features are therfore as random as the filters applied to the images. This means that the features are not optimized for any particular task, but are instead a random sample of the information in the images. This is a key difference between RCFs and traditional CNNs. Traditional CNNs learn features that are optimized for the task they are trying to solve, while RCFs extract features that are random samples of the information in the images. This means that RCFs are much faster to compute and can be used to extract features from large amounts of imagery in a short amount of time.\n\n![[Rolf et al. 2021 Figure 1](https://www.nature.com/articles/s41467-021-24638-z/figures/1): MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.](../images/rolf_et_al_2021-Fig_1.jpg){#fig-Rolf-et-al-2021-Fig-1}\n\n## Random convolutional features (RCFs)\n\n### Turning an image into a feature vector\n\nThe core of MOSAIKS is the random convolutional features (RCFs) that we extract from satellite imagery. The extraction process is simple: we take a bunch of random convolutional filters and apply them to the imagery. Each convolutional filter can be a tensor of randomly generated values (gaussian patches), or they can be sampled from the imagery itself (empirical patches; @fig-Rolf-et-al-2021-Fig-1-C). \n\n![[Rolf et al. 2021 Figure 1 C](https://www.nature.com/articles/s41467-021-24638-z/figures/1): A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. *K* patches are randomly sampled from across the *N* images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image.](../images/rolf_et_al_2021-Fig_1-c.jpg){#fig-Rolf-et-al-2021-Fig-1-C}\n\nIf we take a convolutional filter and apply it to the imagery, we get a new image. This new image is the result of the convolutional filter \"looking\" at the imagery and \"seeing\" something. The filter might be looking for edges, colors, textures, patterns, or anything else. The filter might be looking for something that we can't even see with our eyes. After the convolution is applied, a non-linear activation function transforms the result. In this case, we use the rectified linear unit (ReLU) activation function. ReLU is a simple function that takes the maximum of the input and zero. This means that if the input is negative, the output is zero, and if the input is positive, the output is the input. After this, we use an adaptive average pooling layer to reduce the size of the activation map to a single number. This summary value is the feature for this filter.\n\n::: {.callout-note}\nAfter repeating this process for all the filters, we end up with a feature vector for the image. This feature vector is a representation of the image that. \n:::\n\n### Intuition for RCFs\n\nHere we will take a look at a cartoon representation of what a RCF vector might be looking for in the imagery. We will use a few simple 3 pixel by 3 pixel filters in the convolution step. The filters can have a few common names including weights, kernels, or patches. In the cartoon, we will use the term patch because these filters are drawn from the imagery (i.e. they are a small \"patch\" of the imagery).\n\n![A single image with 3 random convolutional patches, highlighting 3 different feature activations of the imagery.](../images/cartoon_features-1.jpg){#fig-cartoon-features-1}\n\nIn the cartoon above (@fig-cartoon-features-1), our first patch is drawn from the forest canopy and so we see a 3x3 set of green pixels. Patch 2 is a 3x3 patch of grey pixels from a road. Patch 3 is a 3x3 patch of blue pixels from a river. When we apply these patches to the imagery, we get a new image that highlights the features that the patch is looking for. For example, when we apply the forest patch to the imagery, we get a new image that highlights the forest canopy. When we apply the road patch to the imagery, we get a new image that highlights the road. When we apply the river patch to the imagery, we get a new image that highlights the river.\n\n![Two cartoon images with the same patches used in the convolution step. We see that we have several labels for each image and that we can model each outcome with the same set of features.](../images/cartoon_features.mp4){#fig-cartoon-label-video autoplay=\"true\"}\n\nHaving more trees in the image will increase the value of the feature that represents the forest patch. In @fig-cartoon-label-video, image 1 has more trees than image 2, so we see the feature in the first place in the vector has a higher value. Having more roads in the image will increase the value of the feature that represents the road patch, which is seen with image 2 having a higher second value. This is how we can use the features to predict different outcomes from the imagery. We can use the same set of features to predict the presence or levels of different outcomes in the imagery.\n\n![A cartoon image projected into random feature space. If we look at just 2 dimensions, how grey on the x-axis and how green on the y-axis, we can find the vector through this feature space which represents our outcome.](../images/cartoon_vectors.mp4){#fig-cartoon-vect-video autoplay=\"true\"}\n\n::: {.callout-note}\n# Looking forward\n\nIn the next section, we will look at what is publicly available in the MOSAIKS API.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}