{
  "hash": "1170f81f1013a4a2fc9cf431f0da9048",
  "result": {
    "markdown": "# Satellite Features  \n\n\n\n:::: status\n::: callout-warning \nThis chapter is in early draft form and may be incomplete.\n:::\n::::\n\n\n::: {.callout-tip}\nThis section of the book is not for everyone. If you just want to use MOSAIKS to predict outcomes, you can skip this section. \n:::\n\n## Kitchen sinks?\n\nMOSAIKS stands for **<ins>M</ins>ulti-task <ins>O</ins>bservation using <ins>SA</ins>tellite <ins>I</ins>magery & <ins>K</ins>itchen <ins>S</ins>inks**. Whenever we present this system, people always ask us, \"ah, what about kitchen sinks?\" The answer stems from the phrase \"everything but the kitchen sink,\" which means \"almost everything imaginable.\" If you were to hear someone describe making a sandwhich with \"everything but the kitchen sink,\" you would know that they used every ingredient at their disposal. You might also hear the phrase \"everything and the kitchen sink,\" which means \"everything imaginable.\" For MOSAIKS, I like the idea of \"everything but the kitchen sink\" because we are not extracting every last drop of information from the imagery, but it implies that we are getting a lot out of the imagery. Additionally, if it helps, you can think of the imagery as something we are not taking along with us, rather just the useful bits we extracted. \n\n![Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by [Grace Lewin](https://young-lab.eemb.ucsb.edu/people/grace-lewin).](images/Everything%20but%20the%20kitchen%20sink.gif){#fig-everything-but-the-kitchen-sink}\n\nThis idea of leaving behind the imagery is one of the reasons MOSAIKS is so powerful. It means that for most users, they never have to sufer through the issues that arise from dealing with large amounts of satellite imagery. We (the MOSAIKS team) take the imagery, extract a bunch of random convolutional features, and then we leave the imagery behind. We don't need to know what the imagery looks like, we don't need to interpret what any feature is, we just need to know that we have a bunch of features that we can use to predict some outcome. Then users can download these features and use them to predict *their* outcomes.\n\nIn this section of the book, we leave this simplistic aspect of MOSAIKS behind and focus on what that imagery looks like, how we extract features from it, and attempt to provide some intuition for what these features are.\n\n::: {.callout-tip}\nFrom here on out, we will refer to the random convolutional features as either RCFs or features. You should consider these terms interchangeable in the context of MOSAIKS.\n:::\n\n## Why use RCFs?\n\n### Traditional convolutional neural networks (CNNs)\n\nTo learn why we use random convolutional features (RCFs), we need to understand how traditional convolutional neural networks (CNNs) work. CNNs are a type of neural network that are designed to work with images. They are made up of layers of neurons that are connected in a way that allows them to learn features from the images. In the case of CNNs, these features are learned through a process called backpropagation. Backpropagation is a method for training neural networks that uses the chain rule of calculus to calculate the gradient of the loss function with respect to the weights of the network. This gradient is then used to update the weights of the network in a way that minimizes the loss function. This means that the network is learning to recognize features in the images that are important for the task it is trying to solve. It is therfore using minimization to learn the features that are important for the task.\n\n![A simplified diagram showing a typical convolutional nueral network model architecture used for ](images/CNN.jpg){#fig-CNN}\n\n### Replacing minimization with randomization in learning\n\nMOSAIKS takes a different approach to learning features from images. Instead of using backpropagation to learn the features, we use random convolutional filters to extract features from the images. The resultant features are therfore as random as the filters applied to the images. This means that the features are not optimized for any particular task, but are instead a random sample of the information in the images. This is a key difference between RCFs and traditional CNNs. Traditional CNNs learn features that are optimized for the task they are trying to solve, while RCFs extract features that are random samples of the information in the images. This means that RCFs are much faster to compute and can be used to extract features from large amounts of imagery in a short amount of time.\n\n![[Rolf et al. 2021 Figure 1](https://www.nature.com/articles/s41467-021-24638-z/figures/1): MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.](images/rolf_et_al_2021-Fig_1.jpg){#fig-Rolf-et-al-2021-Fig-1}\n\n## Random convolutional features (RCFs)\n\nThe core of MOSAIKS is the random convolutional features (RCFs) that we extract from satellite imagery. The extraction process is simple: we take a bunch of random convolutional filters and apply them to the imagery. Each convolutional filter can be a tensor of randomly generated values (gaussian patches), or they can be sampled from the imagery itself (empirical patches; @fig-Rolf-et-al-2021-Fig-1-C). \n\n![[Rolf et al. 2021 Figure 1 C](https://www.nature.com/articles/s41467-021-24638-z/figures/1): A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. *K* patches are randomly sampled from across the *N* images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image.](images/rolf_et_al_2021-Fig_1-c.jpg){#fig-Rolf-et-al-2021-Fig-1-C}\n\nLets stay with the idea of empiricial features for a moment. If we take a convolutional filter and apply it to the imagery, we get a new image. This new image is the result of the convolutional filter \"looking\" at the imagery and \"seeing\" something. The filter might be looking for edges, or colors, or textures, or patterns, or anything else. The filter might be looking for something that we can't even see with our eyes. After the convolution is applied, a non-linear activation function transforms the result. In this case, we use the rectified linear unit (ReLU) activation function. ReLU is a simple function that takes the maximum of the input and zero. This means that if the input is negative, the output is zero, and if the input is positive, the output is the input. After this, we use an adaptive average pooling layer to reduce the size of the image. This is a simple operation that takes the average of the pixels in a region of the image and outputs a single value. This value is our feature for this filter. \n\n### Intuition for RCFs\n\nHere we will take a look at a simplified cartoon representation of what a RCF might be looking for in the imagery. We will use a few simple 3x3 filters drawn directly from a single image. \n\n![](images/cartoon_features-1.jpg){#fig-cartoon-features-1}\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}